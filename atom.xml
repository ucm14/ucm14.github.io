<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mason&#39;s Radio</title>
  
  
  <link href="http://ucm14.github.io/atom.xml" rel="self"/>
  
  <link href="http://ucm14.github.io/"/>
  <updated>2024-10-04T16:38:11.630Z</updated>
  <id>http://ucm14.github.io/</id>
  
  <author>
    <name>Minfeng &quot;Mason&quot; Yu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Mason&#39;s Radio#3</title>
    <link href="http://ucm14.github.io/2024/10/05/Mason-s-Radio-3/"/>
    <id>http://ucm14.github.io/2024/10/05/Mason-s-Radio-3/</id>
    <published>2024-10-04T16:24:04.000Z</published>
    <updated>2024-10-04T16:38:11.630Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>The song features an arrangement and emotional progression ahead of its time, with realistic and finely detailed lyrics. Combined with Lena Park‚Äôs near-perfect performance, it creates a vivid and touching experience for the listeners.</p></blockquote><iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/1hNYiYh10zoFl4L6RrLDHq?utm_source=generator" width="100%" height="352" frameborder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe><p>This song has been performed many times on South Korean television programs. The following video is, in my opinion, the best live performance.</p><div style="text-align: center;">    <iframe width="560" height="315" src="https://www.youtube.com/embed/FrjNd92fXVo?si=kBPhNm3o5alfFab2" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Music" scheme="http://ucm14.github.io/categories/Music/"/>
    
    
    <category term="Pop" scheme="http://ucm14.github.io/tags/Pop/"/>
    
  </entry>
  
  <entry>
    <title>Lost in Vibe</title>
    <link href="http://ucm14.github.io/2024/10/04/Lost-in-the-vibe/"/>
    <id>http://ucm14.github.io/2024/10/04/Lost-in-the-vibe/</id>
    <published>2024-10-04T15:24:44.000Z</published>
    <updated>2024-10-04T17:12:17.290Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>Just few photos I took in days before.</p></blockquote><h1 id="Yangtze-River"><a href="#Yangtze-River" class="headerlink" title="Yangtze River"></a>Yangtze River</h1><p><img src="https://s2.loli.net/2024/10/04/uRyVpGH2aroUmJl.jpg" alt="photo_2_2024-10-04_23-22-07.jpg"></p><p>Yangtze River Bridge.</p><p><img src="https://s2.loli.net/2024/10/04/DFOj1VJ2WYNvqmt.jpg" alt="photo_4_2024-10-04_23-22-07.jpg"></p><p>I was planed to take this ferry service to the southside of Nanjing. However, too many tourists made me give up the idea.</p><p><img src="https://s2.loli.net/2024/10/04/qmGMaAUlBvfIx7b.jpg" alt="photo_7_2024-10-04_23-22-07.jpg"></p><p>Relief.</p><h1 id="On-the-street"><a href="#On-the-street" class="headerlink" title="On the street"></a>On the street</h1><p><img src="https://s2.loli.net/2024/10/04/SjIp9Kx2VGWsFYJ.jpg" alt="photo_10_2024-10-04_23-22-07.jpg"></p><p>A interesting name for a road.</p><p><img src="https://s2.loli.net/2024/10/04/NcKLpCMj7DnVmbR.jpg" alt="photo_5_2024-10-04_23-22-07.jpg"></p><p>Sounds like a Internet Cafe.</p><p><img src="https://s2.loli.net/2024/10/04/XQJj8cfdbRZGmCe.jpg" alt="photo_6_2024-10-04_23-22-07.jpg"></p><p>The railway.</p><p><img src="https://s2.loli.net/2024/10/04/9eliuy8qgZj5cHM.jpg" alt="photo_1_2024-10-04_23-22-07.jpg"></p><p>An insane e-bike outfit.</p><p><img src="https://s2.loli.net/2024/10/04/xXBe8h4kCWVULEt.jpg" alt="photo_3_2024-10-04_23-22-07.jpg"></p><p>Truly a strange name for a road in Chinese.</p><p><img src="https://s2.loli.net/2024/10/04/gb6kAC2li3XQrpI.jpg" alt="photo_9_2024-10-04_23-22-07.jpg"></p><p>A very historic road.</p><p><img src="https://s2.loli.net/2024/10/04/EMwomNZHGchBjuW.jpg" alt="photo_8_2024-10-04_23-22-07.jpg"></p><p>I‚Äôm not good at playing either Chinese chess or chess, but enjoying watching chess game.</p><p><img src="https://s2.loli.net/2024/10/05/CZzBPX2maexLKF8.jpg" alt="photo_12_2024-10-04_23-22-07.jpg"></p><p>Old building sieged by developed urban area.</p><h1 id="Specialties"><a href="#Specialties" class="headerlink" title="Specialties"></a>Specialties</h1><p><img src="https://s2.loli.net/2024/10/05/cdPkYHSBG61ZfTy.jpg" alt="photo_14_2024-10-04_23-22-07.jpg"></p><p>Steamed Milk Egg Custard with Oreo top.</p><p><img src="https://s2.loli.net/2024/10/05/oDdNeW4hV8kcxn6.jpg" alt="photo_13_2024-10-04_23-22-07.jpg"></p><p>Black sesame sweet soup with rice dumplings &amp; Milk brick.</p><p><img src="https://s2.loli.net/2024/10/05/tm8h7YJn3ZCX6LB.jpg" alt="photo_18_2024-10-04_23-22-07.jpg"></p><p>Traditional Xuzhou cuisines.</p><p><img src="https://s2.loli.net/2024/10/05/rJBcXhTWM6naoqK.jpg" alt="photo_11_2024-10-04_23-22-07.jpg"></p><p>Nine-delicacy noodle.</p><p><img src="https://s2.loli.net/2024/10/05/FJw9WKiLPprTYDR.jpg" alt="photo_15_2024-10-04_23-22-07.jpg"></p><p>Nanjing roast duck with sweet brine.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Misc" scheme="http://ucm14.github.io/categories/Misc/"/>
    
    
    <category term="Daily Life" scheme="http://ucm14.github.io/tags/Daily-Life/"/>
    
  </entry>
  
  <entry>
    <title>2. Begin with model-free algorithms</title>
    <link href="http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/"/>
    <id>http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/</id>
    <published>2024-10-02T06:20:50.000Z</published>
    <updated>2024-10-02T14:40:39.484Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Model-based-Model-free-RL"><a href="#Model-based-Model-free-RL" class="headerlink" title="Model-based / Model-free RL"></a>Model-based / Model-free RL</h1><p>Everybody knows reinforcement learning is one of a machine learning methods aiming to make agent explore and find the optimal policy through interactions with environment and maximize cumulative reward. Before starting this passage, we have to understand what is model-free and what is model-based?</p><p><img src="https://s2.loli.net/2024/08/03/4QSDOYacz9pHLAG.png" alt="image.png"></p><h1 id="Monte-Carlo-MC"><a href="#Monte-Carlo-MC" class="headerlink" title="Monte-Carlo (MC)"></a>Monte-Carlo (MC)</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>The Monte Carlo algorithm is a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The core idea is to use randomness to solve problems that might be deterministic in principle. The key concepts of MC are: <strong>random sampling</strong>, <strong>estimation</strong> by taking the average of outcomes from random samples, applicability of <strong>high-dimensional spaces</strong>. It can be used in simulations, optimization, numerical integration, financial anticipation and statistical physics. Advantages and disadvantages of MC are shown in the table below.</p><table><thead><tr><th>Monte Carlo</th><th>Advantages</th><th>Disadvantages</th></tr></thead><tbody><tr><td>1</td><td>Simple to implement</td><td>Can be computationally intensive</td></tr><tr><td>2</td><td>Flexible and can handle a wide range of problems</td><td>Convergence can be slow</td></tr><tr><td>3</td><td>Suitable for high-dimensional integrals</td><td>Results are probabilistic, not deterministic</td></tr></tbody></table><p>Here is a classic example of MC application: Estimate the value of $œÄ$.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">defestimate_pi(num_samples):</span><br><span class="line">    inside_circle =<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _inrange(num_samples):</span><br><span class="line">        x = random.uniform(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">        y = random.uniform(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> x**<span class="number">2</span> + y**<span class="number">2</span> &lt;=<span class="number">1</span>:</span><br><span class="line">            inside_circle +=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> (inside_circle / num_samples) *<span class="number">4</span></span><br><span class="line"></span><br><span class="line">pi_estimate = estimate_pi(<span class="number">100000</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Estimated value of œÄ:<span class="subst">{pi_estimate}</span>"</span>)</span><br></pre></td></tr></tbody></table></figure><p>The result is <code>œÄ: 3.13544</code>.</p><p>Assuming we have a state sequence under a policy œÄ:<br>$$<br>[S_1,A_1,R_1,S_2,A_2,R_2,S_3,A_3,R_3,‚Ä¶,S_T,A_T,R_T]\tag{1}<br>$$<br>In MDP, the value function is:<br>$$<br>v_œÄ(s)=E_œÄ[G_t|S_t=s]G_t=R_{t+1}+Œ≥R_{t+2}+‚Ä¶+Œ≥^{T‚àít‚àí1}R_Tv_œÄ(s)‚âàaverage(G_t)\tag{2}<br>$$<br>However, the average consumes so many storage. A better method is obtaining average value in iterations:<br>$$<br> Œºt=\frac 1 t ‚àëj=\frac 1 t x_j=\frac 1 t (x_t+\sum_{j=1}^{t‚àí1}x_j)=Œº_{t‚àí1}+\frac 1 t (x_t‚àíŒº_{t‚àí1})\tag{3}<br>$$</p><h1 id="Q-Learning-the-first-step-of-model-free-RL"><a href="#Q-Learning-the-first-step-of-model-free-RL" class="headerlink" title="Q Learning: the first step of model-free RL"></a>Q Learning: the first step of model-free RL</h1><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Q learning is a value-based RL algorithm. Thus, Q value is the fundamental variable in this algorithm, and this is the reason why this algorithm called ‚ÄúQ Learning‚Äù. Assuming we already know what is agent, what is environment, what are the states, the actions have to be discrete and the reward function, let us cut to the point and introduce the learning method of agent in Q Learning.<br>$$<br> Q(s,a)\leftarrow Q(s,a)+Œ±(r+Œ≥max_{a^‚Ä≤}Q(s^‚Ä≤,a^‚Ä≤)‚àíQ(s,a))\tag{4}<br>$$<br>Equation above is the update method of Q value. Œ± is learning rate (basically learning rate defines the pace of updating), Œ≥ is discount factor. $Q(s,a)$ is the action value of current state; $Q(s^‚Ä≤,a^‚Ä≤)$ is the action value of next state; $max_{a^‚Ä≤}Q(s^‚Ä≤,a^‚Ä≤)$ means choosing the maximum $Q(s^‚Ä≤,a^‚Ä≤)$ among all the possible actions.</p><p>The Q learning is quite similar to value iteration. We are going to explain the differences between Q learning and value iteration in detail through a simple implementation.</p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h3><p>The environment is a 4x4 grid. Our target is to train the agent and let it successfully moves from (0, 0) to (3,3).</p><p><img src="https://s2.loli.net/2024/08/03/qbufGrCwPdHk2oU.png" alt="image.png"></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">grid_size = <span class="number">4</span></span><br><span class="line">goal_state = (<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">start_state = (<span class="number">0</span>,<span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="Hyperparameters"><a href="#Hyperparameters" class="headerlink" title="Hyperparameters"></a>Hyperparameters</h3><p>$œµ$ means the agent have probability equals to œµ choosing a random action and $(1-œµ)$ probability choosing the optimal action.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">alpha =<span class="number">0.1</span> <span class="comment">#learning rate</span></span><br><span class="line">gamma =<span class="number">0.9</span> <span class="comment">#discount factor</span></span><br><span class="line">epsilon =<span class="number">0.1</span> <span class="comment">#epsilon-greedy</span></span><br><span class="line">num_episodes = <span class="number">1000</span></span><br></pre></td></tr></tbody></table></figure><h3 id="State-s-and-action-s"><a href="#State-s-and-action-s" class="headerlink" title="State(s) and action(s)"></a>State(s) and action(s)</h3><p>The state is just all the points in the 4x4 girds. We create two functions for action selection and transition to next state.</p><p>In function <code>get_next_state</code>, the current coordinates based on the action:</p><ul><li>If the action is <code>'up'</code> and <code>x</code> is greater than 0, it moves up by decrementing <code>x</code> by 1.</li><li>If the action is <code>'down'</code> and <code>x</code> is less than <code>grid_size - 1</code>, it moves down by incrementing <code>x</code> by 1.</li><li>If the action is <code>'left'</code> and <code>y</code> is greater than 0, it moves left by decrementing <code>y</code> by 1.</li><li>If the action is <code>'right'</code> and <code>y</code> is less than <code>grid_size - 1</code>, it moves right by incrementing <code>y</code> by 1.</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_next_state</span>(<span class="params">state, action</span>):</span><br><span class="line">    x, y = state</span><br><span class="line">    <span class="keyword">if</span> action == <span class="string">'up'</span> <span class="keyword">and</span> x &gt; <span class="number">0</span>:</span><br><span class="line">        x -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="string">'down'</span> <span class="keyword">and</span> x &lt; grid_size - <span class="number">1</span>:</span><br><span class="line">        x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="string">'left'</span> <span class="keyword">and</span> y &gt; <span class="number">0</span>:</span><br><span class="line">        y -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="string">'right'</span> <span class="keyword">and</span> y &lt; grid_size - <span class="number">1</span>:</span><br><span class="line">        y += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> (x, y)</span><br><span class="line"></span><br><span class="line">actions = [<span class="string">'up'</span>,<span class="string">'down'</span>,<span class="string">'left'</span>,<span class="string">'right'</span>]</span><br><span class="line">num_actions =<span class="built_in">len</span>(actions)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_action</span>(<span class="params">state</span>):</span><br><span class="line"><span class="keyword">if</span> np.random.uniform(<span class="number">0</span>,<span class="number">1</span>) &lt; epsilon:</span><br><span class="line"><span class="keyword">return</span> np.random.choice(num_actions)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> np.argmax(Q_table[state[<span class="number">0</span>], state[<span class="number">1</span>], :])</span><br></pre></td></tr></tbody></table></figure><h3 id="Reward-function"><a href="#Reward-function" class="headerlink" title="Reward function"></a>Reward function</h3><p>Agent will get reward = 100 if arriving at (3, 3), otherwise -1.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_reward</span>(<span class="params">state</span>):</span><br><span class="line"><span class="keyword">if</span> state == goal_state:</span><br><span class="line"><span class="keyword">return</span> <span class="number">100</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Q-value-function"><a href="#Q-value-function" class="headerlink" title="Q value function"></a>Q value function</h3><p>This part is the fundamental training loop of algorithm. <code>while state != goal_state</code> means continues iterations until the goal state is reached; <code>action_index = choose_action(state)</code> means select an action index based on policy which not shown; <code>action = actions[action_index]</code>is to convert the action index to an actual action; <code>next_state = get_next_state(state, action)</code> is used to determine the next state based on the current state and action; finally,<code>reward = get_reward(next_state)</code>can calculate the reward for transitioning to the next state.</p><p>Then, we store Q value under current (s, a) pair in Q table, computes the maximum Q value for next state and updates Q value for current $(s,a)$ pair using Q learning update rule.</p><p>Since we have the maximum Q value of each state stored in Q table, we can try to compute the optimal policy and turn it into the form of path.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> episodeinrange(num_episodes):</span><br><span class="line">    state = start_state</span><br><span class="line"><span class="keyword">while</span> state != goal_state:</span><br><span class="line">        action_index = choose_action(state)</span><br><span class="line">        action = actions[action_index]</span><br><span class="line">        next_state = get_next_state(state, action)</span><br><span class="line">        reward = get_reward(next_state)</span><br><span class="line"></span><br><span class="line">        Q_table[state[<span class="number">0</span>], state[<span class="number">1</span>], action_index] += alpha * (</span><br><span class="line">                reward + gamma * np.<span class="built_in">max</span>(Q_table[next_state[<span class="number">0</span>], next_state[<span class="number">1</span>], :]) - Q_table[</span><br><span class="line">            state[<span class="number">0</span>], state[<span class="number">1</span>], action_index]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        state = next_state</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Final Q-Table:"</span>)</span><br><span class="line"><span class="built_in">print</span>(Q_table)</span><br><span class="line"></span><br><span class="line">state = start_state</span><br><span class="line">path = [state]</span><br><span class="line"><span class="keyword">while</span> state != goal_state:</span><br><span class="line">    action_index = np.argmax(Q_table[state[<span class="number">0</span>], state[<span class="number">1</span>], :])</span><br><span class="line">    action = actions[action_index]</span><br><span class="line">    state = get_next_state(state, action)</span><br><span class="line">    path.append(state)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Path from start to goal:"</span>)</span><br><span class="line"><span class="built_in">print</span>(path)</span><br></pre></td></tr></tbody></table></figure><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>The path after training is shown in the figure.</p><p><img src="https://s2.loli.net/2024/08/03/kjtiXCEQDSoywsK.png" alt="image.png"></p><h1 id="Sarsa-An-on-policy-RL-algorithm"><a href="#Sarsa-An-on-policy-RL-algorithm" class="headerlink" title="Sarsa: An on-policy RL algorithm"></a>Sarsa: An on-policy RL algorithm</h1><h2 id="Difference-between-on-policy-off-policy"><a href="#Difference-between-on-policy-off-policy" class="headerlink" title="Difference between on-policy &amp; off-policy"></a>Difference between on-policy &amp; off-policy</h2><p>In reinforcement learning, <strong>two different policies</strong> are also used for active agents: <strong>a behavior policy</strong> and <strong>a target policy</strong>. A behavior policy is used to decide actions in a given state (what behavior the agent is currently using to interact with its environment), while a target policy is used to learn about desired actions and what rewards are received (the ideal policy the agent seeks to use to interact with its environment).</p><blockquote><p>If an algorithm‚Äôs behavior policy matches its target policy, this means it is an on-policy algorithm. If these policies in an algorithm don‚Äôt match, then it is an off-policy algorithm.</p></blockquote><p><img src="https://s2.loli.net/2024/08/03/D3tYshGI4uvSbdo.png" alt="image.png"></p><p>Sarsa operates by choosing an action following the current epsilon-greedy policy and updates its Q values accordingly. On-policy algorithms like Sarsa select random actions where non-greedy actions have some probability of being selected, providing a balance between exploitation and exploration techniques. Since Sarsa Q values are generally learned using the same epsilon-greedy policy for behavior and target, it classifies as on-policy.</p><p>Q learning, unlike Sarsa, tends to choose the greedy action in sequence. A greedy action is one that gives the maximum Q value for the state, that is, it follows an optimal policy. Off-policy algorithms like Q learning learn a target policy regardless of what actions are selected from exploration. Since Q learning uses greedy actions, and can evaluate one behavior policy while following a separate target policy, it classifies as off-policy.</p><h2 id="Algorithm-1"><a href="#Algorithm-1" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>SARSA, unlike Q learning, is an on-policy algorithm, which means it updates the policy based on the actions taken. Quite like policy iteration. But in Sarsa, the update of policy will not be as ‚Äúhard‚Äù as policy iteration since we have the influence of learning rate $Œ±$.</p><p>Still, the algorithm keeps updating Q value. One thing different is agent of Sarsa already come up with which action to choose and predict next state and next action. That is why the algorithm called SARSA - State, Action, Reward, State, Action.</p><h2 id="Implementation-1"><a href="#Implementation-1" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Maze</span></span><br><span class="line">maze = np.array([</span><br><span class="line">    [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, -<span class="number">1</span>,<span class="number">0</span>, -<span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>, -<span class="number">1</span>],</span><br><span class="line">    [-<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start state and terminal state</span></span><br><span class="line">start_state = (<span class="number">3</span>,<span class="number">0</span>)</span><br><span class="line">goal_state = (<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># action space</span></span><br><span class="line">actions = [(<span class="number">0</span>,<span class="number">1</span>), (<span class="number">0</span>, -<span class="number">1</span>), (-<span class="number">1</span>,<span class="number">0</span>), (<span class="number">1</span>,<span class="number">0</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize value function</span></span><br><span class="line">Q = np.zeros((<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyperparameters</span></span><br><span class="line">alpha = <span class="number">0.1</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">max_episodes = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SARSA</span></span><br><span class="line"><span class="keyword">for</span> episodeinrange(max_episodes):</span><br><span class="line">    state = start_state</span><br><span class="line">    action = np.random.choice(<span class="built_in">range</span>(<span class="number">4</span>))<span class="keyword">if</span> np.random.rand() &lt; epsilonelse np.argmax(Q[state])</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> state != goal_state:</span><br><span class="line"><span class="comment"># next_state = (state[0] + actions[action][0], state[1] + actions[action][1])</span></span><br><span class="line">        a = state[<span class="number">0</span>] + actions[action][<span class="number">0</span>]</span><br><span class="line">        b = state[<span class="number">1</span>] + actions[action][<span class="number">1</span>]</span><br><span class="line"><span class="keyword">if</span> a &gt;<span class="number">3</span>:</span><br><span class="line">            a-=<span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> b &gt;<span class="number">3</span>:</span><br><span class="line">            b-=<span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> a &lt; -<span class="number">4</span>:</span><br><span class="line">            a+=<span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> b &lt; -<span class="number">4</span>:</span><br><span class="line">            b+=<span class="number">1</span></span><br><span class="line">        next_state = (a,b)</span><br><span class="line">        reward = maze[next_state]</span><br><span class="line">        next_action = np.random.choice(<span class="built_in">range</span>(<span class="number">4</span>))<span class="keyword">if</span> np.random.rand() &lt; epsilonelse np.argmax(Q[next_state])</span><br><span class="line">        Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])</span><br><span class="line"></span><br><span class="line">        state = next_state</span><br><span class="line">        action = next_action</span><br><span class="line"></span><br><span class="line"><span class="comment"># print result</span></span><br><span class="line"><span class="keyword">for</span> iinrange(<span class="number">4</span>):</span><br><span class="line"><span class="keyword">for</span> jinrange(<span class="number">4</span>):</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"State:"</span>, (i, j))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Up:"</span>, Q[i][j][<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Down:"</span>, Q[i][j][<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Left:"</span>, Q[i][j][<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Right:"</span>, Q[i][j][<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>()</span><br></pre></td></tr></tbody></table></figure><h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">State: (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">Up: -<span class="number">0.008042294056935573</span></span><br><span class="line">Down: -<span class="number">0.007868742418369764</span></span><br><span class="line">Left: -<span class="number">0.016173595452674966</span></span><br><span class="line">Right: <span class="number">0.006662566560762523</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">Up: <span class="number">0.048576025675988774</span></span><br><span class="line">Down: -<span class="number">0.0035842473161881465</span></span><br><span class="line">Left: <span class="number">0.024420015715567546</span></span><br><span class="line">Right: -<span class="number">0.46168987981312615</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">Up: <span class="number">0.04523751845081987</span></span><br><span class="line">Down: <span class="number">0.04266319340558091</span></span><br><span class="line">Left: <span class="number">0.044949583791193154</span></span><br><span class="line">Right: <span class="number">0.026234839551098416</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">Up: <span class="number">0.01629652821649763</span></span><br><span class="line">Down: <span class="number">0.050272192325180515</span></span><br><span class="line">Left: -<span class="number">0.009916869922464355</span></span><br><span class="line">Right: -<span class="number">0.4681667868865369</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">Up: -<span class="number">0.09991342319696966</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: <span class="number">0.0</span></span><br><span class="line">Right: <span class="number">0.036699099068340166</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">Up: <span class="number">0.008563965102313987</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: <span class="number">0.0</span></span><br><span class="line">Right: <span class="number">0.3883250678150607</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">Up: -<span class="number">0.3435187267522706</span></span><br><span class="line">Down: -<span class="number">0.2554776873673874</span></span><br><span class="line">Left: <span class="number">0.05651543121932354</span></span><br><span class="line">Right: <span class="number">0.004593450910446022</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">Up: -<span class="number">0.1</span></span><br><span class="line">Down: -<span class="number">0.013616634831997914</span></span><br><span class="line">Left: <span class="number">0.01298827764814053</span></span><br><span class="line">Right: <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">2</span>, <span class="number">0</span>)</span><br><span class="line">Up: <span class="number">0.28092113053540924</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: <span class="number">0.0024286388798406364</span></span><br><span class="line">Right: <span class="number">0.06302299434701504</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">Up: <span class="number">0.0</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: -<span class="number">0.16509175606504775</span></span><br><span class="line">Right: <span class="number">1.9146361697676122</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">Up: -<span class="number">0.1</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: <span class="number">0.03399106390140035</span></span><br><span class="line">Right: <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">Up: -<span class="number">0.3438668479533914</span></span><br><span class="line">Down: <span class="number">0.004696957810272524</span></span><br><span class="line">Left: -<span class="number">0.19</span></span><br><span class="line">Right: <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">3</span>, <span class="number">0</span>)</span><br><span class="line">Up: <span class="number">3.3060693607932445</span></span><br><span class="line">Down: <span class="number">0.8893977121867367</span></span><br><span class="line">Left: <span class="number">0.0</span></span><br><span class="line">Right: <span class="number">0.13715553550041798</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">Up: <span class="number">4.825854511712306</span></span><br><span class="line">Down: -<span class="number">0.03438123168566812</span></span><br><span class="line">Left: <span class="number">0.10867882029322147</span></span><br><span class="line">Right: <span class="number">1.0015572397722665</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">Up: <span class="number">5.875704328143301</span></span><br><span class="line">Down: <span class="number">0.9315770230698863</span></span><br><span class="line">Left: <span class="number">0.0006851481810742227</span></span><br><span class="line">Right: <span class="number">0.47794799892127526</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">Up: <span class="number">5.4028951599661275</span></span><br><span class="line">Down: <span class="number">2.6989177956329757</span></span><br><span class="line">Left: -<span class="number">0.6454474033238188</span></span><br><span class="line">Right: <span class="number">0.018474082554518417</span></span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://ucm14.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Reinforcement Learning" scheme="http://ucm14.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>1. Value iteration vs Policy iteration</title>
    <link href="http://ucm14.github.io/2024/10/02/1-Value-iteration-vs-Policy-iteration/"/>
    <id>http://ucm14.github.io/2024/10/02/1-Value-iteration-vs-Policy-iteration/</id>
    <published>2024-10-02T02:06:43.000Z</published>
    <updated>2024-10-02T06:17:15.413Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Markov-chains-transition-probability-matrix"><a href="#Markov-chains-transition-probability-matrix" class="headerlink" title="Markov chains &amp; transition probability matrix"></a>Markov chains &amp; transition probability matrix</h1><h2 id="Markov-chains"><a href="#Markov-chains" class="headerlink" title="Markov chains"></a>Markov chains</h2><p>In machine learning algorithms, <em><strong>Markov chains</strong></em> are widely applied in time series models. The main idea is that regardless of the initial state, as long as the state transition matrix remains unchanged, the final state will always converge to a fixed value. This memory-lessness is called the Markov property. The equation is as below:<br>$$<br>P(x_{t+1}‚à£‚Ä¶,x_{t‚àí2},x_{t‚àí1},x_{t})=P(x_{t+1}‚à£x_{t})\tag{1}<br>$$</p><h2 id="Transition-probability-matrix"><a href="#Transition-probability-matrix" class="headerlink" title="Transition probability matrix"></a>Transition probability matrix</h2><p>Each element of the matrix is represented by a probability. The values are non-negative, and the sum of the elements in each row equals 1. Under certain conditions, they can transition between each other, hence it is called a <em><strong>transition probability matrix</strong></em>. The two-step transition probability matrix is exactly the square of the one-step transition probability matrix. The <strong>$ùëò$</strong> step transition probability matrix is the <strong>$ùëò^{th}$</strong> power of the one-step transition probability matrix. In the <strong>$ùëò$</strong> step transition probability matrix, the sum of the elements in each row is also 1.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>For example, the value of $P(i,j)$ in matrix is $P(j|i)$, which is the probability of from state $i$ to state $j$. The transition probability matrix is as belowÔºö<br>$$<br>\begin{bmatrix} 0.9 &amp; 0.075 &amp; 0.025 \\ 0.15 &amp; 0.8 &amp; 0.05 \\ 0.25 &amp; 0.25 &amp; 0.5 \end{bmatrix}\tag{2}<br>$$</p><h3 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h3><p>Giving an initial state $P_{01}=[0.5,0.2,0.3]$ and $P_{02}=[0.1,0.4,0.5]$, the <strong>ùëò</strong> step transition is $P_0=P_0‚àóP_k$. $P_0$ can reach a stable value after multiple iterations. If k=30, please calculate the eventual result.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl</span><br><span class="line"></span><br><span class="line">p01 = np.array([<span class="number">0.5</span>, <span class="number">0.2</span>, <span class="number">0.3</span>]) </span><br><span class="line">p02 = np.array([<span class="number">0.1</span>,<span class="number">0.4</span>,<span class="number">0.5</span>]) </span><br><span class="line">p = np.array([[<span class="number">0.9</span>, <span class="number">0.075</span>, <span class="number">0.025</span>], [<span class="number">0.15</span>, <span class="number">0.8</span>, <span class="number">0.05</span>],[<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.5</span>]])</span><br><span class="line">n = <span class="number">30</span></span><br><span class="line">c = np.array([<span class="string">'r'</span>,<span class="string">'g'</span>,<span class="string">'b'</span>])       </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calanddraw</span>(<span class="params">p0,p </span>):</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">     p0 = np.mat(p0) * np.mat(p)       </span><br><span class="line">     <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(np.array(p0)[<span class="number">0</span>])):</span><br><span class="line">          pl.scatter(i,p0[<span class="number">0</span>,j], c = c[j], s=<span class="number">.5</span>)</span><br><span class="line">pl.subplot(<span class="number">121</span>)</span><br><span class="line">calanddraw(p01,p)</span><br><span class="line">pl.subplot(<span class="number">122</span>)</span><br><span class="line">calanddraw(p02,p)</span><br><span class="line">pl.show()</span><br></pre></td></tr></tbody></table></figure><p>From the figure below we can reach the conclusion that both states converge eventually and their value are close to each other.</p><p><img src="https://s2.loli.net/2024/08/01/I9NQp4icfJaZ52h.png" alt="image.png"></p><h1 id="Model-based-algorithms"><a href="#Model-based-algorithms" class="headerlink" title="Model-based algorithms"></a>Model-based algorithms</h1><h2 id="Value-iteration"><a href="#Value-iteration" class="headerlink" title="Value iteration"></a>Value iteration</h2><p>Unlike simply times initial state with transition probability matrix multiple times, <em><strong>value iteration</strong></em> is a commonly used <strong>dynamic planning (DP)</strong> method (<em>DP methods assume that we have a <strong>perfect model</strong> of the environment‚Äôs MDP. That‚Äôs usually not the case in practice, but it‚Äôs important to study DP anyway</em>), which is mainly used to solve the optimal strategy problem in Markov Decision Process (MDP). The core idea of the Value Iteration algorithm is to update the value function of the state iteratively, gradually approaching the optimal value function, so as to obtain the optimal policy. The main advantage of the value iteration algorithm is that it is simple and easy to implement, and it is applicable to various types of MDP problems. However, the main disadvantage of the value iteration algorithm is its <strong>high time complexity</strong>, especially when the state space is large. Therefore, in practical applications, the value iteration algorithm usually needs to be combined with other optimization techniques, such as dynamic programming optimization and parallel computing, to improve the computational efficiency. The equation of algorithm is as below.</p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>$$<br>V_{(k+1)}(s)=max_a[R(s,a)+Œ≥_{Œ£s^‚Ä≤‚ààS}P(s^‚Ä≤|s,a)V^k(s^‚Ä≤)]\tag{3}<br>$$</p><p>First, we start with a random value function $V(s)$. At each step, we update it. Hence, we look ahead one step and go over all possible actions at each iteration to find the maximum. Moreover, the only difference is that in the value iteration algorithm, we take the maximum number of possible actions. Instead of evaluating and then improving, the value iteration algorithm updates the state value function in a single step. In particular, this is possible by calculating all possible rewards by looking ahead. Finally, the value iteration algorithm is guaranteed to converge to the optimal values.</p><h3 id="Implementation-1"><a href="#Implementation-1" class="headerlink" title="Implementation"></a>Implementation</h3><p>This is the entire code of value iteration implementation. I will break down every line and explain.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">value_iteration</span>(<span class="params">states, actions, transition_prob, reward, discount_factor=<span class="number">0.9</span>, theta=<span class="number">1e-6</span></span>):</span><br><span class="line"></span><br><span class="line">    value_function = np.zeros(<span class="built_in">len</span>(states))</span><br><span class="line"></span><br><span class="line">    policy = np.zeros(<span class="built_in">len</span>(states), dtype=<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">            v = value_function[s]</span><br><span class="line">            action_values = np.zeros(<span class="built_in">len</span>(actions))</span><br><span class="line">            <span class="keyword">for</span> a <span class="keyword">in</span> actions:</span><br><span class="line">                action_value = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> next_s <span class="keyword">in</span> states:</span><br><span class="line">                    prob = transition_prob.get((s, a, next_s), <span class="number">0</span>)</span><br><span class="line">                    action_value += prob * (reward.get((s, a, next_s), <span class="number">0</span>) + discount_factor * value_function[next_s])</span><br><span class="line">                action_values[a] = action_value</span><br><span class="line">            value_function[s] = <span class="built_in">max</span>(action_values)</span><br><span class="line">            delta = <span class="built_in">max</span>(delta, <span class="built_in">abs</span>(v - value_function[s]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> delta &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">        action_values = np.zeros(<span class="built_in">len</span>(actions))</span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> actions:</span><br><span class="line">            action_value = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> next_s <span class="keyword">in</span> states:</span><br><span class="line">                prob = transition_prob.get((s, a, next_s), <span class="number">0</span>)</span><br><span class="line">                action_value += prob * (reward.get((s, a, next_s), <span class="number">0</span>) + discount_factor * value_function[next_s])</span><br><span class="line"></span><br><span class="line">            action_values[a] = action_value</span><br><span class="line">        policy[s] = np.argmax(action_values)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> policy, value_function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">states = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">actions = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">transition_prob = {</span><br><span class="line">    (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>): <span class="number">0.5</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>): <span class="number">0.5</span>,</span><br><span class="line">    (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>): <span class="number">0.2</span>, (<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>): <span class="number">0.8</span>,</span><br><span class="line">    (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>): <span class="number">0.7</span>, (<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>): <span class="number">0.3</span>,</span><br><span class="line">    (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>): <span class="number">0.6</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>): <span class="number">0.4</span>,</span><br><span class="line">    (<span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>): <span class="number">1.0</span>,</span><br><span class="line">    (<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>): <span class="number">0.5</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>): <span class="number">0.5</span>,</span><br><span class="line">}</span><br><span class="line">reward = {</span><br><span class="line">    (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>): <span class="number">1</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>): <span class="number">1</span>,</span><br><span class="line">    (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>): <span class="number">0</span>, (<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>): <span class="number">1</span>,</span><br><span class="line">    (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>): <span class="number">1</span>, (<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>): <span class="number">2</span>,</span><br><span class="line">    (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>): <span class="number">0</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>): <span class="number">3</span>,</span><br><span class="line">    (<span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>): <span class="number">0</span>,</span><br><span class="line">    (<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>): <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>): <span class="number">0</span>,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">policy, value_function = value_iteration(states, actions, transition_prob, reward)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Optimal Policy:"</span>, policy)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Value Function:"</span>, value_function)</span><br></pre></td></tr></tbody></table></figure><p>It will be much easier to understand the algorithm with a practice. So we can try to explain value iteration well by a simple implementation. We first import NumPy, of course. Then definite a function, which is the main character of this section: value iteration. This function requires several input values including states, actions, transition probability, reward, discount factor and theta. In this implementation, $Œ≥=0.9$,$$Œ∏=1^{‚àí6}$$.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">defvalue_iteration(states, actions, transition_prob, reward, discount_factor=<span class="number">0.9</span>, theta=<span class="number">1e-6</span>):</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">    Performs value iteration for a given MDP.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param states: List of states</span></span><br><span class="line"><span class="string">    :param actions: List of actions</span></span><br><span class="line"><span class="string">    :param transition_prob: A dictionary that maps (state, action, next_state) to the transition probability</span></span><br><span class="line"><span class="string">    :param reward: A dictionary that maps (state, action, next_state) to a reward</span></span><br><span class="line"><span class="string">    :param discount_factor: Discount factor (gamma)</span></span><br><span class="line"><span class="string">    :param theta: A threshold for convergence</span></span><br><span class="line"><span class="string">    :return: A tuple (policy, value_function)</span></span><br><span class="line"><span class="string">    """</span></span><br></pre></td></tr></tbody></table></figure><p>In this function, we need to create two arrays for the storage of possible state and policy. State is easy to understand, but what is ‚Äúpolicy‚Äù?</p><p>A policy is a strategy that an agent uses in pursuit of goals. The policy dictates the actions that the agent takes as a function of the agent‚Äôs state and the environment. In reinforcement learning, a policy is a strategy used by an agent to determine its actions at any given state. Formally, a policy is a mapping from states of the environment to actions to be taken when in those states. It can be deterministic or stochastic.</p><blockquote><p>Deterministic Policy: This type of policy maps each state to a specific action. If œÄ is a deterministic policy and s is a state, then œÄ(s) is the action taken when in state s.</p></blockquote><blockquote><p>Stochastic Policy: This type of policy provides a probability distribution over actions for each state. If œÄ is a stochastic policy and s is a state, then œÄ(a|s) represents the probability of taking action a when in state s.</p></blockquote><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">value_function = np.zeros(<span class="built_in">len</span>(states))</span><br><span class="line">   policy = np.zeros(<span class="built_in">len</span>(states), dtype=<span class="built_in">int</span>)</span><br></pre></td></tr></tbody></table></figure><p>Then we enter the iteration of value function. Before start iteration, we have to define a variable called Œî. It is a variable used to record the variation of value function in a iteration and can judge whether the iteration is converging. <code>for s in states</code> is a order asking agent to go through every state, <code>v = value_function[s]</code> is to store all the result of s in v. These are the pre-moves of iteration.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    delta = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">        v = value_function[s]</span><br><span class="line">        action_values = np.zeros(<span class="built_in">len</span>(actions))</span><br></pre></td></tr></tbody></table></figure><p>The result of value function comes from the maximum value of action values. We create an empty array for the storage of action values. Firstly initialize action value, then for each action, we go through every possible next state. After cumulating action value based on bellman function and store them as an array, we can obtain the maximum action value and take it as the value function value of current state.</p><p>In order to judge the convergency, update delta (Œî) at the end of value function calculation. <code>abs(v - value_function[s])</code> is the variation of value function. We determine the value of delta by comparing the value between previous Œî and new Œî and taking the maximum value. Cycle will ceased if the update range smaller than Œ∏.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">action_values = np.zeros(<span class="built_in">len</span>(actions))</span><br><span class="line">   <span class="keyword">for</span> a <span class="keyword">in</span> actions:</span><br><span class="line">        <span class="comment"># go through every possible action</span></span><br><span class="line">               action_value = <span class="number">0</span></span><br><span class="line">           <span class="comment"># initialize action value</span></span><br><span class="line">               <span class="keyword">for</span> next_s <span class="keyword">in</span> states:</span><br><span class="line">               <span class="comment"># go through every possible next state</span></span><br><span class="line">                   prob = transition_prob.get((s, a, next_s), <span class="number">0</span>)</span><br><span class="line">               <span class="comment"># achieve transition probability (0 if not exist)</span></span><br><span class="line">                   action_value += prob * (reward.get((s, a, next_s), <span class="number">0</span>) + discount_factor * value_function[next_s])</span><br><span class="line">               <span class="comment"># cumulative action value based on bellman function</span></span><br><span class="line">               action_values[a] = action_value</span><br><span class="line">               <span class="comment"># store result in array of action value</span></span><br><span class="line">           value_function[s] = <span class="built_in">max</span>(action_values)</span><br><span class="line">           <span class="comment"># update value fuction and obtain maximum value among all the possible action value</span></span><br><span class="line">           delta = <span class="built_in">max</span>(delta, <span class="built_in">abs</span>(v - value_function[s]))</span><br><span class="line">           <span class="comment"># update delta and record maximum variation</span></span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> delta &lt; theta:</span><br><span class="line">           <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure><p>Simultaneously, we try to find the optimal policy. Basically trying to find the action which can bring maximum action value in each state then form a sequence by combining these actions.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">   <span class="comment"># go through every state</span></span><br><span class="line">       action_values = np.zeros(<span class="built_in">len</span>(actions))</span><br><span class="line">       <span class="comment"># initialize array</span></span><br><span class="line">       <span class="keyword">for</span> a <span class="keyword">in</span> actions:</span><br><span class="line">       <span class="comment"># go through every possible action</span></span><br><span class="line">           action_value = <span class="number">0</span></span><br><span class="line">           <span class="comment"># initialize current action value</span></span><br><span class="line">           <span class="keyword">for</span> next_s <span class="keyword">in</span> states:</span><br><span class="line">               prob = transition_prob.get((s, a, next_s), <span class="number">0</span>)</span><br><span class="line">               action_value += prob * (reward.get((s, a, next_s), <span class="number">0</span>) + discount_factor * value_function[next_s])</span><br><span class="line">               <span class="comment"># obtain current state, reward of taking action to next state &amp; calculate discounted future value</span></span><br><span class="line">           action_values[a] = action_value</span><br><span class="line">       policy[s] = np.argmax(action_values)</span><br><span class="line">       <span class="comment"># return optimal policy</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> policy, value_function</span><br></pre></td></tr></tbody></table></figure><p>Here is the exercise. In this virtual environment, we can observe three states: 0, 1, 2 and two actions: 0 and 1. The corresponding transition probability and reward table is as below. In this exercise, transition probability means the state have the probability of remaining at the same state. For example, if we are at state 0 and take action 0, we have 50% probability stay at the same state. In addition, (1, 0, 0): 0.7 means if we at state 1 and take action 0, we can have 70% probability return to state 0 ; (2, 1, 1): 0.5 means if we at state 2 and take action 1, we can have 50% probability return to state 1.</p><table><thead><tr><th>scenario</th><th>state</th><th>action</th><th>next state</th><th>transition probability</th><th>reward</th></tr></thead><tbody><tr><td>a11</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>1</td></tr><tr><td>a12</td><td>0</td><td>0</td><td>1</td><td>0.5</td><td>1</td></tr><tr><td>a21</td><td>0</td><td>1</td><td>0</td><td>0.2</td><td>0</td></tr><tr><td>a22</td><td>0</td><td>1</td><td>1</td><td>0.8</td><td>1</td></tr><tr><td>b11</td><td>1</td><td>0</td><td>0</td><td>0.7</td><td>1</td></tr><tr><td>b12</td><td>1</td><td>0</td><td>2</td><td>0.3</td><td>2</td></tr><tr><td>b21</td><td>1</td><td>1</td><td>1</td><td>0.6</td><td>0</td></tr><tr><td>b22</td><td>1</td><td>1</td><td>2</td><td>0.4</td><td>3</td></tr><tr><td>c11</td><td>2</td><td>0</td><td>2</td><td>1.0</td><td>0</td></tr><tr><td>c21</td><td>2</td><td>1</td><td>1</td><td>0.5</td><td>1</td></tr><tr><td>c22</td><td>2</td><td>1</td><td>2</td><td>0.5</td><td>0.5</td></tr></tbody></table><p><img src="https://s2.loli.net/2024/08/02/hm2qLdJyUbg6ZN3.png" alt="image.png"></p><center>Diagram of state, action and transition</center><p>Since we have complete all the requisite, we can run the code and get the result.</p><p><strong>Optimal Policy: 0‚Üí0‚Üí1</strong></p><p><strong>Value Function: [10.16927311, 10.20689125, 9.26018305]</strong></p><h2 id="Policy-iteration"><a href="#Policy-iteration" class="headerlink" title="Policy iteration"></a>Policy iteration</h2><p>The other common way that MDPs are solved is using <strong>policy iteration</strong> ‚Äì an approach that is similar to value iteration. While value iteration iterates over value functions, policy iteration iterates over policies themselves, creating a strictly improved policy in each iteration (except if the iterated policy is already optimal).</p><p>Policy iteration first starts with some (non-optimal) policy, such as a random policy, and then calculates the value of each state of the MDP given that policy ‚Äî this step is called <strong>policy evaluation</strong>. It then updates the policy itself for every state by calculating the expected reward of each action applicable from that state.</p><p>The basic idea here is that policy evaluation is easier to computer than value iteration because the set of actions to consider is fixed by the policy that we have so far.</p><h3 id="Policy-evaluation"><a href="#Policy-evaluation" class="headerlink" title="Policy evaluation"></a>Policy evaluation</h3><p><strong>policy evaluation</strong> is an evaluation of the expected reward of a policy. VœÄ(s), the expected reward of policy œÄ from state s, is the weighted average of reward of the possible state sequences defined by that policy times their probability given œÄ. Policy evaluation can be defined by the following equation:<br>$$<br> V^œÄ(s)=‚àëP_{œÄ(s)}(s^‚Ä≤|s)[r(s,a,s^‚Ä≤)+Œ≥V^œÄ(s^‚Ä≤)]\tag{4}<br>$$<br>Where $V^œÄ(s)=0$ is for terminal state. Note that this is very similar to the Bellman equation, except $V^œÄ(s)$ is not the value of the best action, but instead just as the value for $œÄ(s)$, the action that would be chosen in s by the policy $œÄ$. Note the expression $P_{œÄ(s)}(s^‚Ä≤‚à£s)$ instead of $P_a(s^‚Ä≤‚à£s)$, which means we only evaluate the action that the policy defines.</p><h3 id="Policy-improvement"><a href="#Policy-improvement" class="headerlink" title="Policy improvement"></a>Policy improvement</h3><p>If we have a policy and we want to improve it, we can use <strong>policy improvement</strong> to change the policy (that is, change the actions recommended for states) by updating the actions it recommends based on $V(s)$ that we receive from the policy evaluation.</p><p>Let QœÄ(s,a) be the expected reward from s when doing a first and then following the policy œÄ. Recall from the chapter on Markov Decision Processes that we define define this as:<br>$$<br>Q_œÄ(s,a)=‚àëP_a(s^‚Ä≤|s)[r(s,a,s^‚Ä≤)+Œ≥V^œÄ(s^‚Ä≤)\tag{5}<br>$$<br>If there is an action a make $Q_œÄ(s,a)&gt;Q_œÄ(s,œÄ(s))$, then the policy œÄ can be <strong>strictly improved</strong> by setting $œÄ(s)‚Üêa$. This will improve the overall policy.</p><h3 id="Implementation-2"><a href="#Implementation-2" class="headerlink" title="Implementation"></a>Implementation</h3><p>In this implementation, we have two states and 2 actions. reward during iteration can be 0 or 1. Discount factor $Œ≥=0.9$.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">states = [<span class="string">'1'</span>, <span class="string">'2'</span>]</span><br><span class="line">actions = [<span class="string">'a'</span>, <span class="string">'b'</span>]</span><br><span class="line">rewards = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">discount_factor = <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line">q_value = {states[<span class="number">0</span>]: {actions[<span class="number">0</span>]: <span class="number">0</span>, actions[<span class="number">1</span>]: <span class="number">0</span>}, states[<span class="number">1</span>]: {actions[<span class="number">0</span>]: <span class="number">0</span>, actions[<span class="number">1</span>]: <span class="number">0</span>}} </span><br><span class="line"></span><br><span class="line">pi = {states[<span class="number">0</span>]: {actions[<span class="number">0</span>]: <span class="number">0.5</span>, actions[<span class="number">1</span>]: <span class="number">0.5</span>}, states[<span class="number">1</span>]: {actions[<span class="number">0</span>]: <span class="number">0.5</span>, actions[<span class="number">1</span>]: <span class="number">0.5</span>}}</span><br></pre></td></tr></tbody></table></figure><p>Code shown below is the transition probability and reward. We can visually obtain information through table below.</p><table><thead><tr><th>Scenario</th><th>next state</th><th>transition probability</th><th>reward</th></tr></thead><tbody><tr><td>1a1</td><td>1</td><td>1/3</td><td>0</td></tr><tr><td>1a2</td><td>2</td><td>2/3</td><td>1</td></tr><tr><td>1b1</td><td>1</td><td>2/3</td><td>0</td></tr><tr><td>1b2</td><td>2</td><td>1/3</td><td>1</td></tr><tr><td>2a1</td><td>1</td><td>1/3</td><td>0</td></tr><tr><td>2a2</td><td>2</td><td>2/3</td><td>1</td></tr><tr><td>2b1</td><td>1</td><td>2/3</td><td>0</td></tr><tr><td>2b2</td><td>2</td><td>1/3</td><td>1</td></tr></tbody></table><p>It will be more clearer when explaining the meaning through one of the scenarios. For example, we are at state 1, if we take action $a$ , we have a $\frac{2}{3}$ probability of reaching the next state and receive reward = 0 or $\frac 1 3$ probability of staying at the same state and receive reward = 1.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">p_s_r</span>(<span class="params">state, action</span>):</span><br><span class="line">    <span class="keyword">if</span> state == <span class="string">"1"</span>:</span><br><span class="line">        <span class="keyword">if</span> action == <span class="string">"a"</span>:</span><br><span class="line">            <span class="keyword">return</span> ((<span class="number">1.0</span> / <span class="number">3</span>, <span class="string">"1"</span>, <span class="number">0</span>),</span><br><span class="line">                    (<span class="number">2.0</span> / <span class="number">3</span>, <span class="string">"2"</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> ((<span class="number">2.0</span> / <span class="number">3</span>, <span class="string">"1"</span>, <span class="number">0</span>),</span><br><span class="line">                    (<span class="number">1.0</span> / <span class="number">3</span>, <span class="string">"2"</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">if</span> state == <span class="string">"2"</span>:</span><br><span class="line">        <span class="keyword">if</span> action == <span class="string">"a"</span>:</span><br><span class="line">            <span class="keyword">return</span> ((<span class="number">1.0</span> / <span class="number">3</span>, <span class="string">"1"</span>, <span class="number">0</span>),</span><br><span class="line">                    (<span class="number">2.0</span> / <span class="number">3</span>, <span class="string">"2"</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> ((<span class="number">2.0</span> / <span class="number">3</span>, <span class="string">"1"</span>, <span class="number">0</span>),</span><br><span class="line">                    (<span class="number">1.0</span> / <span class="number">3</span>, <span class="string">"2"</span>, <span class="number">1</span>))</span><br></pre></td></tr></tbody></table></figure><p>Next part is state evaluation function. In this function, we initialize the function and define the value of Œ∏. <code>deepcopy</code> means to copy current V value to old V value for judgement of convergence. Then, assemble to value iteration, we go through every possible action in every state and obtain corresponding transition probability and reward. After that, we calculate Q value (action value) and V value. If the variation of V value smaller than Œ∏, we can conclude that the function is converged and break the cycle.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">policy_evaluate</span>():</span><br><span class="line">    v_value = {states[<span class="number">0</span>]: <span class="number">0</span>, states[<span class="number">1</span>]: <span class="number">0</span>}</span><br><span class="line">    threshold = <span class="number">0.0001</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        v_value_old = copy.deepcopy(v_value)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">            temp_v = <span class="number">0</span></span><br><span class="line"><span class="comment"># temp_q = 0</span></span><br><span class="line">            <span class="keyword">for</span> a, p <span class="keyword">in</span> pi[s].items():</span><br><span class="line">                temp_q = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> t <span class="keyword">in</span> p_s_r(s, a):</span><br><span class="line">                    p_s_s1, s_, r = t[<span class="number">0</span>], t[<span class="number">1</span>], t[<span class="number">2</span>]</span><br><span class="line">                    temp_q += p_s_s1 * (r + discount_factor * v_value[s_])</span><br><span class="line">                q_value[s][a] = temp_q</span><br><span class="line">                temp_v += p * temp_q</span><br><span class="line">            v_value[s] = temp_v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(v_value)):</span><br><span class="line">             delta += np.<span class="built_in">abs</span>(v_value[states[i]] - v_value_old[states[i]])</span><br><span class="line">        <span class="keyword">if</span> delta &lt;= threshold:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> v_value</span><br></pre></td></tr></tbody></table></figure><p>Next, we update policy based on the V value we got. Before we enter the cycle, we have to clarify what is <code>done = True</code> . <code>done = True</code> means cease the operation when the policy becomes stable.</p><p>The function, <code>policy_improve</code> is used to find out the optimal action in current state and turn its probability of been selected to 1. Meanwhile, the probability of choosing other actions will be 0.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">policy_improve</span>(<span class="params">v</span>):</span><br><span class="line">    done = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">        action = <span class="built_in">max</span>(q_value[s],key=q_value[s].get)</span><br><span class="line">    <span class="comment"># print(action)</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> pi[s]:</span><br><span class="line">            <span class="keyword">if</span> k == action:</span><br><span class="line">                <span class="keyword">if</span> pi[s][k] != <span class="number">1.0</span>:</span><br><span class="line">                    pi[s][k] = <span class="number">1.0</span></span><br><span class="line">                    done = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pi[s][k] = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">return</span> done</span><br></pre></td></tr></tbody></table></figure><p>The iteration in this implementation is quite simple - only 2 times. Activate the code we can get the results shown as below:</p><table><thead><tr><th>State</th><th>1</th><th>2</th></tr></thead><tbody><tr><td>V value</td><td>6.666339540673712</td><td>6.666353680224912</td></tr></tbody></table><table><thead><tr><th>Q value</th><th>State 1</th><th>State 2</th></tr></thead><tbody><tr><td>action a</td><td>6.666339540673712</td><td>6.666353680224912</td></tr><tr><td>action b</td><td>6.333001354313227</td><td>6.333029633415626</td></tr></tbody></table><table><thead><tr><th>State</th><th>1</th><th>2</th></tr></thead><tbody><tr><td>Policy</td><td>a = 1, b = 0</td><td>a = 1, b = 0</td></tr></tbody></table><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">'__main__'</span>:</span><br><span class="line">    is_done =<span class="literal">False</span></span><br><span class="line">    i =<span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> is_doneisFalse:</span><br><span class="line">        v = policy_evaluate()</span><br><span class="line">        is_done = policy_improve(v)</span><br><span class="line">        i +=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Policy-Iteration converged at step %d.'</span> % i)</span><br><span class="line"><span class="built_in">print</span>(v)</span><br><span class="line"><span class="built_in">print</span>(q_value)</span><br><span class="line"><span class="built_in">print</span>(pi)</span><br></pre></td></tr></tbody></table></figure><h2 id="Difference-between-value-iteration-and-policy-iteration"><a href="#Difference-between-value-iteration-and-policy-iteration" class="headerlink" title="Difference between value iteration and policy iteration"></a>Difference between value iteration and policy iteration</h2><table><thead><tr><th>Aspect</th><th>Value iteration</th><th>Policy iteration</th></tr></thead><tbody><tr><td>Methodology</td><td>Iteratively updates value functions until convergence</td><td>Alternates between policy evaluation and improvement</td></tr><tr><td>Goal</td><td>Converges to optimal value function</td><td>Converges to the optimal policy</td></tr><tr><td>Execution</td><td>Directly computes value functions</td><td>Evaluate and improve policies sequentially</td></tr><tr><td>Complexity</td><td>Typically simpler to implement and understand</td><td>Involves more steps and computations</td></tr><tr><td>Convergence</td><td>May converge faster in some scenarios</td><td>Generally converges slower but yields better policies</td></tr></tbody></table><p>In summary, both value iteration and policy iteration are effective methods for solving RL problems and deriving optimal policies. Value iteration directly computes optimal value functions iteratively, which can converge faster in some cases and is generally simpler to implement. On the other hand, policy iteration alternates between evaluating and improving policies, resulting in slower convergence but potentially yielding better policies overall. Understanding the differences between these approaches is crucial for selecting the most suitable algorithm based on the problem requirements and computational constraints.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://ucm14.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Reinforcement Learning" scheme="http://ucm14.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Mason&#39;s Radio#2</title>
    <link href="http://ucm14.github.io/2024/10/01/Mason-s-Radio-2/"/>
    <id>http://ucm14.github.io/2024/10/01/Mason-s-Radio-2/</id>
    <published>2024-10-01T15:07:04.000Z</published>
    <updated>2024-10-01T16:11:46.605Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p> <font size="8"> <strong>The greatest album in 1999.</strong></font></p></blockquote><iframe style="border-radius:12px" src="https://open.spotify.com/embed/album/29U9LtzSF0ftWiLNNw1CP6?utm_source=generator" width="100%" height="480" frameborder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe><div style="text-align: center;">    <iframe width="560" height="315" src="https://www.youtube.com/embed/o1sUaVJUeB0?si=nMqpRoT2_Hzlc9xY&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></div><center> "First Love" Music Video (Live Ver.) <center><div style="text-align: center;">    <iframe width="560" height="315" src="https://www.youtube.com/embed/-9DxpPiE458?si=zU7CIfKNUHOi2-MO&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></div><center> "Automatic" Music Video <center></center></center></center></center>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Music" scheme="http://ucm14.github.io/categories/Music/"/>
    
    
    <category term="R&amp;B" scheme="http://ucm14.github.io/tags/R-B/"/>
    
    <category term="Utada Hikaru" scheme="http://ucm14.github.io/tags/Utada-Hikaru/"/>
    
    <category term="J-POP" scheme="http://ucm14.github.io/tags/J-POP/"/>
    
  </entry>
  
  <entry>
    <title>National Day holiday</title>
    <link href="http://ucm14.github.io/2024/10/01/National-Day-holiday/"/>
    <id>http://ucm14.github.io/2024/10/01/National-Day-holiday/</id>
    <published>2024-10-01T12:36:31.000Z</published>
    <updated>2024-10-01T14:02:42.820Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>It is the first day of National Day holiday in 2024. I made a regrettable decision: go out. I was planned to buy some new clothes for autumn and winter since the climate had dropped drastically after a heavy rain. Due to multiple times of unpleasant online shopping experiences, I decided to try those clothes on in a physical store.</p><p>The crowd already made me feel dizzy when I exit the subway. To be honest, it‚Äôs been a long time since I‚Äôve seen so many people crowded in the subway station. Last time I saw a resemble scenery was in 30 the April when I on my way home at the train station, I suddenly felt exhausted at the moment stepping into the railway station. I extremely dislike such environment which is too noisy and irritating.</p><p align="center"><img src="https://s2.loli.net/2024/10/01/gDZbYCABLt2uWJV.jpg" width="600"><img src="https://s2.loli.net/2024/10/01/M9Fq7E1j2ADfuNS.jpg" width="600"></p><p>Then I fleet the station as soon as possible and straightly walked to the store. It turned out to be another horrible place because the store is on sale, thus more people crowded into the store to search for items and try them on, leaving what was once neatly organized in complete disarray. So I left again, trying to escape the heat and noise brought by the crowds. Of course, I didn‚Äôt buy anything, not only because the crowd but also the price is still high after giving discount.</p><p>The only place can bring me joy is the burger shop called ‚ÄúEgg Soul‚Äù. It was established in 2018 when I first came to Nanjing. I really love this burger shop and have came to this places many times that the money I spent here brought me a VIP discount. The vibe is the most iconic characteristics of itself. Colorful and dynamic graffiti on the wall, 90‚Äôs R&amp;B and Hip-Hop music together with aroma of food filling the space. The beef sandwich set is one of my favorites. Only a cup of diet coke, some French fries and a medium beef sandwich with green pepper, olive, beef and cheese. Terrible day, but thanks to Egg Soul I had a great supper.</p><p align="center"><img src="https://s2.loli.net/2024/10/01/z8YgJW3UMP7v4qN.jpg" width="600/"></p><p>Next time I will stay in my dorm and go nowhere, I swear. </p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Misc" scheme="http://ucm14.github.io/categories/Misc/"/>
    
    
    <category term="Daily Life" scheme="http://ucm14.github.io/tags/Daily-Life/"/>
    
  </entry>
  
  <entry>
    <title>First TOEFL result</title>
    <link href="http://ucm14.github.io/2024/09/29/First-TOEFL-result/"/>
    <id>http://ucm14.github.io/2024/09/29/First-TOEFL-result/</id>
    <published>2024-09-29T06:05:13.000Z</published>
    <updated>2024-09-29T07:21:13.958Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Assessment"><a href="#Assessment" class="headerlink" title="Assessment"></a>Assessment</h1><p><img src="https://s2.loli.net/2024/09/29/g9Li3zRQnyTdJwI.png" alt="TOEFL result.png"></p><p>I checked the official website this morning and found my latest, actually my first TOEFL test result had published. The physical transcript hasn‚Äôt arrived so I put the screenshot of my result here. I got 29/30 in reading, 22/30 in listening, 23/30 in speaking and 20/30 in writing. Not bad but still a pity.</p><h2 id="Reading-task"><a href="#Reading-task" class="headerlink" title="Reading task"></a>Reading task</h2><p>In reading task, the first passage talking about the features of some ethnic groups who feed on hunting. For example, most of them lived near river, which can provide them fish and other kind of aquatics. But still, they cultivate graves in case the lack of food in a long journey. The second passage is how astronomers research on moonquakes.  Both passages are simple to me, I can quickly locate the answers of the questions, let alone answering the synonyms. </p><h2 id="Listening-task"><a href="#Listening-task" class="headerlink" title="Listening task"></a>Listening task</h2><p>I bring the confidence reading task gave me to listening task. It is no exaggeration that I can understand every words in the audio , and I practiced a lot and keep 100% concentrated on the audio during the test. However, I just can‚Äôt find the right answer. I felt appalling when seeing my listening task only got 22 points, which is far away from what I thought. Maybe I should practice more.</p><h2 id="Speaking-task"><a href="#Speaking-task" class="headerlink" title="Speaking task"></a>Speaking task</h2><p>The result of speaking task surprised me a lot, more than listening task did because I didn‚Äôt prepare some templates for questions, and I was bothered by others during the test. No excuses, some of the ladies in the room talking like the Kardashians. I totally can‚Äôt understand why they prefer to be sound like Cali girls.</p><h2 id="Writing-task"><a href="#Writing-task" class="headerlink" title="Writing task"></a>Writing task</h2><p>I searched for templates of integrated writing and academic discussion and try to memorize them before the test began. On the contrary, I used none of them when writing the essay. The integrated writing is to summarize a discussion about the decline of  Chinook salmon. The academic discussion is asking whether you support advertising based on private information or not. I tried to make my points clear in the essay but 20 points is lower than what I supposed to have. I wonder how to improve my writing skills, probably some complex-and-long sentences and advanced vocabularies which no one can recognize.</p><table><thead><tr><th>Skill</th><th align="left">Level</th></tr></thead><tbody><tr><td>Reading</td><td align="left">Advanced (24‚Äì30) High-Intermediate (18‚Äì23) Low-Intermediate (4‚Äì17) Below Low-Intermediate (0‚Äì3)</td></tr><tr><td>Listening</td><td align="left">Advanced (22‚Äì30) High-Intermediate (17‚Äì21) Low-Intermediate (9‚Äì16) Below Low-Intermediate (0‚Äì8)</td></tr><tr><td>Speaking</td><td align="left">Advanced (25‚Äì30) High-Intermediate (20‚Äì24) Low-Intermediate (16‚Äì19) Basic (10‚Äì15) Below Basic (0‚Äì9)</td></tr><tr><td>Writing</td><td align="left">Advanced (24‚Äì30) High-Intermediate (17‚Äì23) Low-Intermediate (13‚Äì16) Basic (7‚Äì12) Below Basic (0‚Äì6)</td></tr></tbody></table><p><a href="https://www.ets.org/pdfs/toefl/toefl-ibt-performance-descriptors.pdf">https://www.ets.org/pdfs/toefl/toefl-ibt-performance-descriptors.pdf</a></p><p>Based on the descriptors provided by IBT, my reading skill is advanced, then come with high-intermediate listening/speaking/writing skills. But I don‚Äôt admit that can reflect my true English capability.</p><h1 id="Reflection"><a href="#Reflection" class="headerlink" title="Reflection"></a>Reflection</h1><p>After all, TOEFL is just a test. Of course it can reflect partial English ability of examinees, but we Chinese know hot to cope with test better than learn a language well. Those institutes teach students how to find shortcuts in exams. Nearly every kind of question has its corresponding answer template. I remembered a girl talked to his dad that she perfectly anticipated the writing task question. In addition, I found someone took a picture of the task shown in the screen and  uploaded to social media during the test with a  monitor hanging on everyone‚Äôs head. These made me wondering whether the test can identify people‚Äôs linguistic ability or only a certificate for study abroad. Also I found many examinees are born in 2007-2008, which means they decided to study in foreign universities. Do you think they will speak English more frequently? No, they won‚Äôt. Most of them still hanging with Chinese and only use English in email or shopping. The ultimate goal of the journey is for a foreign university certificate. Now back to the TOEFL test, do they really learn the English well? No one can give a determined answer. They probably can have a great marks in test, but be lame when talking in real-life.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Language" scheme="http://ucm14.github.io/categories/Language/"/>
    
    
    <category term="TOEFL" scheme="http://ucm14.github.io/tags/TOEFL/"/>
    
  </entry>
  
  <entry>
    <title>Mason&#39;s Radio#1</title>
    <link href="http://ucm14.github.io/2024/09/28/Mason-s-Radio-1/"/>
    <id>http://ucm14.github.io/2024/09/28/Mason-s-Radio-1/</id>
    <published>2024-09-28T04:53:26.000Z</published>
    <updated>2024-10-01T15:21:19.211Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/3v5o91PrUtf0nmO6j8J7dZ?utm_source=generator" width="100%" height="152" frameborder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe><blockquote><p><font size="6"> <strong>Best R&amp;B music and stage performance provided by XG.</strong></font></p></blockquote><div style="text-align: center;">    <iframe width="560" height="315" src="https://www.youtube.com/embed/18fe5rgmvYI?si=Vkr_KWsgwW5107tI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Music" scheme="http://ucm14.github.io/categories/Music/"/>
    
    
    <category term="XG" scheme="http://ucm14.github.io/tags/XG/"/>
    
    <category term="R&amp;B" scheme="http://ucm14.github.io/tags/R-B/"/>
    
  </entry>
  
  <entry>
    <title>Speaking Test</title>
    <link href="http://ucm14.github.io/2024/09/28/speaking-test/"/>
    <id>http://ucm14.github.io/2024/09/28/speaking-test/</id>
    <published>2024-09-28T03:10:54.000Z</published>
    <updated>2024-09-28T03:11:27.482Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="TOEFLÂè£ËØ≠ÊÄªÁªì"><a href="#TOEFLÂè£ËØ≠ÊÄªÁªì" class="headerlink" title="TOEFLÂè£ËØ≠ÊÄªÁªì"></a><strong>TOEFLÂè£ËØ≠ÊÄªÁªì</strong></h1><h2 id="Ê≥®ÊÑè"><a href="#Ê≥®ÊÑè" class="headerlink" title="Ê≥®ÊÑè"></a><strong>Ê≥®ÊÑè</strong></h2><ul><li>ËÄÉÂØüÊ†áÂáÜ<ol><li>ÂèëÈü≥„ÄÅËØ≠Ë∞ÉÂíåÊµÅÁïÖÂ∫¶„ÄÇ‰ΩøÁî®ËøûÊé•ËØçÔºå<em>well, I think, what I am going to talk about is, to make it clear.</em> <strong>ËøáÂ§öÂç°È°ø‰ºöÊâ£ÂàÜÁöÑ</strong>„ÄÇ</li><li>ËØ≠Ë®Ä‰ΩøÁî®ÔºåÂåÖÊã¨ËØçÊ±áÂíåËØ≠Ê≥ïËÉΩÂäõ„ÄÇ</li><li>topic developmentÔºåÈôàËø∞ËØùÈ¢òÁöÑËÉΩÂäõ„ÄÇ</li></ol></li><li>ËßÑÂÆöÊó∂Èó¥Ê≤°ËØ¥ÂÆå‰πüËÆ∏Âπ∂Ê≤°ÊúâÂÖ≥Á≥ªÔºå‰ΩÜ‰∏äÈù¢ËØ¥ÁöÑÂç°È°øÈóÆÈ¢òËøòÊòØÊúâÂΩ±ÂìçÁöÑ„ÄÇ</li><li>ËøòÊòØÂæóÂáÜÂ§áÊÆµÂ≠ê„ÄÇ</li><li>Âº†Ê∂µ‰π¶ÈáåÊèêÂà∞ÁªºÂêàÂè£ËØ≠‰∏∫‰ΩìÁé∞ÁªºÂêàÔºåÊúÄÂ•Ω‰∏çË¶ÅÂº∫Ë∞É<em>In the reading/listening passage</em>ËøôÁßçËØùÔºõ‰ΩÜÂÖ∂ÂÆûÂ∫îËØ•ÂÖ≥Á≥ª‰∏çÂ§ßÔºåÂèÇËÄÉÁü•‰πéËøô‰∏™ÈóÆÈ¢ò‰∏ãÁöÑÂõûÁ≠îÔºö<a href="https://www.zhihu.com/question/22102496">Â¶Ç‰ΩïÂáÜÂ§áÊâòÁ¶èÂè£ËØ≠È¢ò</a>„ÄÇÊÄª‰πãÂ∞±ÊòØËøô‰∫õÈÉΩÊòØÁªÜËäÇÔºåÂÉèÊàë‰ª¨ËøòÊòØÂ∞ΩÈáèÂÖàÊäìbig pictureÂêß„ÄÇ</li><li>ÁΩë‰∏äÁúãÁöÑÁªèÈ™åÂ§ßÂ§öÊòØÊ†πÊçÆ‰∏™‰∫∫ÁªèÂéÜÊÄªÁªìÁöÑÔºàÈô§‰∫ÜÂº†Ê∂µÊòØÁúüÊ≠£Âú®ETSÂÆû‰π†Ëøá‚Ä¶‚Ä¶ÔºâÔºåÂèØËÉΩ‰ªñÁî®ÊñπÊ≥ïAÔºåÊïàÊûúÂ•ΩÔºå‰ΩÜ‰ªñÁªèÈ™åÂàÜ‰∫´ÈáåÂ∞±ÂÜôË¶ÅÂùöÂÜ≥ÈÅøÂÖç‰ΩøÁî®ÊñπÊ≥ïBÔºåÂ∞±ÂèØËÉΩÊ≤°‰ªÄ‰πà‰æùÊçÆ‰∫Ü„ÄÇËøòÊòØÂæóËá™Â∑±ÁîÑÂà´„ÄÇ</li></ul><h2 id="Áã¨Á´ãÂè£ËØ≠"><a href="#Áã¨Á´ãÂè£ËØ≠" class="headerlink" title="Áã¨Á´ãÂè£ËØ≠"></a><strong>Áã¨Á´ãÂè£ËØ≠</strong></h2><blockquote><p>For both below:</p><p>preparation time: 15 sec</p><p>response time: 45 sec</p></blockquote><h3 id="Á¨¨‰∏ÄÈ¢òÔºö‰∏™‰∫∫ÂÅèÂ•Ω"><a href="#Á¨¨‰∏ÄÈ¢òÔºö‰∏™‰∫∫ÂÅèÂ•Ω" class="headerlink" title="Á¨¨‰∏ÄÈ¢òÔºö‰∏™‰∫∫ÂÅèÂ•Ω"></a><strong>Á¨¨‰∏ÄÈ¢òÔºö‰∏™‰∫∫ÂÅèÂ•Ω</strong></h3><blockquote><p>Express and defend a personal choice from a given category.</p></blockquote><p>‰∏ªË¶ÅÂõõÁ±ªÔºö<em>Person„ÄÅPlace„ÄÅObject„ÄÅEvent/Activity</em>„ÄÇ</p><p>‰πüÂèØËÉΩÊòØ‰∏âÈÄâ‰∏ÄÔºå‰ªéÈ¢òÁõÆÁªôÂá∫ÁöÑ‰∏â‰∏™ÈÄâÈ°π‰∏≠ÈÄâÂá∫‰Ω†ËÆ§‰∏∫ÊúÄÂ•ΩÁöÑÈÇ£‰∏Ä‰∏™Ôºå‰ΩÜÂá∫Áé∞Ê¶ÇÁéáÊõ¥‰Ωé„ÄÇ</p><p><strong>Ëß£È¢òÊñπÊ≥ï</strong>Ôºö<em>Topic sentence - Supporting sentence - Examples/details</em>„ÄÇ</p><p><strong>‰æãÂ≠ê</strong>Ôºö</p><blockquote><p>È¢òÁõÆÔºöTalk about a person you admire a lot and explain why you admire him or her. Use details and examples in your response.</p><p><strong>Topic Sentence ‰∏ªÈ¢òÂè•</strong> (Áõ¥Êé•ÁªôÂá∫ÂõûÁ≠îÔºâ: Allen Iverson, the NBA superstar, is definitely one of the people for whom I have a huge admiration.</p><p><strong>Supporting Sentence ÊîØÊåÅÂè•</strong>ÔºàÁªôÂá∫ÊîØÊåÅ‰∏ªÈ¢òÂè•ÁöÑÂéüÂõ†Ôºâ: I admire him a lot because he is such a hard-working guy that you would feel like there‚Äôs nothing he cannot do.</p><p><strong>Example/details ‰æãÂ≠ê / ÁªÜËäÇ</strong>Ôºà‰∏∫Ââç‰∏§‰∏™Âè•Â≠êÊèê‰æõÊîØÊíëÔºâ : Once I watched an interview of his coach in high school on NBC. He said that Allen was just super diligent. He was always the first person that arrived for the training, and always the last one to leave. He usually stayed for another 2 hours after all his teammates left for dinner. So it‚Äôs definitely his hard work that made him one of the most phenomenal players in the league.</p></blockquote><p><strong>Ê≥®ÊÑè</strong>Ôºö</p><ol><li>supporting sentenceÈáåÁöÑËßÇÁÇπÊúÄÂ•ΩÂè™Êúâ‰∏Ä‰∏™„ÄÇÂ§ö‰∫ÜËÆ≤‰∏çÂÆåÔºåËÆ≤‰∏çÂ•Ω„ÄÇ</li><li>Â∞ΩÈáè‰ΩøÁî®ÁÜüÊÇâË°®Ëææ„ÄÇÊÉ≥Ê±ÇÈöæÂèØËÉΩÂèçËÄåËØ¥‰∏çÂ•Ω„ÄÇ</li><li>Êó∂Èó¥ÊéßÂà∂Âú®42-45s‰πãÈó¥„ÄÇÂ∞ë‰∫é42sÂèØËÉΩÂØπÂàÜÊï∞ÊúâÂΩ±Âìç„ÄÇ</li></ol><h3 id="Á¨¨‰∫åÈ¢òÔºöÈÄâÊã©"><a href="#Á¨¨‰∫åÈ¢òÔºöÈÄâÊã©" class="headerlink" title="Á¨¨‰∫åÈ¢òÔºöÈÄâÊã©"></a><strong>Á¨¨‰∫åÈ¢òÔºöÈÄâÊã©</strong></h3><blockquote><p>Make and defend a personal choice between two contrasting behaviors or courses of action.</p></blockquote><p>È¢òÂûã‰∏∫<strong>‰∫åÈÄâ‰∏Ä</strong>Êàñ<strong>ÂêåÊÑèorÂèçÂØπ</strong>„ÄÇÂâçËÄÖÊØî‰æãÊõ¥È´ò„ÄÇ</p><p><strong>Ëß£È¢òÊñπÊ≥ï</strong>Ôºö‰∏éÁ¨¨‰∏ÄÈ¢òÁõ∏ÂêåÔºå‰ªçÊòØ<em>Topic sentence - Supporting sentence - Examples/details</em></p><p><strong>Ê≥®ÊÑè</strong>Ôºö</p><ol><li>ÊúÄÂ•ΩÂÅöÊòéÁ°ÆÈÄâÊã©„ÄÇ</li><li>ÈÄâÊõ¥ÊúâËØùËØ¥ÁöÑËßÇÁÇπÔºåËÄåÈùû‰Ω†ËµûÂêåÁöÑËßÇÁÇπ„ÄÇ</li></ol><h2 id="ÁªºÂêàÂè£ËØ≠"><a href="#ÁªºÂêàÂè£ËØ≠" class="headerlink" title="ÁªºÂêàÂè£ËØ≠"></a><strong>ÁªºÂêàÂè£ËØ≠</strong></h2><blockquote><p>For the first two:</p><p>preparation time: 30 sec</p><p>response time: 60 sec</p><p><strong>For the latter two</strong>:</p><p>preparation time: 20 sec</p><p>response time: 60 sec</p></blockquote><h3 id="Á¨¨‰∏âÈ¢òÔºöÊ†°Âõ≠Âú∫ÊôØÔºåËÅîÁ≥ªËßÇÁÇπËß£ÈáäÂéüÂõ†"><a href="#Á¨¨‰∏âÈ¢òÔºöÊ†°Âõ≠Âú∫ÊôØÔºåËÅîÁ≥ªËßÇÁÇπËß£ÈáäÂéüÂõ†" class="headerlink" title="Á¨¨‰∏âÈ¢òÔºöÊ†°Âõ≠Âú∫ÊôØÔºåËÅîÁ≥ªËßÇÁÇπËß£ÈáäÂéüÂõ†"></a><strong>Á¨¨‰∏âÈ¢òÔºöÊ†°Âõ≠Âú∫ÊôØÔºåËÅîÁ≥ªËßÇÁÇπËß£ÈáäÂéüÂõ†</strong></h3><blockquote><p>a reading passage and a listening passage. Summarize the speaker‚Äôs opinion within the context of the reading passage.</p></blockquote><p>reading passageÔºö</p><ol><li>Â≠¶Ê†°Â£∞ÊòéÔºåÊîπÂèòÊüêÁßçÊîøÁ≠ñ„ÄÇ</li><li>Â≠¶ÁîüÊù•‰ø°ÔºåÂª∫ËÆÆÊ†°ÊñπÂÅöÂá∫ÊîπÂèò„ÄÇ</li></ol><p>listening passage: Â≠¶ÁîüÂØπËØùËÆ®ËÆ∫reading passage‰∏≠ÁöÑÂÜÖÂÆπ„ÄÇ</p><p><strong>Ëß£È¢òÊñπÊ≥ï</strong>Ôºö</p><p>Êâæ‰ø°ÊÅØÁÇπ</p><ol><li>Â≠¶Ê†°ËÆ°ÂàíÂÅö‰ªÄ‰πà</li><li>Ëøô‰πàÂÅöÁöÑÁ¨¨‰∏Ä‰∏™ÂéüÂõ†</li><li>Ëøô‰πàÂÅöÁöÑÁ¨¨‰∫å‰∏™ÂéüÂõ†</li><li>Â≠¶ÁîüÂêåÊÑè/ÂèçÂØπ</li><li>ÂêåÊÑè/ÂèçÂØπÁöÑÁ¨¨‰∏Ä‰∏™ÂéüÂõ†</li><li>ÂêåÊÑè/ÂèçÂØπÁöÑÁ¨¨‰∫å‰∏™ÂéüÂõ†</li></ol><p><strong>Ê≥®ÊÑè</strong>Ôºö</p><ol><li>ÈáçÁÇπÂú®listeningÔºåÂõ†Ê≠§ÂêéËÄÖÊØîÈáçÂ∫îËØ•Êõ¥Â§ß„ÄÇËøôÊÑèÂë≥ÁùÄÊàë‰ª¨‰∏çÂ∫îËØ•Ëä±Â§™Â§öÊó∂Èó¥ÊèèËø∞Â≠¶Ê†°‰∏∫‰ªÄ‰πàÂá∫Âè∞ËøôÊ†∑ÁöÑÊîøÁ≠ñ„ÄÇ</li><li>listening‰∏≠ÊúâÊçßÂìèÂíåÈÄóÂìèÁöÑËßíËâ≤Ôºå‰ªÖÈÄóÂìèËÄÖÁªôÂá∫ËßÇÁÇπ„ÄÇ</li><li>‰∏çÊé∫ÂÖ•‰ªª‰Ωï‰∏™‰∫∫ËßÇÁÇπ„ÄÇ</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">The school recently decide to (make a change about) XXX</span><br><span class="line">because</span><br><span class="line">1.they ... .</span><br><span class="line">2. And also ...</span><br><span class="line">The man/woman holds positive/negative attitudes towards it,</span><br><span class="line">(s)he thinks,</span><br><span class="line">1....</span><br><span class="line">2.Besides,...</span><br></pre></td></tr></tbody></table></figure><h3 id="Á¨¨ÂõõÈ¢òÔºöÂ≠¶ÊúØËØæÁ®ãÔºåÊäΩË±°-ÂÖ∑‰Ωì"><a href="#Á¨¨ÂõõÈ¢òÔºöÂ≠¶ÊúØËØæÁ®ãÔºåÊäΩË±°-ÂÖ∑‰Ωì" class="headerlink" title="Á¨¨ÂõõÈ¢òÔºöÂ≠¶ÊúØËØæÁ®ãÔºåÊäΩË±°/ÂÖ∑‰Ωì"></a><strong>Á¨¨ÂõõÈ¢òÔºöÂ≠¶ÊúØËØæÁ®ãÔºåÊäΩË±°/ÂÖ∑‰Ωì</strong></h3><blockquote><p>a reading passage: broadly define a term, process, idea, etc.</p><p>a lecture: provide examples and <strong>specific</strong> information</p><p>Combine and convey important information from them.</p></blockquote><p>reading passageÔºöÂÆö‰πâ‰∏Ä‰∏™Â≠¶ÊúØÊ¶ÇÂøµÔºåÊèê‰æõÂÖ∂ËÉåÊôØÁü•ËØÜ„ÄÅÂÆö‰πâ„ÄÅÁâπÁÇπÁ≠â„ÄÇ</p><p>lectureÔºöÈÄöËøá‰æãÂ≠êÔºåÂØπÊäΩË±°Ê¶ÇÂøµÂÖ∑‰ΩìËß£Èáä</p><p><strong>Ëß£È¢òÊñπÊ≥ï</strong>Ôºö</p><ol><li>ÊâæÂá∫reading passageÁöÑÂÆö‰πâÂè•„ÄÇ</li><li>ÊâæÂá∫lectureÁöÑ‰æãÂ≠ê„ÄÇ</li></ol><p><strong>‰æãÂ≠ê</strong>ÔºöÔºàÂÖ∑‰ΩìËßÅ‰π¶Ôºâ</p><blockquote><p>The professor is talking about a psychological concept called‚Ä¶ÔºàÂêçÂ≠óÔºâ</p><p>which refers to ‚Ä¶.(ÂÆö‰πâ)</p><p>And he gives us two examples to illustrate this concept.</p><p>In the first case, ‚Ä¶.</p><p>In the second example,‚Ä¶</p></blockquote><p><strong>Ê≥®ÊÑè</strong>Ôºö</p><ol><li>‰ªçÊòØÂàöÊâçËØ¥ÁöÑÔºåÂê¨ÂäõÈÉ®ÂàÜÊõ¥ÈáçË¶Å„ÄÇ</li><li>ÂÆö‰πâÂè•ÊúÄÂ•ΩËÉΩËÆ∞‰∏ã„ÄÇ‰∏çÁî®ÁâπÂú∞Âú®ËØ¥ÁöÑÊó∂ÂÄôÊç¢ÁßçËØ¥Ê≥ï‰ª•ÊòæÁ§∫Ëá™Â∑±Ë°®ËææËÉΩÂäõ„ÄÇ</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">The professor is talk about a XXX(the field) concept called ...,</span><br><span class="line">which refers to ...(the definition)</span><br><span class="line">And he gives us two examples to illustrate the concept.</span><br><span class="line">In the first case,...</span><br><span class="line">In the second example,...</span><br></pre></td></tr></tbody></table></figure><h3 id="Á¨¨‰∫îÈ¢òÔºöÊ†°Âõ≠Âú∫ÊôØÔºåÈóÆÈ¢ò-Ëß£ÂÜ≥ÊñπÊ°à"><a href="#Á¨¨‰∫îÈ¢òÔºöÊ†°Âõ≠Âú∫ÊôØÔºåÈóÆÈ¢ò-Ëß£ÂÜ≥ÊñπÊ°à" class="headerlink" title="Á¨¨‰∫îÈ¢òÔºöÊ†°Âõ≠Âú∫ÊôØÔºåÈóÆÈ¢ò/Ëß£ÂÜ≥ÊñπÊ°à"></a><strong>Á¨¨‰∫îÈ¢òÔºöÊ†°Âõ≠Âú∫ÊôØÔºåÈóÆÈ¢ò/Ëß£ÂÜ≥ÊñπÊ°à</strong></h3><blockquote><p>a listening passage: conversation about a student-related problem and two possible solutions.</p><p>Demonstrate an understanding of the problem and express an opinion about solving it.</p></blockquote><p>‰∏§‰∏™Â≠¶Áîü‰∫§ÊµÅÔºåÂÖ∂‰∏≠‰∏Ä‰∏™ÊèêÂá∫Ëá™Â∑±ÁöÑÈóÆÈ¢òÔºåÂè¶‰∏Ä‰∏™‰∫∫ÁªôÂá∫‰∏§ÁßçÊñπÊ°àÔºå‰∏§‰∫∫ÂèØËÉΩÂØπÊñπÊ°àÊúâ‰∫õËØÑ‰ª∑„ÄÇ</p><p><strong>Ëß£È¢òÊñπÊ≥ï</strong>Ôºö</p><ul><li>Êâæ‰ø°ÊÅØÁÇπ<ol><li>‰ªÄ‰πàÈóÆÈ¢ò</li><li>Ëß£ÂÜ≥ÊñπÊ°à1</li><li>Ëß£ÂÜ≥ÊñπÊ°à2</li></ol></li><li>Â°´ÂÖÖ‰ø°ÊÅØÁÇπ<ol><li>‰Ω†ÁöÑÈÄâÊã©</li><li>‰Ω†ÁöÑÁêÜÁî±</li></ol></li></ul><p><strong>Ê≥®ÊÑè</strong>Ôºö</p><ol><li>ËøôÊòØÂîØ‰∏Ä‰∏Ä‰∏™ÈúÄË¶Å‰∏™‰∫∫ËßÇÁÇπÁöÑÈ¢òÁõÆ„ÄÇ„ÄäSpeaking Tips„ÄãÂª∫ËÆÆÁöÑÊó∂Èó¥ÂàÜÈÖçÊòØÔºö10 sec for the problem, 17 sec for each solution, and 10 sec for your choice and why, and about 6 sec pausing <strong>throughout</strong> your answer.ÔºàÊ≥®ÊÑè‰∏çÊòØËØ¥‰Ω†ÊúÄÂêéÁïôÂá∫Êï¥Êï¥6sÔºâ</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The problem the man/woman encountered is ...</span><br><span class="line">The woman/man suggests that ...</span><br><span class="line">Or ...</span><br><span class="line">If I were the man/woman, I prefer the first/latter solution, because</span><br><span class="line">1. the disadvantage of the other one</span><br><span class="line">2. the advantage of the one you choose.</span><br></pre></td></tr></tbody></table></figure><h3 id="Á¨¨ÂÖ≠È¢òÔºö-Â≠¶ÊúØËØæÁ®ãÔºåÊëòË¶Å"><a href="#Á¨¨ÂÖ≠È¢òÔºö-Â≠¶ÊúØËØæÁ®ãÔºåÊëòË¶Å" class="headerlink" title="Á¨¨ÂÖ≠È¢òÔºö Â≠¶ÊúØËØæÁ®ãÔºåÊëòË¶Å"></a><strong>Á¨¨ÂÖ≠È¢òÔºö Â≠¶ÊúØËØæÁ®ãÔºåÊëòË¶Å</strong></h3><blockquote><p>a lecture: explain a term or concept, together with some examples.</p><p>Summarize the lecture and demonstrate an understanding of the relationship between the examples and the overall topic.</p></blockquote><p>‰∏Ä‰∏™Â≠¶ÊúØÊ¶ÇÂøµÁöÑ‰∏§‰∏™ÊñπÈù¢„ÄÇ</p><p><strong>Ëß£È¢òÊñπÊ≥ï</strong>Ôºö</p><p>Êâæ‰ø°ÊÅØÁÇπ</p><ol><li>ÊïôÊéàË∞àËÆ∫ÁöÑ‰∏ªÈ¢ò</li><li>‰∏ªÈ¢òÁöÑÁ¨¨‰∏Ä‰∏™ÊñπÈù¢</li><li>Á¨¨‰∏Ä‰∏™ÊñπÈù¢ÁöÑ‰æãÂ≠ê</li><li>‰∏ªÈ¢òÁöÑÁöÑÁ¨¨‰∫å‰∏™ÊñπÈù¢</li><li>Á¨¨‰∫å‰∏™ÊñπÈù¢ÁöÑ‰æãÂ≠ê</li></ol><p><strong>Ê≥®ÊÑè</strong>Ôºö</p><ol><li><strong>‰æãÂ≠êÊØîÊ¶ÇÂøµÈáçË¶Å„ÄÇ</strong></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Language" scheme="http://ucm14.github.io/categories/Language/"/>
    
    
    <category term="TOEFL" scheme="http://ucm14.github.io/tags/TOEFL/"/>
    
  </entry>
  
  <entry>
    <title>Writing Template</title>
    <link href="http://ucm14.github.io/2024/09/28/Writing-Template/"/>
    <id>http://ucm14.github.io/2024/09/28/Writing-Template/</id>
    <published>2024-09-28T03:04:25.000Z</published>
    <updated>2024-09-28T03:10:30.210Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Integrated-Writing-Templates"><a href="#Integrated-Writing-Templates" class="headerlink" title="Integrated Writing Templates"></a>Integrated Writing Templates</h1><h2 id="Template-1"><a href="#Template-1" class="headerlink" title="Template 1"></a><strong>Template 1</strong></h2><blockquote><p>Already widely used</p></blockquote><p>The reading passage investigates the issue of‚Ä¶,  A, B, and C and the lecturer analyzes the same topic. However, he/she asserts all these three theories have flaws.</p><p>First and foremost, although the author of the reading passage suggests that‚Ä¶.( <strong>1-2 statements in reading</strong>ÔºâÔºå the presenter in the listening material contends that‚Ä¶ . This is because‚Ä¶, which means+the professor ‚Ä¶‚Ä¶   Evidently,  <strong>the professor‚Äôs argument challenges its counterpart in the reading</strong>.</p><p>Moreover, contrary to the statement in the reading that‚Ä¶, the professor indicates that‚Ä¶ . Then he/she <strong>backs up this point with the fact that</strong>‚Ä¶ . In other words, ‚Ä¶ .</p><p>Last but not least, whereas the author of the reading insists that‚Ä¶, the lecturer <strong>proves that this claim is indefensible  by pointing out that</strong>‚Ä¶ because‚Ä¶</p><h2 id="Template-2"><a href="#Template-2" class="headerlink" title="Template 2"></a><strong>Template 2</strong></h2><blockquote><p>Come from TPO 26</p></blockquote><p>The lecture opposes the reading‚Äôs opinion thatÔºà<strong>reading‚Äôs opinion</strong>Ôºâ</p><p>zebra mussel would severely threat the freshwater fish populations in North Ameria, <strong>contending that the arguments in the reading passage are not convincing</strong>.</p><p>Initially, while the reading stated thatÔºà<strong>first statement</strong>Ôºâ</p><p>according to zebra mussel‚Äôs history, their spread could not be stopped, the professor held an opposite opinion. HeÔºàsheÔºâ claimed thatÔºà<strong>first argument</strong>Ôºâ in the past people could not stop mussel from spreading because they lack of knowledge. Nevertheless, now people could pour out the freshwater in the ballast water and refill with ocean water, which would kill mussel.</p><p><strong>In addition, the professor casted doubt on the reading‚Äôs assumption that</strong>Ôºà<strong>second statement</strong>Ôºâ mussels would dominate any new habitat, arguing thatÔºà<strong>second argument</strong>Ôºâ they could only occupy the place at beginning. It would not take a long time before some kinds of birds realize that mussels are new available food. Since a bird could eat a lot of mussels, they would not dominate the habitat.</p><p>Moreover, the reading <strong>maintained</strong> thatÔºà<strong>third statement</strong>Ôºâ zebra mussels would give rise to a decline in the overall fish population, which is contradictory to what the professor said.Even if mussels could become dominant, he arguedÔºà<strong>third argument</strong>Ôºâ, the overall population would increase, as the existence of mussels would benefits other kinds of fish. For example, their waste could provide nutrient to bottom-feed fish, whose population would therefore grow.</p><h1 id="Academic-Discussion-Template"><a href="#Academic-Discussion-Template" class="headerlink" title="Academic Discussion Template"></a>Academic Discussion Template</h1><ol><li>This is a controversial topic, but I ‚Ä¶..(support) A‚Äôs idea that‚Ä¶ (27 words)</li><li>To be more specific ‚Ä¶‚Ä¶ (29 words)</li><li>For example ‚Ä¶‚Ä¶ (29 words)</li><li>In contrary, B‚Äôs opinion is ‚Ä¶‚Ä¶ (27 words)</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Language" scheme="http://ucm14.github.io/categories/Language/"/>
    
    
    <category term="TOEFL" scheme="http://ucm14.github.io/tags/TOEFL/"/>
    
  </entry>
  
  <entry>
    <title>Vocab</title>
    <link href="http://ucm14.github.io/2024/09/28/TOEFL-Practice/"/>
    <id>http://ucm14.github.io/2024/09/28/TOEFL-Practice/</id>
    <published>2024-09-28T03:00:52.000Z</published>
    <updated>2024-09-28T03:04:10.213Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Adjective"><a href="#Adjective" class="headerlink" title="Adjective"></a>Adjective</h1><p>Ë¥´Á©∑ÁöÑÔºöpoor, needy, impoverished, poverty, stricken</p><p>ÂØåË£ïÁöÑÔºörich, wealthy, affluent, well-to-do, well-off</p><p>‰ºòÁßÄÁöÑÔºöexcellent, eminent, top, outstanding</p><p>ÁßØÊûÅÁöÑÔºåÂ•ΩÁöÑÔºögood, conductive, beneficial, advantageous</p><p>Ê∂àÊûÅÁöÑÔºå‰∏çËâØÁöÑÔºöbad, detrimental, baleful, undesirable</p><p>ÊòéÊòæÁöÑÔºöobvious, apparent, evident, manifest</p><p>ÂÅ•Â∫∑ÁöÑÔºöhealthy, robust, sound, wholesome</p><p>ÊÉä‰∫∫ÁöÑÔºösurprising, amazing, extraordinary, miraculous</p><p>Áæé‰∏ΩÁöÑÔºöattractive, gorgeous, eye-catching</p><p>ÊúâÊ¥ªÂäõÁöÑÔºöenergetic, dynamic, vigorous, animated</p><p>ÊµÅË°åÁöÑÔºöpopular, prevailing, prevalent, pervasive</p><h1 id="Verb"><a href="#Verb" class="headerlink" title="Verb"></a>Verb</h1><p>ÊèêÈ´òÔºåÂä†Âº∫Ôºöimprove, enhance, promote, strengthen, optimize</p><p>ÂºïËµ∑Ôºötrigger, endanger, cause</p><p>Ëß£ÂÜ≥Ôºösolve, resolve, address, tackle, cope with, deal with</p><p>ÊãÜÈô§Ôºödestroy, tear down, knock down, eradicate</p><p>ÂüπÂÖªÔºödevelop, cultivate, foster, nurture</p><p>ÊøÄÂèëÔºåÈºìÂä±Ôºöencourage, motivate, stimulate, spur</p><p>ËÆ§‰∏∫Ôºöthink, assert, hold, claim, argue</p><p>ÂÆåÊàêÔºöcomplete, fulfill, accomplish, achieve</p><p>‰øùÁïôÔºökeep, preserve, retain, hold</p><p>ÊúâÂÆ≥‰∫éÔºödestroy, impair, undermine, jeopardize</p><p>ÂáèËΩªÔºöease, alleviate, relieve, lighten</p><h1 id="Noun"><a href="#Noun" class="headerlink" title="Noun"></a>Noun</h1><p>ÂΩ±ÂìçÔºöinfluence, impact</p><p>Âç±Èô©Ôºödanger, perils, hazard</p><p>Ê±°ÊüìÔºöpollution, contamination</p><p>‰∫∫Á±ªÔºöhuman beings, mankind, human race</p><p>ËÄÅ‰∫∫Ôºöthe old, old people, the elderly, the aged, senior citizens</p><p>Âπ∏Á¶èÔºöhappiness, cheerfulness, well-beings</p><p>ËÄÅÂ∏àÔºöteacher, instructor, educator, lecturer</p><p>ÈùíÂ∞ëÂπ¥Ôºöyoungster, adolescent</p><p>ÊïôËÇ≤Ôºöeducation, schooling, upbringing, family parenting</p><p>‰ºòÁÇπÔºöadvantage, merits, superiority, virtue</p><p>Ë¥£‰ªªÔºöresponsibility, obligation, duty, liability</p><p>ËÉΩÂäõÔºöability, capability, power, skill</p><p>ËÅå‰∏öÔºöjob, career, profession, employment</p><p>Â®±‰πêÔºöentertainment, recreation, pastimes, enjoyment</p><p>Â≠©Â≠êÔºöoffspring, descendants</p><p>Âà∫ÊøÄÔºöstimulus</p><h1 id="Phrase"><a href="#Phrase" class="headerlink" title="Phrase"></a>Phrase</h1><p>ÂÖÖÊª°‰∫ÜÔºöbe filled with, be awash with, be inundate with, be saturated with</p><p>Âä™ÂäõÔºöstruggle for, aspire after, strive for, spare no efforts for</p><p>‰ªé‰∫ãÔºöembark on, take up, set about, go in for</p><p>Âú®ÂΩì‰ª£Ôºöin contemporary society, in present-day society, in this day and age</p><p>Â§ßÈáèÁöÑÔºöa host of, a multitude of, a vast number of, a vast amount of</p><h1 id="Sentence"><a href="#Sentence" class="headerlink" title="Sentence"></a>Sentence</h1><table><thead><tr><th>English</th><th>‰∏≠Êñá</th></tr></thead><tbody><tr><td>It is a must for sb to do sth</td><td>ÂøÖÈ°ªÂÅö</td></tr><tr><td>Be bound to</td><td>ÂøÖÁÑ∂‰ºö</td></tr><tr><td>Under the instruction of</td><td>Âú®„ÄÇ„ÄÇ„ÄÇÊåáÂØº‰∏ã</td></tr><tr><td>the real looks of</td><td>„ÄÇ„ÄÇ„ÄÇÁöÑÁúüÈù¢ÁõÆ</td></tr><tr><td>A significant step toward</td><td>Âêë„ÄÇ„ÄÇ„ÄÇËøàÂá∫‰∏ÄÂ§ßÊ≠•</td></tr><tr><td>Be paved with</td><td>Áî±„ÄÇ„ÄÇ„ÄÇÈì∫Êàê</td></tr><tr><td>Do one‚Äôs utmost to</td><td>Â∞ΩÊúÄÂ§ßÂä™ÂäõÂéªÂÅö„ÄÇ„ÄÇ„ÄÇ</td></tr><tr><td>Deprive‚Ä¶of‚Ä¶</td><td>Ââ•Â§∫Ôºå‰Ωø„ÄÇ„ÄÇ„ÄÇ‰∏ßÂ§±</td></tr><tr><td>Be associated with</td><td>‰∏é„ÄÇ„ÄÇÊúâÂÖ≥</td></tr><tr><td>Bring‚Ä¶to the limelight</td><td>‰Ωø„ÄÇ„ÄÇ„ÄÇÊàê‰∏∫Ê≥®ÁõÆÁöÑÁÑ¶ÁÇπ</td></tr><tr><td>Be blessed with</td><td>‰∫´Êúâ</td></tr><tr><td>Excessive obsession with</td><td>ËøáÂàÜÊ≤âËø∑‰∫é</td></tr><tr><td>Well informed about</td><td>ÂØπ„ÄÇ„ÄÇÊúâÊ∑±ÂÖ•ÁöÑ‰∫ÜËß£</td></tr><tr><td>Keep a blind eye to</td><td>ÂØπ„ÄÇ„ÄÇ„ÄÇËßÜËÄå‰∏çËßÅ</td></tr><tr><td>Lay the foundation for</td><td>‰∏∫„ÄÇ„ÄÇ„ÄÇÊâì‰∏ãÂü∫Á°Ä</td></tr><tr><td>Endeavor to</td><td>Â∞ΩÂäõÂÅö</td></tr><tr><td>held in the palm of one‚Äôs hand</td><td>Ë¢´ÊéåÊè°Âú®Êüê‰∫∫Êâã‰∏≠</td></tr><tr><td>immerse oneself to</td><td>Ê≤âÊµ∏‰∫é„ÄÇ„ÄÇ„ÄÇ</td></tr><tr><td>Be of top priority‚Ä¶</td><td>„ÄÇ„ÄÇ„ÄÇÊòØÊúÄÈáçË¶ÅÁöÑ</td></tr><tr><td>Be obliged to</td><td>Êúâ‰πâÂä°ÂÅö</td></tr><tr><td>Prerequisite</td><td>ÂâçÊèêÔºåÂÖàÂÜ≥Êù°‰ª∂</td></tr><tr><td>From all works of life</td><td>Êù•Ëá™ÂêÑË°åÂêÑ‰∏ö</td></tr><tr><td>Bring into full play</td><td>ÂÖÖÂàÜÂèëÊå•‰ΩúÁî®„ÄÇ</td></tr><tr><td>Facilitate</td><td>‰øÉËøõÔºåÂ∏ÆÂä©</td></tr><tr><td>Exert‚Ä¶on‚Ä¶</td><td>Êää„ÄÇ„ÄÇ„ÄÇÂº∫Âä†‰∫é„ÄÇ„ÄÇ„ÄÇ‰∏ä</td></tr><tr><td>Boast</td><td>ÂèñÂæóÔºåÊã•Êúâ</td></tr><tr><td>Ensue</td><td>Êé•ÁùÄÂèëÁîü</td></tr><tr><td>Steel</td><td>‰ΩøÂùöÂõ∫</td></tr><tr><td>Destructive</td><td>Á†¥ÂùèÁöÑÔºå ÊúâÂÆ≥ÁöÑ</td></tr><tr><td>Marvel at</td><td>ÊÉäÂèπ‰∫é„ÄÇ„ÄÇ„ÄÇ</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Language" scheme="http://ucm14.github.io/categories/Language/"/>
    
    
    <category term="TOEFL" scheme="http://ucm14.github.io/tags/TOEFL/"/>
    
  </entry>
  
  <entry>
    <title>The very first blog</title>
    <link href="http://ucm14.github.io/2024/09/28/The-very-first-blog/"/>
    <id>http://ucm14.github.io/2024/09/28/The-very-first-blog/</id>
    <published>2024-09-27T18:09:57.000Z</published>
    <updated>2024-10-01T15:32:33.990Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Dear readers:</p><p>This is Minfeng here. My personal page has gone through many twists and turns, but it has finally been rebuilt for the third time. In the coming days, I will upload my previous personal learning records as soon as possible. Additionally, I will share other noteworthy experiences here.</p><p>I plan to first upload my learning records on reinforcement learning and my experiences preparing for the TOEFL exam, then my scientific research.</p><p>Of course, I will backup local files by uploading them to my GitHub repos :)</p><p>Best regards,</p><p>Minfeng</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Misc" scheme="http://ucm14.github.io/categories/Misc/"/>
    
    
    <category term="Notes" scheme="http://ucm14.github.io/tags/Notes/"/>
    
  </entry>
  
</feed>
