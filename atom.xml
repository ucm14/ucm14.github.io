<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mason&#39;s Radio</title>
  
  
  <link href="http://ucm14.github.io/atom.xml" rel="self"/>
  
  <link href="http://ucm14.github.io/"/>
  <updated>2024-10-04T16:38:11.630Z</updated>
  <id>http://ucm14.github.io/</id>
  
  <author>
    <name>Minfeng &quot;Mason&quot; Yu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Mason&#39;s Radio#3</title>
    <link href="http://ucm14.github.io/2024/10/05/Mason-s-Radio-3/"/>
    <id>http://ucm14.github.io/2024/10/05/Mason-s-Radio-3/</id>
    <published>2024-10-04T16:24:04.000Z</published>
    <updated>2024-10-04T16:38:11.630Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>The song features an arrangement and emotional progression ahead of its time, with realistic and finely detailed lyrics. Combined with Lena Park’s near-perfect performance, it creates a vivid and touching experience for the listeners.</p></blockquote><iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/1hNYiYh10zoFl4L6RrLDHq?utm_source=generator" width="100%" height="352" frameborder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe><p>This song has been performed many times on South Korean television programs. The following video is, in my opinion, the best live performance.</p><div style="text-align: center;">    <iframe width="560" height="315" src="https://www.youtube.com/embed/FrjNd92fXVo?si=kBPhNm3o5alfFab2" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Music" scheme="http://ucm14.github.io/categories/Music/"/>
    
    
    <category term="Pop" scheme="http://ucm14.github.io/tags/Pop/"/>
    
  </entry>
  
  <entry>
    <title>Lost in Vibe</title>
    <link href="http://ucm14.github.io/2024/10/04/Lost-in-the-vibe/"/>
    <id>http://ucm14.github.io/2024/10/04/Lost-in-the-vibe/</id>
    <published>2024-10-04T15:24:44.000Z</published>
    <updated>2024-10-04T17:12:17.290Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>Just few photos I took in days before.</p></blockquote><h1 id="Yangtze-River"><a href="#Yangtze-River" class="headerlink" title="Yangtze River"></a>Yangtze River</h1><p><img src="https://s2.loli.net/2024/10/04/uRyVpGH2aroUmJl.jpg" alt="photo_2_2024-10-04_23-22-07.jpg"></p><p>Yangtze River Bridge.</p><p><img src="https://s2.loli.net/2024/10/04/DFOj1VJ2WYNvqmt.jpg" alt="photo_4_2024-10-04_23-22-07.jpg"></p><p>I was planed to take this ferry service to the southside of Nanjing. However, too many tourists made me give up the idea.</p><p><img src="https://s2.loli.net/2024/10/04/qmGMaAUlBvfIx7b.jpg" alt="photo_7_2024-10-04_23-22-07.jpg"></p><p>Relief.</p><h1 id="On-the-street"><a href="#On-the-street" class="headerlink" title="On the street"></a>On the street</h1><p><img src="https://s2.loli.net/2024/10/04/SjIp9Kx2VGWsFYJ.jpg" alt="photo_10_2024-10-04_23-22-07.jpg"></p><p>A interesting name for a road.</p><p><img src="https://s2.loli.net/2024/10/04/NcKLpCMj7DnVmbR.jpg" alt="photo_5_2024-10-04_23-22-07.jpg"></p><p>Sounds like a Internet Cafe.</p><p><img src="https://s2.loli.net/2024/10/04/XQJj8cfdbRZGmCe.jpg" alt="photo_6_2024-10-04_23-22-07.jpg"></p><p>The railway.</p><p><img src="https://s2.loli.net/2024/10/04/9eliuy8qgZj5cHM.jpg" alt="photo_1_2024-10-04_23-22-07.jpg"></p><p>An insane e-bike outfit.</p><p><img src="https://s2.loli.net/2024/10/04/xXBe8h4kCWVULEt.jpg" alt="photo_3_2024-10-04_23-22-07.jpg"></p><p>Truly a strange name for a road in Chinese.</p><p><img src="https://s2.loli.net/2024/10/04/gb6kAC2li3XQrpI.jpg" alt="photo_9_2024-10-04_23-22-07.jpg"></p><p>A very historic road.</p><p><img src="https://s2.loli.net/2024/10/04/EMwomNZHGchBjuW.jpg" alt="photo_8_2024-10-04_23-22-07.jpg"></p><p>I’m not good at playing either Chinese chess or chess, but enjoying watching chess game.</p><p><img src="https://s2.loli.net/2024/10/05/CZzBPX2maexLKF8.jpg" alt="photo_12_2024-10-04_23-22-07.jpg"></p><p>Old building sieged by developed urban area.</p><h1 id="Specialties"><a href="#Specialties" class="headerlink" title="Specialties"></a>Specialties</h1><p><img src="https://s2.loli.net/2024/10/05/cdPkYHSBG61ZfTy.jpg" alt="photo_14_2024-10-04_23-22-07.jpg"></p><p>Steamed Milk Egg Custard with Oreo top.</p><p><img src="https://s2.loli.net/2024/10/05/oDdNeW4hV8kcxn6.jpg" alt="photo_13_2024-10-04_23-22-07.jpg"></p><p>Black sesame sweet soup with rice dumplings &amp; Milk brick.</p><p><img src="https://s2.loli.net/2024/10/05/tm8h7YJn3ZCX6LB.jpg" alt="photo_18_2024-10-04_23-22-07.jpg"></p><p>Traditional Xuzhou cuisines.</p><p><img src="https://s2.loli.net/2024/10/05/rJBcXhTWM6naoqK.jpg" alt="photo_11_2024-10-04_23-22-07.jpg"></p><p>Nine-delicacy noodle.</p><p><img src="https://s2.loli.net/2024/10/05/FJw9WKiLPprTYDR.jpg" alt="photo_15_2024-10-04_23-22-07.jpg"></p><p>Nanjing roast duck with sweet brine.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Misc" scheme="http://ucm14.github.io/categories/Misc/"/>
    
    
    <category term="Daily Life" scheme="http://ucm14.github.io/tags/Daily-Life/"/>
    
  </entry>
  
  <entry>
    <title>2. Begin with model-free algorithms</title>
    <link href="http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/"/>
    <id>http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/</id>
    <published>2024-10-02T06:20:50.000Z</published>
    <updated>2024-10-02T14:40:39.484Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Model-based-Model-free-RL"><a href="#Model-based-Model-free-RL" class="headerlink" title="Model-based / Model-free RL"></a>Model-based / Model-free RL</h1><p>Everybody knows reinforcement learning is one of a machine learning methods aiming to make agent explore and find the optimal policy through interactions with environment and maximize cumulative reward. Before starting this passage, we have to understand what is model-free and what is model-based?</p><p><img src="https://s2.loli.net/2024/08/03/4QSDOYacz9pHLAG.png" alt="image.png"></p><h1 id="Monte-Carlo-MC"><a href="#Monte-Carlo-MC" class="headerlink" title="Monte-Carlo (MC)"></a>Monte-Carlo (MC)</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>The Monte Carlo algorithm is a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The core idea is to use randomness to solve problems that might be deterministic in principle. The key concepts of MC are: <strong>random sampling</strong>, <strong>estimation</strong> by taking the average of outcomes from random samples, applicability of <strong>high-dimensional spaces</strong>. It can be used in simulations, optimization, numerical integration, financial anticipation and statistical physics. Advantages and disadvantages of MC are shown in the table below.</p><table><thead><tr><th>Monte Carlo</th><th>Advantages</th><th>Disadvantages</th></tr></thead><tbody><tr><td>1</td><td>Simple to implement</td><td>Can be computationally intensive</td></tr><tr><td>2</td><td>Flexible and can handle a wide range of problems</td><td>Convergence can be slow</td></tr><tr><td>3</td><td>Suitable for high-dimensional integrals</td><td>Results are probabilistic, not deterministic</td></tr></tbody></table><p>Here is a classic example of MC application: Estimate the value of $π$.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">defestimate_pi(num_samples):</span><br><span class="line">    inside_circle =<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _inrange(num_samples):</span><br><span class="line">        x = random.uniform(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">        y = random.uniform(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> x**<span class="number">2</span> + y**<span class="number">2</span> &lt;=<span class="number">1</span>:</span><br><span class="line">            inside_circle +=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> (inside_circle / num_samples) *<span class="number">4</span></span><br><span class="line"></span><br><span class="line">pi_estimate = estimate_pi(<span class="number">100000</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Estimated value of π:<span class="subst">{pi_estimate}</span>"</span>)</span><br></pre></td></tr></tbody></table></figure><p>The result is <code>π: 3.13544</code>.</p><p>Assuming we have a state sequence under a policy π:<br>$$<br>[S_1,A_1,R_1,S_2,A_2,R_2,S_3,A_3,R_3,…,S_T,A_T,R_T]\tag{1}<br>$$<br>In MDP, the value function is:<br>$$<br>v_π(s)=E_π[G_t|S_t=s]G_t=R_{t+1}+γR_{t+2}+…+γ^{T−t−1}R_Tv_π(s)≈average(G_t)\tag{2}<br>$$<br>However, the average consumes so many storage. A better method is obtaining average value in iterations:<br>$$<br> μt=\frac 1 t ∑j=\frac 1 t x_j=\frac 1 t (x_t+\sum_{j=1}^{t−1}x_j)=μ_{t−1}+\frac 1 t (x_t−μ_{t−1})\tag{3}<br>$$</p><h1 id="Q-Learning-the-first-step-of-model-free-RL"><a href="#Q-Learning-the-first-step-of-model-free-RL" class="headerlink" title="Q Learning: the first step of model-free RL"></a>Q Learning: the first step of model-free RL</h1><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Q learning is a value-based RL algorithm. Thus, Q value is the fundamental variable in this algorithm, and this is the reason why this algorithm called “Q Learning”. Assuming we already know what is agent, what is environment, what are the states, the actions have to be discrete and the reward function, let us cut to the point and introduce the learning method of agent in Q Learning.<br>$$<br> Q(s,a)\leftarrow Q(s,a)+α(r+γmax_{a^′}Q(s^′,a^′)−Q(s,a))\tag{4}<br>$$<br>Equation above is the update method of Q value. α is learning rate (basically learning rate defines the pace of updating), γ is discount factor. $Q(s,a)$ is the action value of current state; $Q(s^′,a^′)$ is the action value of next state; $max_{a^′}Q(s^′,a^′)$ means choosing the maximum $Q(s^′,a^′)$ among all the possible actions.</p><p>The Q learning is quite similar to value iteration. We are going to explain the differences between Q learning and value iteration in detail through a simple implementation.</p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h3><p>The environment is a 4x4 grid. Our target is to train the agent and let it successfully moves from (0, 0) to (3,3).</p><p><img src="https://s2.loli.net/2024/08/03/qbufGrCwPdHk2oU.png" alt="image.png"></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">grid_size = <span class="number">4</span></span><br><span class="line">goal_state = (<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">start_state = (<span class="number">0</span>,<span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="Hyperparameters"><a href="#Hyperparameters" class="headerlink" title="Hyperparameters"></a>Hyperparameters</h3><p>$ϵ$ means the agent have probability equals to ϵ choosing a random action and $(1-ϵ)$ probability choosing the optimal action.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">alpha =<span class="number">0.1</span> <span class="comment">#learning rate</span></span><br><span class="line">gamma =<span class="number">0.9</span> <span class="comment">#discount factor</span></span><br><span class="line">epsilon =<span class="number">0.1</span> <span class="comment">#epsilon-greedy</span></span><br><span class="line">num_episodes = <span class="number">1000</span></span><br></pre></td></tr></tbody></table></figure><h3 id="State-s-and-action-s"><a href="#State-s-and-action-s" class="headerlink" title="State(s) and action(s)"></a>State(s) and action(s)</h3><p>The state is just all the points in the 4x4 girds. We create two functions for action selection and transition to next state.</p><p>In function <code>get_next_state</code>, the current coordinates based on the action:</p><ul><li>If the action is <code>'up'</code> and <code>x</code> is greater than 0, it moves up by decrementing <code>x</code> by 1.</li><li>If the action is <code>'down'</code> and <code>x</code> is less than <code>grid_size - 1</code>, it moves down by incrementing <code>x</code> by 1.</li><li>If the action is <code>'left'</code> and <code>y</code> is greater than 0, it moves left by decrementing <code>y</code> by 1.</li><li>If the action is <code>'right'</code> and <code>y</code> is less than <code>grid_size - 1</code>, it moves right by incrementing <code>y</code> by 1.</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_next_state</span>(<span class="params">state, action</span>):</span><br><span class="line">    x, y = state</span><br><span class="line">    <span class="keyword">if</span> action == <span class="string">'up'</span> <span class="keyword">and</span> x &gt; <span class="number">0</span>:</span><br><span class="line">        x -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="string">'down'</span> <span class="keyword">and</span> x &lt; grid_size - <span class="number">1</span>:</span><br><span class="line">        x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="string">'left'</span> <span class="keyword">and</span> y &gt; <span class="number">0</span>:</span><br><span class="line">        y -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="string">'right'</span> <span class="keyword">and</span> y &lt; grid_size - <span class="number">1</span>:</span><br><span class="line">        y += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> (x, y)</span><br><span class="line"></span><br><span class="line">actions = [<span class="string">'up'</span>,<span class="string">'down'</span>,<span class="string">'left'</span>,<span class="string">'right'</span>]</span><br><span class="line">num_actions =<span class="built_in">len</span>(actions)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_action</span>(<span class="params">state</span>):</span><br><span class="line"><span class="keyword">if</span> np.random.uniform(<span class="number">0</span>,<span class="number">1</span>) &lt; epsilon:</span><br><span class="line"><span class="keyword">return</span> np.random.choice(num_actions)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> np.argmax(Q_table[state[<span class="number">0</span>], state[<span class="number">1</span>], :])</span><br></pre></td></tr></tbody></table></figure><h3 id="Reward-function"><a href="#Reward-function" class="headerlink" title="Reward function"></a>Reward function</h3><p>Agent will get reward = 100 if arriving at (3, 3), otherwise -1.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_reward</span>(<span class="params">state</span>):</span><br><span class="line"><span class="keyword">if</span> state == goal_state:</span><br><span class="line"><span class="keyword">return</span> <span class="number">100</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Q-value-function"><a href="#Q-value-function" class="headerlink" title="Q value function"></a>Q value function</h3><p>This part is the fundamental training loop of algorithm. <code>while state != goal_state</code> means continues iterations until the goal state is reached; <code>action_index = choose_action(state)</code> means select an action index based on policy which not shown; <code>action = actions[action_index]</code>is to convert the action index to an actual action; <code>next_state = get_next_state(state, action)</code> is used to determine the next state based on the current state and action; finally,<code>reward = get_reward(next_state)</code>can calculate the reward for transitioning to the next state.</p><p>Then, we store Q value under current (s, a) pair in Q table, computes the maximum Q value for next state and updates Q value for current $(s,a)$ pair using Q learning update rule.</p><p>Since we have the maximum Q value of each state stored in Q table, we can try to compute the optimal policy and turn it into the form of path.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> episodeinrange(num_episodes):</span><br><span class="line">    state = start_state</span><br><span class="line"><span class="keyword">while</span> state != goal_state:</span><br><span class="line">        action_index = choose_action(state)</span><br><span class="line">        action = actions[action_index]</span><br><span class="line">        next_state = get_next_state(state, action)</span><br><span class="line">        reward = get_reward(next_state)</span><br><span class="line"></span><br><span class="line">        Q_table[state[<span class="number">0</span>], state[<span class="number">1</span>], action_index] += alpha * (</span><br><span class="line">                reward + gamma * np.<span class="built_in">max</span>(Q_table[next_state[<span class="number">0</span>], next_state[<span class="number">1</span>], :]) - Q_table[</span><br><span class="line">            state[<span class="number">0</span>], state[<span class="number">1</span>], action_index]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        state = next_state</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Final Q-Table:"</span>)</span><br><span class="line"><span class="built_in">print</span>(Q_table)</span><br><span class="line"></span><br><span class="line">state = start_state</span><br><span class="line">path = [state]</span><br><span class="line"><span class="keyword">while</span> state != goal_state:</span><br><span class="line">    action_index = np.argmax(Q_table[state[<span class="number">0</span>], state[<span class="number">1</span>], :])</span><br><span class="line">    action = actions[action_index]</span><br><span class="line">    state = get_next_state(state, action)</span><br><span class="line">    path.append(state)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Path from start to goal:"</span>)</span><br><span class="line"><span class="built_in">print</span>(path)</span><br></pre></td></tr></tbody></table></figure><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>The path after training is shown in the figure.</p><p><img src="https://s2.loli.net/2024/08/03/kjtiXCEQDSoywsK.png" alt="image.png"></p><h1 id="Sarsa-An-on-policy-RL-algorithm"><a href="#Sarsa-An-on-policy-RL-algorithm" class="headerlink" title="Sarsa: An on-policy RL algorithm"></a>Sarsa: An on-policy RL algorithm</h1><h2 id="Difference-between-on-policy-off-policy"><a href="#Difference-between-on-policy-off-policy" class="headerlink" title="Difference between on-policy &amp; off-policy"></a>Difference between on-policy &amp; off-policy</h2><p>In reinforcement learning, <strong>two different policies</strong> are also used for active agents: <strong>a behavior policy</strong> and <strong>a target policy</strong>. A behavior policy is used to decide actions in a given state (what behavior the agent is currently using to interact with its environment), while a target policy is used to learn about desired actions and what rewards are received (the ideal policy the agent seeks to use to interact with its environment).</p><blockquote><p>If an algorithm’s behavior policy matches its target policy, this means it is an on-policy algorithm. If these policies in an algorithm don’t match, then it is an off-policy algorithm.</p></blockquote><p><img src="https://s2.loli.net/2024/08/03/D3tYshGI4uvSbdo.png" alt="image.png"></p><p>Sarsa operates by choosing an action following the current epsilon-greedy policy and updates its Q values accordingly. On-policy algorithms like Sarsa select random actions where non-greedy actions have some probability of being selected, providing a balance between exploitation and exploration techniques. Since Sarsa Q values are generally learned using the same epsilon-greedy policy for behavior and target, it classifies as on-policy.</p><p>Q learning, unlike Sarsa, tends to choose the greedy action in sequence. A greedy action is one that gives the maximum Q value for the state, that is, it follows an optimal policy. Off-policy algorithms like Q learning learn a target policy regardless of what actions are selected from exploration. Since Q learning uses greedy actions, and can evaluate one behavior policy while following a separate target policy, it classifies as off-policy.</p><h2 id="Algorithm-1"><a href="#Algorithm-1" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>SARSA, unlike Q learning, is an on-policy algorithm, which means it updates the policy based on the actions taken. Quite like policy iteration. But in Sarsa, the update of policy will not be as “hard” as policy iteration since we have the influence of learning rate $α$.</p><p>Still, the algorithm keeps updating Q value. One thing different is agent of Sarsa already come up with which action to choose and predict next state and next action. That is why the algorithm called SARSA - State, Action, Reward, State, Action.</p><h2 id="Implementation-1"><a href="#Implementation-1" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Maze</span></span><br><span class="line">maze = np.array([</span><br><span class="line">    [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, -<span class="number">1</span>,<span class="number">0</span>, -<span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>, -<span class="number">1</span>],</span><br><span class="line">    [-<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start state and terminal state</span></span><br><span class="line">start_state = (<span class="number">3</span>,<span class="number">0</span>)</span><br><span class="line">goal_state = (<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># action space</span></span><br><span class="line">actions = [(<span class="number">0</span>,<span class="number">1</span>), (<span class="number">0</span>, -<span class="number">1</span>), (-<span class="number">1</span>,<span class="number">0</span>), (<span class="number">1</span>,<span class="number">0</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize value function</span></span><br><span class="line">Q = np.zeros((<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyperparameters</span></span><br><span class="line">alpha = <span class="number">0.1</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">max_episodes = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SARSA</span></span><br><span class="line"><span class="keyword">for</span> episodeinrange(max_episodes):</span><br><span class="line">    state = start_state</span><br><span class="line">    action = np.random.choice(<span class="built_in">range</span>(<span class="number">4</span>))<span class="keyword">if</span> np.random.rand() &lt; epsilonelse np.argmax(Q[state])</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> state != goal_state:</span><br><span class="line"><span class="comment"># next_state = (state[0] + actions[action][0], state[1] + actions[action][1])</span></span><br><span class="line">        a = state[<span class="number">0</span>] + actions[action][<span class="number">0</span>]</span><br><span class="line">        b = state[<span class="number">1</span>] + actions[action][<span class="number">1</span>]</span><br><span class="line"><span class="keyword">if</span> a &gt;<span class="number">3</span>:</span><br><span class="line">            a-=<span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> b &gt;<span class="number">3</span>:</span><br><span class="line">            b-=<span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> a &lt; -<span class="number">4</span>:</span><br><span class="line">            a+=<span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> b &lt; -<span class="number">4</span>:</span><br><span class="line">            b+=<span class="number">1</span></span><br><span class="line">        next_state = (a,b)</span><br><span class="line">        reward = maze[next_state]</span><br><span class="line">        next_action = np.random.choice(<span class="built_in">range</span>(<span class="number">4</span>))<span class="keyword">if</span> np.random.rand() &lt; epsilonelse np.argmax(Q[next_state])</span><br><span class="line">        Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])</span><br><span class="line"></span><br><span class="line">        state = next_state</span><br><span class="line">        action = next_action</span><br><span class="line"></span><br><span class="line"><span class="comment"># print result</span></span><br><span class="line"><span class="keyword">for</span> iinrange(<span class="number">4</span>):</span><br><span class="line"><span class="keyword">for</span> jinrange(<span class="number">4</span>):</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"State:"</span>, (i, j))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Up:"</span>, Q[i][j][<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Down:"</span>, Q[i][j][<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Left:"</span>, Q[i][j][<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Right:"</span>, Q[i][j][<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>()</span><br></pre></td></tr></tbody></table></figure><h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">State: (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">Up: -<span class="number">0.008042294056935573</span></span><br><span class="line">Down: -<span class="number">0.007868742418369764</span></span><br><span class="line">Left: -<span class="number">0.016173595452674966</span></span><br><span class="line">Right: <span class="number">0.006662566560762523</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">Up: <span class="number">0.048576025675988774</span></span><br><span class="line">Down: -<span class="number">0.0035842473161881465</span></span><br><span class="line">Left: <span class="number">0.024420015715567546</span></span><br><span class="line">Right: -<span class="number">0.46168987981312615</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">Up: <span class="number">0.04523751845081987</span></span><br><span class="line">Down: <span class="number">0.04266319340558091</span></span><br><span class="line">Left: <span class="number">0.044949583791193154</span></span><br><span class="line">Right: <span class="number">0.026234839551098416</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">Up: <span class="number">0.01629652821649763</span></span><br><span class="line">Down: <span class="number">0.050272192325180515</span></span><br><span class="line">Left: -<span class="number">0.009916869922464355</span></span><br><span class="line">Right: -<span class="number">0.4681667868865369</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">Up: -<span class="number">0.09991342319696966</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: <span class="number">0.0</span></span><br><span class="line">Right: <span class="number">0.036699099068340166</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">Up: <span class="number">0.008563965102313987</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: <span class="number">0.0</span></span><br><span class="line">Right: <span class="number">0.3883250678150607</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">Up: -<span class="number">0.3435187267522706</span></span><br><span class="line">Down: -<span class="number">0.2554776873673874</span></span><br><span class="line">Left: <span class="number">0.05651543121932354</span></span><br><span class="line">Right: <span class="number">0.004593450910446022</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">Up: -<span class="number">0.1</span></span><br><span class="line">Down: -<span class="number">0.013616634831997914</span></span><br><span class="line">Left: <span class="number">0.01298827764814053</span></span><br><span class="line">Right: <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">2</span>, <span class="number">0</span>)</span><br><span class="line">Up: <span class="number">0.28092113053540924</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: <span class="number">0.0024286388798406364</span></span><br><span class="line">Right: <span class="number">0.06302299434701504</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">Up: <span class="number">0.0</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: -<span class="number">0.16509175606504775</span></span><br><span class="line">Right: <span class="number">1.9146361697676122</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">Up: -<span class="number">0.1</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: <span class="number">0.03399106390140035</span></span><br><span class="line">Right: <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">Up: -<span class="number">0.3438668479533914</span></span><br><span class="line">Down: <span class="number">0.004696957810272524</span></span><br><span class="line">Left: -<span class="number">0.19</span></span><br><span class="line">Right: <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">3</span>, <span class="number">0</span>)</span><br><span class="line">Up: <span class="number">3.3060693607932445</span></span><br><span class="line">Down: <span class="number">0.8893977121867367</span></span><br><span class="line">Left: <span class="number">0.0</span></span><br><span class="line">Right: <span class="number">0.13715553550041798</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">Up: <span class="number">4.825854511712306</span></span><br><span class="line">Down: -<span class="number">0.03438123168566812</span></span><br><span class="line">Left: <span class="number">0.10867882029322147</span></span><br><span class="line">Right: <span class="number">1.0015572397722665</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">Up: <span class="number">5.875704328143301</span></span><br><span class="line">Down: <span class="number">0.9315770230698863</span></span><br><span class="line">Left: <span class="number">0.0006851481810742227</span></span><br><span class="line">Right: <span class="number">0.47794799892127526</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">Up: <span class="number">5.4028951599661275</span></span><br><span class="line">Down: <span class="number">2.6989177956329757</span></span><br><span class="line">Left: -<span class="number">0.6454474033238188</span></span><br><span class="line">Right: <span class="number">0.018474082554518417</span></span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://ucm14.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Reinforcement Learning" scheme="http://ucm14.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>1. Value iteration vs Policy iteration</title>
    <link href="http://ucm14.github.io/2024/10/02/1-Value-iteration-vs-Policy-iteration/"/>
    <id>http://ucm14.github.io/2024/10/02/1-Value-iteration-vs-Policy-iteration/</id>
    <published>2024-10-02T02:06:43.000Z</published>
    <updated>2024-10-02T06:17:15.413Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Markov-chains-transition-probability-matrix"><a href="#Markov-chains-transition-probability-matrix" class="headerlink" title="Markov chains &amp; transition probability matrix"></a>Markov chains &amp; transition probability matrix</h1><h2 id="Markov-chains"><a href="#Markov-chains" class="headerlink" title="Markov chains"></a>Markov chains</h2><p>In machine learning algorithms, <em><strong>Markov chains</strong></em> are widely applied in time series models. The main idea is that regardless of the initial state, as long as the state transition matrix remains unchanged, the final state will always converge to a fixed value. This memory-lessness is called the Markov property. The equation is as below:<br>$$<br>P(x_{t+1}∣…,x_{t−2},x_{t−1},x_{t})=P(x_{t+1}∣x_{t})\tag{1}<br>$$</p><h2 id="Transition-probability-matrix"><a href="#Transition-probability-matrix" class="headerlink" title="Transition probability matrix"></a>Transition probability matrix</h2><p>Each element of the matrix is represented by a probability. The values are non-negative, and the sum of the elements in each row equals 1. Under certain conditions, they can transition between each other, hence it is called a <em><strong>transition probability matrix</strong></em>. The two-step transition probability matrix is exactly the square of the one-step transition probability matrix. The <strong>$𝑘$</strong> step transition probability matrix is the <strong>$𝑘^{th}$</strong> power of the one-step transition probability matrix. In the <strong>$𝑘$</strong> step transition probability matrix, the sum of the elements in each row is also 1.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>For example, the value of $P(i,j)$ in matrix is $P(j|i)$, which is the probability of from state $i$ to state $j$. The transition probability matrix is as below：<br>$$<br>\begin{bmatrix} 0.9 &amp; 0.075 &amp; 0.025 \\ 0.15 &amp; 0.8 &amp; 0.05 \\ 0.25 &amp; 0.25 &amp; 0.5 \end{bmatrix}\tag{2}<br>$$</p><h3 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h3><p>Giving an initial state $P_{01}=[0.5,0.2,0.3]$ and $P_{02}=[0.1,0.4,0.5]$, the <strong>𝑘</strong> step transition is $P_0=P_0∗P_k$. $P_0$ can reach a stable value after multiple iterations. If k=30, please calculate the eventual result.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl</span><br><span class="line"></span><br><span class="line">p01 = np.array([<span class="number">0.5</span>, <span class="number">0.2</span>, <span class="number">0.3</span>]) </span><br><span class="line">p02 = np.array([<span class="number">0.1</span>,<span class="number">0.4</span>,<span class="number">0.5</span>]) </span><br><span class="line">p = np.array([[<span class="number">0.9</span>, <span class="number">0.075</span>, <span class="number">0.025</span>], [<span class="number">0.15</span>, <span class="number">0.8</span>, <span class="number">0.05</span>],[<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.5</span>]])</span><br><span class="line">n = <span class="number">30</span></span><br><span class="line">c = np.array([<span class="string">'r'</span>,<span class="string">'g'</span>,<span class="string">'b'</span>])       </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calanddraw</span>(<span class="params">p0,p </span>):</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">     p0 = np.mat(p0) * np.mat(p)       </span><br><span class="line">     <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(np.array(p0)[<span class="number">0</span>])):</span><br><span class="line">          pl.scatter(i,p0[<span class="number">0</span>,j], c = c[j], s=<span class="number">.5</span>)</span><br><span class="line">pl.subplot(<span class="number">121</span>)</span><br><span class="line">calanddraw(p01,p)</span><br><span class="line">pl.subplot(<span class="number">122</span>)</span><br><span class="line">calanddraw(p02,p)</span><br><span class="line">pl.show()</span><br></pre></td></tr></tbody></table></figure><p>From the figure below we can reach the conclusion that both states converge eventually and their value are close to each other.</p><p><img src="https://s2.loli.net/2024/08/01/I9NQp4icfJaZ52h.png" alt="image.png"></p><h1 id="Model-based-algorithms"><a href="#Model-based-algorithms" class="headerlink" title="Model-based algorithms"></a>Model-based algorithms</h1><h2 id="Value-iteration"><a href="#Value-iteration" class="headerlink" title="Value iteration"></a>Value iteration</h2><p>Unlike simply times initial state with transition probability matrix multiple times, <em><strong>value iteration</strong></em> is a commonly used <strong>dynamic planning (DP)</strong> method (<em>DP methods assume that we have a <strong>perfect model</strong> of the environment’s MDP. That’s usually not the case in practice, but it’s important to study DP anyway</em>), which is mainly used to solve the optimal strategy problem in Markov Decision Process (MDP). The core idea of the Value Iteration algorithm is to update the value function of the state iteratively, gradually approaching the optimal value function, so as to obtain the optimal policy. The main advantage of the value iteration algorithm is that it is simple and easy to implement, and it is applicable to various types of MDP problems. However, the main disadvantage of the value iteration algorithm is its <strong>high time complexity</strong>, especially when the state space is large. Therefore, in practical applications, the value iteration algorithm usually needs to be combined with other optimization techniques, such as dynamic programming optimization and parallel computing, to improve the computational efficiency. The equation of algorithm is as below.</p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>$$<br>V_{(k+1)}(s)=max_a[R(s,a)+γ_{Σs^′∈S}P(s^′|s,a)V^k(s^′)]\tag{3}<br>$$</p><p>First, we start with a random value function $V(s)$. At each step, we update it. Hence, we look ahead one step and go over all possible actions at each iteration to find the maximum. Moreover, the only difference is that in the value iteration algorithm, we take the maximum number of possible actions. Instead of evaluating and then improving, the value iteration algorithm updates the state value function in a single step. In particular, this is possible by calculating all possible rewards by looking ahead. Finally, the value iteration algorithm is guaranteed to converge to the optimal values.</p><h3 id="Implementation-1"><a href="#Implementation-1" class="headerlink" title="Implementation"></a>Implementation</h3><p>This is the entire code of value iteration implementation. I will break down every line and explain.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">value_iteration</span>(<span class="params">states, actions, transition_prob, reward, discount_factor=<span class="number">0.9</span>, theta=<span class="number">1e-6</span></span>):</span><br><span class="line"></span><br><span class="line">    value_function = np.zeros(<span class="built_in">len</span>(states))</span><br><span class="line"></span><br><span class="line">    policy = np.zeros(<span class="built_in">len</span>(states), dtype=<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">            v = value_function[s]</span><br><span class="line">            action_values = np.zeros(<span class="built_in">len</span>(actions))</span><br><span class="line">            <span class="keyword">for</span> a <span class="keyword">in</span> actions:</span><br><span class="line">                action_value = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> next_s <span class="keyword">in</span> states:</span><br><span class="line">                    prob = transition_prob.get((s, a, next_s), <span class="number">0</span>)</span><br><span class="line">                    action_value += prob * (reward.get((s, a, next_s), <span class="number">0</span>) + discount_factor * value_function[next_s])</span><br><span class="line">                action_values[a] = action_value</span><br><span class="line">            value_function[s] = <span class="built_in">max</span>(action_values)</span><br><span class="line">            delta = <span class="built_in">max</span>(delta, <span class="built_in">abs</span>(v - value_function[s]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> delta &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">        action_values = np.zeros(<span class="built_in">len</span>(actions))</span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> actions:</span><br><span class="line">            action_value = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> next_s <span class="keyword">in</span> states:</span><br><span class="line">                prob = transition_prob.get((s, a, next_s), <span class="number">0</span>)</span><br><span class="line">                action_value += prob * (reward.get((s, a, next_s), <span class="number">0</span>) + discount_factor * value_function[next_s])</span><br><span class="line"></span><br><span class="line">            action_values[a] = action_value</span><br><span class="line">        policy[s] = np.argmax(action_values)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> policy, value_function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">states = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">actions = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">transition_prob = {</span><br><span class="line">    (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>): <span class="number">0.5</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>): <span class="number">0.5</span>,</span><br><span class="line">    (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>): <span class="number">0.2</span>, (<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>): <span class="number">0.8</span>,</span><br><span class="line">    (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>): <span class="number">0.7</span>, (<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>): <span class="number">0.3</span>,</span><br><span class="line">    (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>): <span class="number">0.6</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>): <span class="number">0.4</span>,</span><br><span class="line">    (<span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>): <span class="number">1.0</span>,</span><br><span class="line">    (<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>): <span class="number">0.5</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>): <span class="number">0.5</span>,</span><br><span class="line">}</span><br><span class="line">reward = {</span><br><span class="line">    (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>): <span class="number">1</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>): <span class="number">1</span>,</span><br><span class="line">    (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>): <span class="number">0</span>, (<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>): <span class="number">1</span>,</span><br><span class="line">    (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>): <span class="number">1</span>, (<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>): <span class="number">2</span>,</span><br><span class="line">    (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>): <span class="number">0</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>): <span class="number">3</span>,</span><br><span class="line">    (<span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>): <span class="number">0</span>,</span><br><span class="line">    (<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>): <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>): <span class="number">0</span>,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">policy, value_function = value_iteration(states, actions, transition_prob, reward)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Optimal Policy:"</span>, policy)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Value Function:"</span>, value_function)</span><br></pre></td></tr></tbody></table></figure><p>It will be much easier to understand the algorithm with a practice. So we can try to explain value iteration well by a simple implementation. We first import NumPy, of course. Then definite a function, which is the main character of this section: value iteration. This function requires several input values including states, actions, transition probability, reward, discount factor and theta. In this implementation, $γ=0.9$,$$θ=1^{−6}$$.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">defvalue_iteration(states, actions, transition_prob, reward, discount_factor=<span class="number">0.9</span>, theta=<span class="number">1e-6</span>):</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">    Performs value iteration for a given MDP.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param states: List of states</span></span><br><span class="line"><span class="string">    :param actions: List of actions</span></span><br><span class="line"><span class="string">    :param transition_prob: A dictionary that maps (state, action, next_state) to the transition probability</span></span><br><span class="line"><span class="string">    :param reward: A dictionary that maps (state, action, next_state) to a reward</span></span><br><span class="line"><span class="string">    :param discount_factor: Discount factor (gamma)</span></span><br><span class="line"><span class="string">    :param theta: A threshold for convergence</span></span><br><span class="line"><span class="string">    :return: A tuple (policy, value_function)</span></span><br><span class="line"><span class="string">    """</span></span><br></pre></td></tr></tbody></table></figure><p>In this function, we need to create two arrays for the storage of possible state and policy. State is easy to understand, but what is “policy”?</p><p>A policy is a strategy that an agent uses in pursuit of goals. The policy dictates the actions that the agent takes as a function of the agent’s state and the environment. In reinforcement learning, a policy is a strategy used by an agent to determine its actions at any given state. Formally, a policy is a mapping from states of the environment to actions to be taken when in those states. It can be deterministic or stochastic.</p><blockquote><p>Deterministic Policy: This type of policy maps each state to a specific action. If π is a deterministic policy and s is a state, then π(s) is the action taken when in state s.</p></blockquote><blockquote><p>Stochastic Policy: This type of policy provides a probability distribution over actions for each state. If π is a stochastic policy and s is a state, then π(a|s) represents the probability of taking action a when in state s.</p></blockquote><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">value_function = np.zeros(<span class="built_in">len</span>(states))</span><br><span class="line">   policy = np.zeros(<span class="built_in">len</span>(states), dtype=<span class="built_in">int</span>)</span><br></pre></td></tr></tbody></table></figure><p>Then we enter the iteration of value function. Before start iteration, we have to define a variable called Δ. It is a variable used to record the variation of value function in a iteration and can judge whether the iteration is converging. <code>for s in states</code> is a order asking agent to go through every state, <code>v = value_function[s]</code> is to store all the result of s in v. These are the pre-moves of iteration.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    delta = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">        v = value_function[s]</span><br><span class="line">        action_values = np.zeros(<span class="built_in">len</span>(actions))</span><br></pre></td></tr></tbody></table></figure><p>The result of value function comes from the maximum value of action values. We create an empty array for the storage of action values. Firstly initialize action value, then for each action, we go through every possible next state. After cumulating action value based on bellman function and store them as an array, we can obtain the maximum action value and take it as the value function value of current state.</p><p>In order to judge the convergency, update delta (Δ) at the end of value function calculation. <code>abs(v - value_function[s])</code> is the variation of value function. We determine the value of delta by comparing the value between previous Δ and new Δ and taking the maximum value. Cycle will ceased if the update range smaller than θ.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">action_values = np.zeros(<span class="built_in">len</span>(actions))</span><br><span class="line">   <span class="keyword">for</span> a <span class="keyword">in</span> actions:</span><br><span class="line">        <span class="comment"># go through every possible action</span></span><br><span class="line">               action_value = <span class="number">0</span></span><br><span class="line">           <span class="comment"># initialize action value</span></span><br><span class="line">               <span class="keyword">for</span> next_s <span class="keyword">in</span> states:</span><br><span class="line">               <span class="comment"># go through every possible next state</span></span><br><span class="line">                   prob = transition_prob.get((s, a, next_s), <span class="number">0</span>)</span><br><span class="line">               <span class="comment"># achieve transition probability (0 if not exist)</span></span><br><span class="line">                   action_value += prob * (reward.get((s, a, next_s), <span class="number">0</span>) + discount_factor * value_function[next_s])</span><br><span class="line">               <span class="comment"># cumulative action value based on bellman function</span></span><br><span class="line">               action_values[a] = action_value</span><br><span class="line">               <span class="comment"># store result in array of action value</span></span><br><span class="line">           value_function[s] = <span class="built_in">max</span>(action_values)</span><br><span class="line">           <span class="comment"># update value fuction and obtain maximum value among all the possible action value</span></span><br><span class="line">           delta = <span class="built_in">max</span>(delta, <span class="built_in">abs</span>(v - value_function[s]))</span><br><span class="line">           <span class="comment"># update delta and record maximum variation</span></span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> delta &lt; theta:</span><br><span class="line">           <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure><p>Simultaneously, we try to find the optimal policy. Basically trying to find the action which can bring maximum action value in each state then form a sequence by combining these actions.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">   <span class="comment"># go through every state</span></span><br><span class="line">       action_values = np.zeros(<span class="built_in">len</span>(actions))</span><br><span class="line">       <span class="comment"># initialize array</span></span><br><span class="line">       <span class="keyword">for</span> a <span class="keyword">in</span> actions:</span><br><span class="line">       <span class="comment"># go through every possible action</span></span><br><span class="line">           action_value = <span class="number">0</span></span><br><span class="line">           <span class="comment"># initialize current action value</span></span><br><span class="line">           <span class="keyword">for</span> next_s <span class="keyword">in</span> states:</span><br><span class="line">               prob = transition_prob.get((s, a, next_s), <span class="number">0</span>)</span><br><span class="line">               action_value += prob * (reward.get((s, a, next_s), <span class="number">0</span>) + discount_factor * value_function[next_s])</span><br><span class="line">               <span class="comment"># obtain current state, reward of taking action to next state &amp; calculate discounted future value</span></span><br><span class="line">           action_values[a] = action_value</span><br><span class="line">       policy[s] = np.argmax(action_values)</span><br><span class="line">       <span class="comment"># return optimal policy</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> policy, value_function</span><br></pre></td></tr></tbody></table></figure><p>Here is the exercise. In this virtual environment, we can observe three states: 0, 1, 2 and two actions: 0 and 1. The corresponding transition probability and reward table is as below. In this exercise, transition probability means the state have the probability of remaining at the same state. For example, if we are at state 0 and take action 0, we have 50% probability stay at the same state. In addition, (1, 0, 0): 0.7 means if we at state 1 and take action 0, we can have 70% probability return to state 0 ; (2, 1, 1): 0.5 means if we at state 2 and take action 1, we can have 50% probability return to state 1.</p><table><thead><tr><th>scenario</th><th>state</th><th>action</th><th>next state</th><th>transition probability</th><th>reward</th></tr></thead><tbody><tr><td>a11</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>1</td></tr><tr><td>a12</td><td>0</td><td>0</td><td>1</td><td>0.5</td><td>1</td></tr><tr><td>a21</td><td>0</td><td>1</td><td>0</td><td>0.2</td><td>0</td></tr><tr><td>a22</td><td>0</td><td>1</td><td>1</td><td>0.8</td><td>1</td></tr><tr><td>b11</td><td>1</td><td>0</td><td>0</td><td>0.7</td><td>1</td></tr><tr><td>b12</td><td>1</td><td>0</td><td>2</td><td>0.3</td><td>2</td></tr><tr><td>b21</td><td>1</td><td>1</td><td>1</td><td>0.6</td><td>0</td></tr><tr><td>b22</td><td>1</td><td>1</td><td>2</td><td>0.4</td><td>3</td></tr><tr><td>c11</td><td>2</td><td>0</td><td>2</td><td>1.0</td><td>0</td></tr><tr><td>c21</td><td>2</td><td>1</td><td>1</td><td>0.5</td><td>1</td></tr><tr><td>c22</td><td>2</td><td>1</td><td>2</td><td>0.5</td><td>0.5</td></tr></tbody></table><p><img src="https://s2.loli.net/2024/08/02/hm2qLdJyUbg6ZN3.png" alt="image.png"></p><center>Diagram of state, action and transition</center><p>Since we have complete all the requisite, we can run the code and get the result.</p><p><strong>Optimal Policy: 0→0→1</strong></p><p><strong>Value Function: [10.16927311, 10.20689125, 9.26018305]</strong></p><h2 id="Policy-iteration"><a href="#Policy-iteration" class="headerlink" title="Policy iteration"></a>Policy iteration</h2><p>The other common way that MDPs are solved is using <strong>policy iteration</strong> – an approach that is similar to value iteration. While value iteration iterates over value functions, policy iteration iterates over policies themselves, creating a strictly improved policy in each iteration (except if the iterated policy is already optimal).</p><p>Policy iteration first starts with some (non-optimal) policy, such as a random policy, and then calculates the value of each state of the MDP given that policy — this step is called <strong>policy evaluation</strong>. It then updates the policy itself for every state by calculating the expected reward of each action applicable from that state.</p><p>The basic idea here is that policy evaluation is easier to computer than value iteration because the set of actions to consider is fixed by the policy that we have so far.</p><h3 id="Policy-evaluation"><a href="#Policy-evaluation" class="headerlink" title="Policy evaluation"></a>Policy evaluation</h3><p><strong>policy evaluation</strong> is an evaluation of the expected reward of a policy. Vπ(s), the expected reward of policy π from state s, is the weighted average of reward of the possible state sequences defined by that policy times their probability given π. Policy evaluation can be defined by the following equation:<br>$$<br> V^π(s)=∑P_{π(s)}(s^′|s)[r(s,a,s^′)+γV^π(s^′)]\tag{4}<br>$$<br>Where $V^π(s)=0$ is for terminal state. Note that this is very similar to the Bellman equation, except $V^π(s)$ is not the value of the best action, but instead just as the value for $π(s)$, the action that would be chosen in s by the policy $π$. Note the expression $P_{π(s)}(s^′∣s)$ instead of $P_a(s^′∣s)$, which means we only evaluate the action that the policy defines.</p><h3 id="Policy-improvement"><a href="#Policy-improvement" class="headerlink" title="Policy improvement"></a>Policy improvement</h3><p>If we have a policy and we want to improve it, we can use <strong>policy improvement</strong> to change the policy (that is, change the actions recommended for states) by updating the actions it recommends based on $V(s)$ that we receive from the policy evaluation.</p><p>Let Qπ(s,a) be the expected reward from s when doing a first and then following the policy π. Recall from the chapter on Markov Decision Processes that we define define this as:<br>$$<br>Q_π(s,a)=∑P_a(s^′|s)[r(s,a,s^′)+γV^π(s^′)\tag{5}<br>$$<br>If there is an action a make $Q_π(s,a)&gt;Q_π(s,π(s))$, then the policy π can be <strong>strictly improved</strong> by setting $π(s)←a$. This will improve the overall policy.</p><h3 id="Implementation-2"><a href="#Implementation-2" class="headerlink" title="Implementation"></a>Implementation</h3><p>In this implementation, we have two states and 2 actions. reward during iteration can be 0 or 1. Discount factor $γ=0.9$.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">states = [<span class="string">'1'</span>, <span class="string">'2'</span>]</span><br><span class="line">actions = [<span class="string">'a'</span>, <span class="string">'b'</span>]</span><br><span class="line">rewards = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">discount_factor = <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line">q_value = {states[<span class="number">0</span>]: {actions[<span class="number">0</span>]: <span class="number">0</span>, actions[<span class="number">1</span>]: <span class="number">0</span>}, states[<span class="number">1</span>]: {actions[<span class="number">0</span>]: <span class="number">0</span>, actions[<span class="number">1</span>]: <span class="number">0</span>}} </span><br><span class="line"></span><br><span class="line">pi = {states[<span class="number">0</span>]: {actions[<span class="number">0</span>]: <span class="number">0.5</span>, actions[<span class="number">1</span>]: <span class="number">0.5</span>}, states[<span class="number">1</span>]: {actions[<span class="number">0</span>]: <span class="number">0.5</span>, actions[<span class="number">1</span>]: <span class="number">0.5</span>}}</span><br></pre></td></tr></tbody></table></figure><p>Code shown below is the transition probability and reward. We can visually obtain information through table below.</p><table><thead><tr><th>Scenario</th><th>next state</th><th>transition probability</th><th>reward</th></tr></thead><tbody><tr><td>1a1</td><td>1</td><td>1/3</td><td>0</td></tr><tr><td>1a2</td><td>2</td><td>2/3</td><td>1</td></tr><tr><td>1b1</td><td>1</td><td>2/3</td><td>0</td></tr><tr><td>1b2</td><td>2</td><td>1/3</td><td>1</td></tr><tr><td>2a1</td><td>1</td><td>1/3</td><td>0</td></tr><tr><td>2a2</td><td>2</td><td>2/3</td><td>1</td></tr><tr><td>2b1</td><td>1</td><td>2/3</td><td>0</td></tr><tr><td>2b2</td><td>2</td><td>1/3</td><td>1</td></tr></tbody></table><p>It will be more clearer when explaining the meaning through one of the scenarios. For example, we are at state 1, if we take action $a$ , we have a $\frac{2}{3}$ probability of reaching the next state and receive reward = 0 or $\frac 1 3$ probability of staying at the same state and receive reward = 1.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">p_s_r</span>(<span class="params">state, action</span>):</span><br><span class="line">    <span class="keyword">if</span> state == <span class="string">"1"</span>:</span><br><span class="line">        <span class="keyword">if</span> action == <span class="string">"a"</span>:</span><br><span class="line">            <span class="keyword">return</span> ((<span class="number">1.0</span> / <span class="number">3</span>, <span class="string">"1"</span>, <span class="number">0</span>),</span><br><span class="line">                    (<span class="number">2.0</span> / <span class="number">3</span>, <span class="string">"2"</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> ((<span class="number">2.0</span> / <span class="number">3</span>, <span class="string">"1"</span>, <span class="number">0</span>),</span><br><span class="line">                    (<span class="number">1.0</span> / <span class="number">3</span>, <span class="string">"2"</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">if</span> state == <span class="string">"2"</span>:</span><br><span class="line">        <span class="keyword">if</span> action == <span class="string">"a"</span>:</span><br><span class="line">            <span class="keyword">return</span> ((<span class="number">1.0</span> / <span class="number">3</span>, <span class="string">"1"</span>, <span class="number">0</span>),</span><br><span class="line">                    (<span class="number">2.0</span> / <span class="number">3</span>, <span class="string">"2"</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> ((<span class="number">2.0</span> / <span class="number">3</span>, <span class="string">"1"</span>, <span class="number">0</span>),</span><br><span class="line">                    (<span class="number">1.0</span> / <span class="number">3</span>, <span class="string">"2"</span>, <span class="number">1</span>))</span><br></pre></td></tr></tbody></table></figure><p>Next part is state evaluation function. In this function, we initialize the function and define the value of θ. <code>deepcopy</code> means to copy current V value to old V value for judgement of convergence. Then, assemble to value iteration, we go through every possible action in every state and obtain corresponding transition probability and reward. After that, we calculate Q value (action value) and V value. If the variation of V value smaller than θ, we can conclude that the function is converged and break the cycle.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">policy_evaluate</span>():</span><br><span class="line">    v_value = {states[<span class="number">0</span>]: <span class="number">0</span>, states[<span class="number">1</span>]: <span class="number">0</span>}</span><br><span class="line">    threshold = <span class="number">0.0001</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        v_value_old = copy.deepcopy(v_value)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">            temp_v = <span class="number">0</span></span><br><span class="line"><span class="comment"># temp_q = 0</span></span><br><span class="line">            <span class="keyword">for</span> a, p <span class="keyword">in</span> pi[s].items():</span><br><span class="line">                temp_q = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> t <span class="keyword">in</span> p_s_r(s, a):</span><br><span class="line">                    p_s_s1, s_, r = t[<span class="number">0</span>], t[<span class="number">1</span>], t[<span class="number">2</span>]</span><br><span class="line">                    temp_q += p_s_s1 * (r + discount_factor * v_value[s_])</span><br><span class="line">                q_value[s][a] = temp_q</span><br><span class="line">                temp_v += p * temp_q</span><br><span class="line">            v_value[s] = temp_v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(v_value)):</span><br><span class="line">             delta += np.<span class="built_in">abs</span>(v_value[states[i]] - v_value_old[states[i]])</span><br><span class="line">        <span class="keyword">if</span> delta &lt;= threshold:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> v_value</span><br></pre></td></tr></tbody></table></figure><p>Next, we update policy based on the V value we got. Before we enter the cycle, we have to clarify what is <code>done = True</code> . <code>done = True</code> means cease the operation when the policy becomes stable.</p><p>The function, <code>policy_improve</code> is used to find out the optimal action in current state and turn its probability of been selected to 1. Meanwhile, the probability of choosing other actions will be 0.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">policy_improve</span>(<span class="params">v</span>):</span><br><span class="line">    done = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">        action = <span class="built_in">max</span>(q_value[s],key=q_value[s].get)</span><br><span class="line">    <span class="comment"># print(action)</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> pi[s]:</span><br><span class="line">            <span class="keyword">if</span> k == action:</span><br><span class="line">                <span class="keyword">if</span> pi[s][k] != <span class="number">1.0</span>:</span><br><span class="line">                    pi[s][k] = <span class="number">1.0</span></span><br><span class="line">                    done = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pi[s][k] = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">return</span> done</span><br></pre></td></tr></tbody></table></figure><p>The iteration in this implementation is quite simple - only 2 times. Activate the code we can get the results shown as below:</p><table><thead><tr><th>State</th><th>1</th><th>2</th></tr></thead><tbody><tr><td>V value</td><td>6.666339540673712</td><td>6.666353680224912</td></tr></tbody></table><table><thead><tr><th>Q value</th><th>State 1</th><th>State 2</th></tr></thead><tbody><tr><td>action a</td><td>6.666339540673712</td><td>6.666353680224912</td></tr><tr><td>action b</td><td>6.333001354313227</td><td>6.333029633415626</td></tr></tbody></table><table><thead><tr><th>State</th><th>1</th><th>2</th></tr></thead><tbody><tr><td>Policy</td><td>a = 1, b = 0</td><td>a = 1, b = 0</td></tr></tbody></table><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">'__main__'</span>:</span><br><span class="line">    is_done =<span class="literal">False</span></span><br><span class="line">    i =<span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> is_doneisFalse:</span><br><span class="line">        v = policy_evaluate()</span><br><span class="line">        is_done = policy_improve(v)</span><br><span class="line">        i +=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Policy-Iteration converged at step %d.'</span> % i)</span><br><span class="line"><span class="built_in">print</span>(v)</span><br><span class="line"><span class="built_in">print</span>(q_value)</span><br><span class="line"><span class="built_in">print</span>(pi)</span><br></pre></td></tr></tbody></table></figure><h2 id="Difference-between-value-iteration-and-policy-iteration"><a href="#Difference-between-value-iteration-and-policy-iteration" class="headerlink" title="Difference between value iteration and policy iteration"></a>Difference between value iteration and policy iteration</h2><table><thead><tr><th>Aspect</th><th>Value iteration</th><th>Policy iteration</th></tr></thead><tbody><tr><td>Methodology</td><td>Iteratively updates value functions until convergence</td><td>Alternates between policy evaluation and improvement</td></tr><tr><td>Goal</td><td>Converges to optimal value function</td><td>Converges to the optimal policy</td></tr><tr><td>Execution</td><td>Directly computes value functions</td><td>Evaluate and improve policies sequentially</td></tr><tr><td>Complexity</td><td>Typically simpler to implement and understand</td><td>Involves more steps and computations</td></tr><tr><td>Convergence</td><td>May converge faster in some scenarios</td><td>Generally converges slower but yields better policies</td></tr></tbody></table><p>In summary, both value iteration and policy iteration are effective methods for solving RL problems and deriving optimal policies. Value iteration directly computes optimal value functions iteratively, which can converge faster in some cases and is generally simpler to implement. On the other hand, policy iteration alternates between evaluating and improving policies, resulting in slower convergence but potentially yielding better policies overall. Understanding the differences between these approaches is crucial for selecting the most suitable algorithm based on the problem requirements and computational constraints.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://ucm14.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Reinforcement Learning" scheme="http://ucm14.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Mason&#39;s Radio#2</title>
    <link href="http://ucm14.github.io/2024/10/01/Mason-s-Radio-2/"/>
    <id>http://ucm14.github.io/2024/10/01/Mason-s-Radio-2/</id>
    <published>2024-10-01T15:07:04.000Z</published>
    <updated>2024-10-01T16:11:46.605Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p> <font size="8"> <strong>The greatest album in 1999.</strong></font></p></blockquote><iframe style="border-radius:12px" src="https://open.spotify.com/embed/album/29U9LtzSF0ftWiLNNw1CP6?utm_source=generator" width="100%" height="480" frameborder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe><div style="text-align: center;">    <iframe width="560" height="315" src="https://www.youtube.com/embed/o1sUaVJUeB0?si=nMqpRoT2_Hzlc9xY&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></div><center> "First Love" Music Video (Live Ver.) <center><div style="text-align: center;">    <iframe width="560" height="315" src="https://www.youtube.com/embed/-9DxpPiE458?si=zU7CIfKNUHOi2-MO&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></div><center> "Automatic" Music Video <center></center></center></center></center>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Music" scheme="http://ucm14.github.io/categories/Music/"/>
    
    
    <category term="R&amp;B" scheme="http://ucm14.github.io/tags/R-B/"/>
    
    <category term="Utada Hikaru" scheme="http://ucm14.github.io/tags/Utada-Hikaru/"/>
    
    <category term="J-POP" scheme="http://ucm14.github.io/tags/J-POP/"/>
    
  </entry>
  
  <entry>
    <title>National Day holiday</title>
    <link href="http://ucm14.github.io/2024/10/01/National-Day-holiday/"/>
    <id>http://ucm14.github.io/2024/10/01/National-Day-holiday/</id>
    <published>2024-10-01T12:36:31.000Z</published>
    <updated>2024-10-01T14:02:42.820Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>It is the first day of National Day holiday in 2024. I made a regrettable decision: go out. I was planned to buy some new clothes for autumn and winter since the climate had dropped drastically after a heavy rain. Due to multiple times of unpleasant online shopping experiences, I decided to try those clothes on in a physical store.</p><p>The crowd already made me feel dizzy when I exit the subway. To be honest, it’s been a long time since I’ve seen so many people crowded in the subway station. Last time I saw a resemble scenery was in 30 the April when I on my way home at the train station, I suddenly felt exhausted at the moment stepping into the railway station. I extremely dislike such environment which is too noisy and irritating.</p><p align="center"><img src="https://s2.loli.net/2024/10/01/gDZbYCABLt2uWJV.jpg" width="600"><img src="https://s2.loli.net/2024/10/01/M9Fq7E1j2ADfuNS.jpg" width="600"></p><p>Then I fleet the station as soon as possible and straightly walked to the store. It turned out to be another horrible place because the store is on sale, thus more people crowded into the store to search for items and try them on, leaving what was once neatly organized in complete disarray. So I left again, trying to escape the heat and noise brought by the crowds. Of course, I didn’t buy anything, not only because the crowd but also the price is still high after giving discount.</p><p>The only place can bring me joy is the burger shop called “Egg Soul”. It was established in 2018 when I first came to Nanjing. I really love this burger shop and have came to this places many times that the money I spent here brought me a VIP discount. The vibe is the most iconic characteristics of itself. Colorful and dynamic graffiti on the wall, 90’s R&amp;B and Hip-Hop music together with aroma of food filling the space. The beef sandwich set is one of my favorites. Only a cup of diet coke, some French fries and a medium beef sandwich with green pepper, olive, beef and cheese. Terrible day, but thanks to Egg Soul I had a great supper.</p><p align="center"><img src="https://s2.loli.net/2024/10/01/z8YgJW3UMP7v4qN.jpg" width="600/"></p><p>Next time I will stay in my dorm and go nowhere, I swear. </p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Misc" scheme="http://ucm14.github.io/categories/Misc/"/>
    
    
    <category term="Daily Life" scheme="http://ucm14.github.io/tags/Daily-Life/"/>
    
  </entry>
  
  <entry>
    <title>First TOEFL result</title>
    <link href="http://ucm14.github.io/2024/09/29/First-TOEFL-result/"/>
    <id>http://ucm14.github.io/2024/09/29/First-TOEFL-result/</id>
    <published>2024-09-29T06:05:13.000Z</published>
    <updated>2024-09-29T07:21:13.958Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Assessment"><a href="#Assessment" class="headerlink" title="Assessment"></a>Assessment</h1><p><img src="https://s2.loli.net/2024/09/29/g9Li3zRQnyTdJwI.png" alt="TOEFL result.png"></p><p>I checked the official website this morning and found my latest, actually my first TOEFL test result had published. The physical transcript hasn’t arrived so I put the screenshot of my result here. I got 29/30 in reading, 22/30 in listening, 23/30 in speaking and 20/30 in writing. Not bad but still a pity.</p><h2 id="Reading-task"><a href="#Reading-task" class="headerlink" title="Reading task"></a>Reading task</h2><p>In reading task, the first passage talking about the features of some ethnic groups who feed on hunting. For example, most of them lived near river, which can provide them fish and other kind of aquatics. But still, they cultivate graves in case the lack of food in a long journey. The second passage is how astronomers research on moonquakes.  Both passages are simple to me, I can quickly locate the answers of the questions, let alone answering the synonyms. </p><h2 id="Listening-task"><a href="#Listening-task" class="headerlink" title="Listening task"></a>Listening task</h2><p>I bring the confidence reading task gave me to listening task. It is no exaggeration that I can understand every words in the audio , and I practiced a lot and keep 100% concentrated on the audio during the test. However, I just can’t find the right answer. I felt appalling when seeing my listening task only got 22 points, which is far away from what I thought. Maybe I should practice more.</p><h2 id="Speaking-task"><a href="#Speaking-task" class="headerlink" title="Speaking task"></a>Speaking task</h2><p>The result of speaking task surprised me a lot, more than listening task did because I didn’t prepare some templates for questions, and I was bothered by others during the test. No excuses, some of the ladies in the room talking like the Kardashians. I totally can’t understand why they prefer to be sound like Cali girls.</p><h2 id="Writing-task"><a href="#Writing-task" class="headerlink" title="Writing task"></a>Writing task</h2><p>I searched for templates of integrated writing and academic discussion and try to memorize them before the test began. On the contrary, I used none of them when writing the essay. The integrated writing is to summarize a discussion about the decline of  Chinook salmon. The academic discussion is asking whether you support advertising based on private information or not. I tried to make my points clear in the essay but 20 points is lower than what I supposed to have. I wonder how to improve my writing skills, probably some complex-and-long sentences and advanced vocabularies which no one can recognize.</p><table><thead><tr><th>Skill</th><th align="left">Level</th></tr></thead><tbody><tr><td>Reading</td><td align="left">Advanced (24–30) High-Intermediate (18–23) Low-Intermediate (4–17) Below Low-Intermediate (0–3)</td></tr><tr><td>Listening</td><td align="left">Advanced (22–30) High-Intermediate (17–21) Low-Intermediate (9–16) Below Low-Intermediate (0–8)</td></tr><tr><td>Speaking</td><td align="left">Advanced (25–30) High-Intermediate (20–24) Low-Intermediate (16–19) Basic (10–15) Below Basic (0–9)</td></tr><tr><td>Writing</td><td align="left">Advanced (24–30) High-Intermediate (17–23) Low-Intermediate (13–16) Basic (7–12) Below Basic (0–6)</td></tr></tbody></table><p><a href="https://www.ets.org/pdfs/toefl/toefl-ibt-performance-descriptors.pdf">https://www.ets.org/pdfs/toefl/toefl-ibt-performance-descriptors.pdf</a></p><p>Based on the descriptors provided by IBT, my reading skill is advanced, then come with high-intermediate listening/speaking/writing skills. But I don’t admit that can reflect my true English capability.</p><h1 id="Reflection"><a href="#Reflection" class="headerlink" title="Reflection"></a>Reflection</h1><p>After all, TOEFL is just a test. Of course it can reflect partial English ability of examinees, but we Chinese know hot to cope with test better than learn a language well. Those institutes teach students how to find shortcuts in exams. Nearly every kind of question has its corresponding answer template. I remembered a girl talked to his dad that she perfectly anticipated the writing task question. In addition, I found someone took a picture of the task shown in the screen and  uploaded to social media during the test with a  monitor hanging on everyone’s head. These made me wondering whether the test can identify people’s linguistic ability or only a certificate for study abroad. Also I found many examinees are born in 2007-2008, which means they decided to study in foreign universities. Do you think they will speak English more frequently? No, they won’t. Most of them still hanging with Chinese and only use English in email or shopping. The ultimate goal of the journey is for a foreign university certificate. Now back to the TOEFL test, do they really learn the English well? No one can give a determined answer. They probably can have a great marks in test, but be lame when talking in real-life.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Language" scheme="http://ucm14.github.io/categories/Language/"/>
    
    
    <category term="TOEFL" scheme="http://ucm14.github.io/tags/TOEFL/"/>
    
  </entry>
  
  <entry>
    <title>Mason&#39;s Radio#1</title>
    <link href="http://ucm14.github.io/2024/09/28/Mason-s-Radio-1/"/>
    <id>http://ucm14.github.io/2024/09/28/Mason-s-Radio-1/</id>
    <published>2024-09-28T04:53:26.000Z</published>
    <updated>2024-10-01T15:21:19.211Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/3v5o91PrUtf0nmO6j8J7dZ?utm_source=generator" width="100%" height="152" frameborder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe><blockquote><p><font size="6"> <strong>Best R&amp;B music and stage performance provided by XG.</strong></font></p></blockquote><div style="text-align: center;">    <iframe width="560" height="315" src="https://www.youtube.com/embed/18fe5rgmvYI?si=Vkr_KWsgwW5107tI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Music" scheme="http://ucm14.github.io/categories/Music/"/>
    
    
    <category term="XG" scheme="http://ucm14.github.io/tags/XG/"/>
    
    <category term="R&amp;B" scheme="http://ucm14.github.io/tags/R-B/"/>
    
  </entry>
  
  <entry>
    <title>Speaking Test</title>
    <link href="http://ucm14.github.io/2024/09/28/speaking-test/"/>
    <id>http://ucm14.github.io/2024/09/28/speaking-test/</id>
    <published>2024-09-28T03:10:54.000Z</published>
    <updated>2024-09-28T03:11:27.482Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="TOEFL口语总结"><a href="#TOEFL口语总结" class="headerlink" title="TOEFL口语总结"></a><strong>TOEFL口语总结</strong></h1><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a><strong>注意</strong></h2><ul><li>考察标准<ol><li>发音、语调和流畅度。使用连接词，<em>well, I think, what I am going to talk about is, to make it clear.</em> <strong>过多卡顿会扣分的</strong>。</li><li>语言使用，包括词汇和语法能力。</li><li>topic development，陈述话题的能力。</li></ol></li><li>规定时间没说完也许并没有关系，但上面说的卡顿问题还是有影响的。</li><li>还是得准备段子。</li><li>张涵书里提到综合口语为体现综合，最好不要强调<em>In the reading/listening passage</em>这种话；但其实应该关系不大，参考知乎这个问题下的回答：<a href="https://www.zhihu.com/question/22102496">如何准备托福口语题</a>。总之就是这些都是细节，像我们还是尽量先抓big picture吧。</li><li>网上看的经验大多是根据个人经历总结的（除了张涵是真正在ETS实习过……），可能他用方法A，效果好，但他经验分享里就写要坚决避免使用方法B，就可能没什么依据了。还是得自己甄别。</li></ul><h2 id="独立口语"><a href="#独立口语" class="headerlink" title="独立口语"></a><strong>独立口语</strong></h2><blockquote><p>For both below:</p><p>preparation time: 15 sec</p><p>response time: 45 sec</p></blockquote><h3 id="第一题：个人偏好"><a href="#第一题：个人偏好" class="headerlink" title="第一题：个人偏好"></a><strong>第一题：个人偏好</strong></h3><blockquote><p>Express and defend a personal choice from a given category.</p></blockquote><p>主要四类：<em>Person、Place、Object、Event/Activity</em>。</p><p>也可能是三选一，从题目给出的三个选项中选出你认为最好的那一个，但出现概率更低。</p><p><strong>解题方法</strong>：<em>Topic sentence - Supporting sentence - Examples/details</em>。</p><p><strong>例子</strong>：</p><blockquote><p>题目：Talk about a person you admire a lot and explain why you admire him or her. Use details and examples in your response.</p><p><strong>Topic Sentence 主题句</strong> (直接给出回答）: Allen Iverson, the NBA superstar, is definitely one of the people for whom I have a huge admiration.</p><p><strong>Supporting Sentence 支持句</strong>（给出支持主题句的原因）: I admire him a lot because he is such a hard-working guy that you would feel like there’s nothing he cannot do.</p><p><strong>Example/details 例子 / 细节</strong>（为前两个句子提供支撑） : Once I watched an interview of his coach in high school on NBC. He said that Allen was just super diligent. He was always the first person that arrived for the training, and always the last one to leave. He usually stayed for another 2 hours after all his teammates left for dinner. So it’s definitely his hard work that made him one of the most phenomenal players in the league.</p></blockquote><p><strong>注意</strong>：</p><ol><li>supporting sentence里的观点最好只有一个。多了讲不完，讲不好。</li><li>尽量使用熟悉表达。想求难可能反而说不好。</li><li>时间控制在42-45s之间。少于42s可能对分数有影响。</li></ol><h3 id="第二题：选择"><a href="#第二题：选择" class="headerlink" title="第二题：选择"></a><strong>第二题：选择</strong></h3><blockquote><p>Make and defend a personal choice between two contrasting behaviors or courses of action.</p></blockquote><p>题型为<strong>二选一</strong>或<strong>同意or反对</strong>。前者比例更高。</p><p><strong>解题方法</strong>：与第一题相同，仍是<em>Topic sentence - Supporting sentence - Examples/details</em></p><p><strong>注意</strong>：</p><ol><li>最好做明确选择。</li><li>选更有话说的观点，而非你赞同的观点。</li></ol><h2 id="综合口语"><a href="#综合口语" class="headerlink" title="综合口语"></a><strong>综合口语</strong></h2><blockquote><p>For the first two:</p><p>preparation time: 30 sec</p><p>response time: 60 sec</p><p><strong>For the latter two</strong>:</p><p>preparation time: 20 sec</p><p>response time: 60 sec</p></blockquote><h3 id="第三题：校园场景，联系观点解释原因"><a href="#第三题：校园场景，联系观点解释原因" class="headerlink" title="第三题：校园场景，联系观点解释原因"></a><strong>第三题：校园场景，联系观点解释原因</strong></h3><blockquote><p>a reading passage and a listening passage. Summarize the speaker’s opinion within the context of the reading passage.</p></blockquote><p>reading passage：</p><ol><li>学校声明，改变某种政策。</li><li>学生来信，建议校方做出改变。</li></ol><p>listening passage: 学生对话讨论reading passage中的内容。</p><p><strong>解题方法</strong>：</p><p>找信息点</p><ol><li>学校计划做什么</li><li>这么做的第一个原因</li><li>这么做的第二个原因</li><li>学生同意/反对</li><li>同意/反对的第一个原因</li><li>同意/反对的第二个原因</li></ol><p><strong>注意</strong>：</p><ol><li>重点在listening，因此后者比重应该更大。这意味着我们不应该花太多时间描述学校为什么出台这样的政策。</li><li>listening中有捧哏和逗哏的角色，仅逗哏者给出观点。</li><li>不掺入任何个人观点。</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">The school recently decide to (make a change about) XXX</span><br><span class="line">because</span><br><span class="line">1.they ... .</span><br><span class="line">2. And also ...</span><br><span class="line">The man/woman holds positive/negative attitudes towards it,</span><br><span class="line">(s)he thinks,</span><br><span class="line">1....</span><br><span class="line">2.Besides,...</span><br></pre></td></tr></tbody></table></figure><h3 id="第四题：学术课程，抽象-具体"><a href="#第四题：学术课程，抽象-具体" class="headerlink" title="第四题：学术课程，抽象/具体"></a><strong>第四题：学术课程，抽象/具体</strong></h3><blockquote><p>a reading passage: broadly define a term, process, idea, etc.</p><p>a lecture: provide examples and <strong>specific</strong> information</p><p>Combine and convey important information from them.</p></blockquote><p>reading passage：定义一个学术概念，提供其背景知识、定义、特点等。</p><p>lecture：通过例子，对抽象概念具体解释</p><p><strong>解题方法</strong>：</p><ol><li>找出reading passage的定义句。</li><li>找出lecture的例子。</li></ol><p><strong>例子</strong>：（具体见书）</p><blockquote><p>The professor is talking about a psychological concept called…（名字）</p><p>which refers to ….(定义)</p><p>And he gives us two examples to illustrate this concept.</p><p>In the first case, ….</p><p>In the second example,…</p></blockquote><p><strong>注意</strong>：</p><ol><li>仍是刚才说的，听力部分更重要。</li><li>定义句最好能记下。不用特地在说的时候换种说法以显示自己表达能力。</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">The professor is talk about a XXX(the field) concept called ...,</span><br><span class="line">which refers to ...(the definition)</span><br><span class="line">And he gives us two examples to illustrate the concept.</span><br><span class="line">In the first case,...</span><br><span class="line">In the second example,...</span><br></pre></td></tr></tbody></table></figure><h3 id="第五题：校园场景，问题-解决方案"><a href="#第五题：校园场景，问题-解决方案" class="headerlink" title="第五题：校园场景，问题/解决方案"></a><strong>第五题：校园场景，问题/解决方案</strong></h3><blockquote><p>a listening passage: conversation about a student-related problem and two possible solutions.</p><p>Demonstrate an understanding of the problem and express an opinion about solving it.</p></blockquote><p>两个学生交流，其中一个提出自己的问题，另一个人给出两种方案，两人可能对方案有些评价。</p><p><strong>解题方法</strong>：</p><ul><li>找信息点<ol><li>什么问题</li><li>解决方案1</li><li>解决方案2</li></ol></li><li>填充信息点<ol><li>你的选择</li><li>你的理由</li></ol></li></ul><p><strong>注意</strong>：</p><ol><li>这是唯一一个需要个人观点的题目。《Speaking Tips》建议的时间分配是：10 sec for the problem, 17 sec for each solution, and 10 sec for your choice and why, and about 6 sec pausing <strong>throughout</strong> your answer.（注意不是说你最后留出整整6s）</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The problem the man/woman encountered is ...</span><br><span class="line">The woman/man suggests that ...</span><br><span class="line">Or ...</span><br><span class="line">If I were the man/woman, I prefer the first/latter solution, because</span><br><span class="line">1. the disadvantage of the other one</span><br><span class="line">2. the advantage of the one you choose.</span><br></pre></td></tr></tbody></table></figure><h3 id="第六题：-学术课程，摘要"><a href="#第六题：-学术课程，摘要" class="headerlink" title="第六题： 学术课程，摘要"></a><strong>第六题： 学术课程，摘要</strong></h3><blockquote><p>a lecture: explain a term or concept, together with some examples.</p><p>Summarize the lecture and demonstrate an understanding of the relationship between the examples and the overall topic.</p></blockquote><p>一个学术概念的两个方面。</p><p><strong>解题方法</strong>：</p><p>找信息点</p><ol><li>教授谈论的主题</li><li>主题的第一个方面</li><li>第一个方面的例子</li><li>主题的的第二个方面</li><li>第二个方面的例子</li></ol><p><strong>注意</strong>：</p><ol><li><strong>例子比概念重要。</strong></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Language" scheme="http://ucm14.github.io/categories/Language/"/>
    
    
    <category term="TOEFL" scheme="http://ucm14.github.io/tags/TOEFL/"/>
    
  </entry>
  
  <entry>
    <title>Writing Template</title>
    <link href="http://ucm14.github.io/2024/09/28/Writing-Template/"/>
    <id>http://ucm14.github.io/2024/09/28/Writing-Template/</id>
    <published>2024-09-28T03:04:25.000Z</published>
    <updated>2024-09-28T03:10:30.210Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Integrated-Writing-Templates"><a href="#Integrated-Writing-Templates" class="headerlink" title="Integrated Writing Templates"></a>Integrated Writing Templates</h1><h2 id="Template-1"><a href="#Template-1" class="headerlink" title="Template 1"></a><strong>Template 1</strong></h2><blockquote><p>Already widely used</p></blockquote><p>The reading passage investigates the issue of…,  A, B, and C and the lecturer analyzes the same topic. However, he/she asserts all these three theories have flaws.</p><p>First and foremost, although the author of the reading passage suggests that….( <strong>1-2 statements in reading</strong>）， the presenter in the listening material contends that… . This is because…, which means+the professor ……   Evidently,  <strong>the professor’s argument challenges its counterpart in the reading</strong>.</p><p>Moreover, contrary to the statement in the reading that…, the professor indicates that… . Then he/she <strong>backs up this point with the fact that</strong>… . In other words, … .</p><p>Last but not least, whereas the author of the reading insists that…, the lecturer <strong>proves that this claim is indefensible  by pointing out that</strong>… because…</p><h2 id="Template-2"><a href="#Template-2" class="headerlink" title="Template 2"></a><strong>Template 2</strong></h2><blockquote><p>Come from TPO 26</p></blockquote><p>The lecture opposes the reading’s opinion that（<strong>reading’s opinion</strong>）</p><p>zebra mussel would severely threat the freshwater fish populations in North Ameria, <strong>contending that the arguments in the reading passage are not convincing</strong>.</p><p>Initially, while the reading stated that（<strong>first statement</strong>）</p><p>according to zebra mussel’s history, their spread could not be stopped, the professor held an opposite opinion. He（she） claimed that（<strong>first argument</strong>） in the past people could not stop mussel from spreading because they lack of knowledge. Nevertheless, now people could pour out the freshwater in the ballast water and refill with ocean water, which would kill mussel.</p><p><strong>In addition, the professor casted doubt on the reading’s assumption that</strong>（<strong>second statement</strong>） mussels would dominate any new habitat, arguing that（<strong>second argument</strong>） they could only occupy the place at beginning. It would not take a long time before some kinds of birds realize that mussels are new available food. Since a bird could eat a lot of mussels, they would not dominate the habitat.</p><p>Moreover, the reading <strong>maintained</strong> that（<strong>third statement</strong>） zebra mussels would give rise to a decline in the overall fish population, which is contradictory to what the professor said.Even if mussels could become dominant, he argued（<strong>third argument</strong>）, the overall population would increase, as the existence of mussels would benefits other kinds of fish. For example, their waste could provide nutrient to bottom-feed fish, whose population would therefore grow.</p><h1 id="Academic-Discussion-Template"><a href="#Academic-Discussion-Template" class="headerlink" title="Academic Discussion Template"></a>Academic Discussion Template</h1><ol><li>This is a controversial topic, but I …..(support) A’s idea that… (27 words)</li><li>To be more specific …… (29 words)</li><li>For example …… (29 words)</li><li>In contrary, B’s opinion is …… (27 words)</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Language" scheme="http://ucm14.github.io/categories/Language/"/>
    
    
    <category term="TOEFL" scheme="http://ucm14.github.io/tags/TOEFL/"/>
    
  </entry>
  
  <entry>
    <title>Vocab</title>
    <link href="http://ucm14.github.io/2024/09/28/TOEFL-Practice/"/>
    <id>http://ucm14.github.io/2024/09/28/TOEFL-Practice/</id>
    <published>2024-09-28T03:00:52.000Z</published>
    <updated>2024-09-28T03:04:10.213Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Adjective"><a href="#Adjective" class="headerlink" title="Adjective"></a>Adjective</h1><p>贫穷的：poor, needy, impoverished, poverty, stricken</p><p>富裕的：rich, wealthy, affluent, well-to-do, well-off</p><p>优秀的：excellent, eminent, top, outstanding</p><p>积极的，好的：good, conductive, beneficial, advantageous</p><p>消极的，不良的：bad, detrimental, baleful, undesirable</p><p>明显的：obvious, apparent, evident, manifest</p><p>健康的：healthy, robust, sound, wholesome</p><p>惊人的：surprising, amazing, extraordinary, miraculous</p><p>美丽的：attractive, gorgeous, eye-catching</p><p>有活力的：energetic, dynamic, vigorous, animated</p><p>流行的：popular, prevailing, prevalent, pervasive</p><h1 id="Verb"><a href="#Verb" class="headerlink" title="Verb"></a>Verb</h1><p>提高，加强：improve, enhance, promote, strengthen, optimize</p><p>引起：trigger, endanger, cause</p><p>解决：solve, resolve, address, tackle, cope with, deal with</p><p>拆除：destroy, tear down, knock down, eradicate</p><p>培养：develop, cultivate, foster, nurture</p><p>激发，鼓励：encourage, motivate, stimulate, spur</p><p>认为：think, assert, hold, claim, argue</p><p>完成：complete, fulfill, accomplish, achieve</p><p>保留：keep, preserve, retain, hold</p><p>有害于：destroy, impair, undermine, jeopardize</p><p>减轻：ease, alleviate, relieve, lighten</p><h1 id="Noun"><a href="#Noun" class="headerlink" title="Noun"></a>Noun</h1><p>影响：influence, impact</p><p>危险：danger, perils, hazard</p><p>污染：pollution, contamination</p><p>人类：human beings, mankind, human race</p><p>老人：the old, old people, the elderly, the aged, senior citizens</p><p>幸福：happiness, cheerfulness, well-beings</p><p>老师：teacher, instructor, educator, lecturer</p><p>青少年：youngster, adolescent</p><p>教育：education, schooling, upbringing, family parenting</p><p>优点：advantage, merits, superiority, virtue</p><p>责任：responsibility, obligation, duty, liability</p><p>能力：ability, capability, power, skill</p><p>职业：job, career, profession, employment</p><p>娱乐：entertainment, recreation, pastimes, enjoyment</p><p>孩子：offspring, descendants</p><p>刺激：stimulus</p><h1 id="Phrase"><a href="#Phrase" class="headerlink" title="Phrase"></a>Phrase</h1><p>充满了：be filled with, be awash with, be inundate with, be saturated with</p><p>努力：struggle for, aspire after, strive for, spare no efforts for</p><p>从事：embark on, take up, set about, go in for</p><p>在当代：in contemporary society, in present-day society, in this day and age</p><p>大量的：a host of, a multitude of, a vast number of, a vast amount of</p><h1 id="Sentence"><a href="#Sentence" class="headerlink" title="Sentence"></a>Sentence</h1><table><thead><tr><th>English</th><th>中文</th></tr></thead><tbody><tr><td>It is a must for sb to do sth</td><td>必须做</td></tr><tr><td>Be bound to</td><td>必然会</td></tr><tr><td>Under the instruction of</td><td>在。。。指导下</td></tr><tr><td>the real looks of</td><td>。。。的真面目</td></tr><tr><td>A significant step toward</td><td>向。。。迈出一大步</td></tr><tr><td>Be paved with</td><td>由。。。铺成</td></tr><tr><td>Do one’s utmost to</td><td>尽最大努力去做。。。</td></tr><tr><td>Deprive…of…</td><td>剥夺，使。。。丧失</td></tr><tr><td>Be associated with</td><td>与。。有关</td></tr><tr><td>Bring…to the limelight</td><td>使。。。成为注目的焦点</td></tr><tr><td>Be blessed with</td><td>享有</td></tr><tr><td>Excessive obsession with</td><td>过分沉迷于</td></tr><tr><td>Well informed about</td><td>对。。有深入的了解</td></tr><tr><td>Keep a blind eye to</td><td>对。。。视而不见</td></tr><tr><td>Lay the foundation for</td><td>为。。。打下基础</td></tr><tr><td>Endeavor to</td><td>尽力做</td></tr><tr><td>held in the palm of one’s hand</td><td>被掌握在某人手中</td></tr><tr><td>immerse oneself to</td><td>沉浸于。。。</td></tr><tr><td>Be of top priority…</td><td>。。。是最重要的</td></tr><tr><td>Be obliged to</td><td>有义务做</td></tr><tr><td>Prerequisite</td><td>前提，先决条件</td></tr><tr><td>From all works of life</td><td>来自各行各业</td></tr><tr><td>Bring into full play</td><td>充分发挥作用。</td></tr><tr><td>Facilitate</td><td>促进，帮助</td></tr><tr><td>Exert…on…</td><td>把。。。强加于。。。上</td></tr><tr><td>Boast</td><td>取得，拥有</td></tr><tr><td>Ensue</td><td>接着发生</td></tr><tr><td>Steel</td><td>使坚固</td></tr><tr><td>Destructive</td><td>破坏的， 有害的</td></tr><tr><td>Marvel at</td><td>惊叹于。。。</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Language" scheme="http://ucm14.github.io/categories/Language/"/>
    
    
    <category term="TOEFL" scheme="http://ucm14.github.io/tags/TOEFL/"/>
    
  </entry>
  
  <entry>
    <title>The very first blog</title>
    <link href="http://ucm14.github.io/2024/09/28/The-very-first-blog/"/>
    <id>http://ucm14.github.io/2024/09/28/The-very-first-blog/</id>
    <published>2024-09-27T18:09:57.000Z</published>
    <updated>2024-10-01T15:32:33.990Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Dear readers:</p><p>This is Minfeng here. My personal page has gone through many twists and turns, but it has finally been rebuilt for the third time. In the coming days, I will upload my previous personal learning records as soon as possible. Additionally, I will share other noteworthy experiences here.</p><p>I plan to first upload my learning records on reinforcement learning and my experiences preparing for the TOEFL exam, then my scientific research.</p><p>Of course, I will backup local files by uploading them to my GitHub repos :)</p><p>Best regards,</p><p>Minfeng</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;&#92;assets&#92;css&#92;APlayer.min.css&quot;&gt;&lt;script src=&quot;&#92;assets&#92;js&#92;APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Misc" scheme="http://ucm14.github.io/categories/Misc/"/>
    
    
    <category term="Notes" scheme="http://ucm14.github.io/tags/Notes/"/>
    
  </entry>
  
</feed>
