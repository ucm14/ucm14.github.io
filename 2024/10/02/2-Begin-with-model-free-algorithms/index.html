<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Model-based &#x2F; Model-free RLEverybody knows reinforcement learning is one of a machine learning methods aiming to make agent explore and find the optimal policy through interactions with environment an">
<meta property="og:type" content="article">
<meta property="og:title" content="2. Begin with model-free algorithms">
<meta property="og:url" content="http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/index.html">
<meta property="og:site_name" content="Mason&#39;s Radio">
<meta property="og:description" content="Model-based &#x2F; Model-free RLEverybody knows reinforcement learning is one of a machine learning methods aiming to make agent explore and find the optimal policy through interactions with environment an">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2024/08/03/4QSDOYacz9pHLAG.png">
<meta property="og:image" content="https://s2.loli.net/2024/08/03/qbufGrCwPdHk2oU.png">
<meta property="og:image" content="https://s2.loli.net/2024/08/03/kjtiXCEQDSoywsK.png">
<meta property="og:image" content="https://s2.loli.net/2024/08/03/D3tYshGI4uvSbdo.png">
<meta property="article:published_time" content="2024-10-02T06:20:50.000Z">
<meta property="article:modified_time" content="2024-10-02T14:40:39.484Z">
<meta property="article:author" content="Minfeng &quot;Mason&quot; Yu">
<meta property="article:tag" content="Reinforcement Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2024/08/03/4QSDOYacz9pHLAG.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>2. Begin with model-free algorithms</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 7.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Mason's Radio" type="application/atom+xml">
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2024/10/04/Lost-in-the-vibe/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2024/10/02/1-Value-iteration-vs-Policy-iteration/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&text=2. Begin with model-free algorithms"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&title=2. Begin with model-free algorithms"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&is_video=false&description=2. Begin with model-free algorithms"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=2. Begin with model-free algorithms&body=Check out this article: http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&title=2. Begin with model-free algorithms"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&title=2. Begin with model-free algorithms"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&title=2. Begin with model-free algorithms"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&title=2. Begin with model-free algorithms"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&name=2. Begin with model-free algorithms&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&t=2. Begin with model-free algorithms"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Model-based-Model-free-RL"><span class="toc-number">1.</span> <span class="toc-text">Model-based &#x2F; Model-free RL</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Monte-Carlo-MC"><span class="toc-number">2.</span> <span class="toc-text">Monte-Carlo (MC)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Overview"><span class="toc-number">2.1.</span> <span class="toc-text">Overview</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Q-Learning-the-first-step-of-model-free-RL"><span class="toc-number">3.</span> <span class="toc-text">Q Learning: the first step of model-free RL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Algorithm"><span class="toc-number">3.1.</span> <span class="toc-text">Algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Implementation"><span class="toc-number">3.2.</span> <span class="toc-text">Implementation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Environment"><span class="toc-number">3.2.1.</span> <span class="toc-text">Environment</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hyperparameters"><span class="toc-number">3.2.2.</span> <span class="toc-text">Hyperparameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#State-s-and-action-s"><span class="toc-number">3.2.3.</span> <span class="toc-text">State(s) and action(s)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reward-function"><span class="toc-number">3.2.4.</span> <span class="toc-text">Reward function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q-value-function"><span class="toc-number">3.2.5.</span> <span class="toc-text">Q value function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Results"><span class="toc-number">3.2.6.</span> <span class="toc-text">Results</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Sarsa-An-on-policy-RL-algorithm"><span class="toc-number">4.</span> <span class="toc-text">Sarsa: An on-policy RL algorithm</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Difference-between-on-policy-off-policy"><span class="toc-number">4.1.</span> <span class="toc-text">Difference between on-policy &amp; off-policy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Algorithm-1"><span class="toc-number">4.2.</span> <span class="toc-text">Algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Implementation-1"><span class="toc-number">4.3.</span> <span class="toc-text">Implementation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#code"><span class="toc-number">4.3.1.</span> <span class="toc-text">code</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Result"><span class="toc-number">4.3.2.</span> <span class="toc-text">Result</span></a></li></ol></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        2. Begin with model-free algorithms
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">Minfeng "Mason" Yu</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2024-10-02T06:20:50.000Z" class="dt-published" itemprop="datePublished">2024-10-02</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>
    </div>


      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/Reinforcement-Learning/" rel="tag">Reinforcement Learning</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h1 id="Model-based-Model-free-RL"><a href="#Model-based-Model-free-RL" class="headerlink" title="Model-based / Model-free RL"></a>Model-based / Model-free RL</h1><p>Everybody knows reinforcement learning is one of a machine learning methods aiming to make agent explore and find the optimal policy through interactions with environment and maximize cumulative reward. Before starting this passage, we have to understand what is model-free and what is model-based?</p>
<p><img src="https://s2.loli.net/2024/08/03/4QSDOYacz9pHLAG.png" alt="image.png"></p>
<h1 id="Monte-Carlo-MC"><a href="#Monte-Carlo-MC" class="headerlink" title="Monte-Carlo (MC)"></a>Monte-Carlo (MC)</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>The Monte Carlo algorithm is a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The core idea is to use randomness to solve problems that might be deterministic in principle. The key concepts of MC are: <strong>random sampling</strong>, <strong>estimation</strong> by taking the average of outcomes from random samples, applicability of <strong>high-dimensional spaces</strong>. It can be used in simulations, optimization, numerical integration, financial anticipation and statistical physics. Advantages and disadvantages of MC are shown in the table below.</p>
<table>
<thead>
<tr>
<th>Monte Carlo</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>Simple to implement</td>
<td>Can be computationally intensive</td>
</tr>
<tr>
<td>2</td>
<td>Flexible and can handle a wide range of problems</td>
<td>Convergence can be slow</td>
</tr>
<tr>
<td>3</td>
<td>Suitable for high-dimensional integrals</td>
<td>Results are probabilistic, not deterministic</td>
</tr>
</tbody></table>
<p>Here is a classic example of MC application: Estimate the value of $π$.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">defestimate_pi(num_samples):</span><br><span class="line">    inside_circle =<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _inrange(num_samples):</span><br><span class="line">        x = random.uniform(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">        y = random.uniform(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> x**<span class="number">2</span> + y**<span class="number">2</span> &lt;=<span class="number">1</span>:</span><br><span class="line">            inside_circle +=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> (inside_circle / num_samples) *<span class="number">4</span></span><br><span class="line"></span><br><span class="line">pi_estimate = estimate_pi(<span class="number">100000</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Estimated value of π:<span class="subst">{pi_estimate}</span>"</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>The result is <code>π: 3.13544</code>.</p>
<p>Assuming we have a state sequence under a policy π:<br>$$<br>[S_1,A_1,R_1,S_2,A_2,R_2,S_3,A_3,R_3,…,S_T,A_T,R_T]\tag{1}<br>$$<br>In MDP, the value function is:<br>$$<br>v_π(s)=E_π[G_t|S_t=s]G_t=R_{t+1}+γR_{t+2}+…+γ^{T−t−1}R_Tv_π(s)≈average(G_t)\tag{2}<br>$$<br>However, the average consumes so many storage. A better method is obtaining average value in iterations:<br>$$<br> μt=\frac 1 t ∑j=\frac 1 t x_j=\frac 1 t (x_t+\sum_{j=1}^{t−1}x_j)=μ_{t−1}+\frac 1 t (x_t−μ_{t−1})\tag{3}<br>$$</p>
<h1 id="Q-Learning-the-first-step-of-model-free-RL"><a href="#Q-Learning-the-first-step-of-model-free-RL" class="headerlink" title="Q Learning: the first step of model-free RL"></a>Q Learning: the first step of model-free RL</h1><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Q learning is a value-based RL algorithm. Thus, Q value is the fundamental variable in this algorithm, and this is the reason why this algorithm called “Q Learning”. Assuming we already know what is agent, what is environment, what are the states, the actions have to be discrete and the reward function, let us cut to the point and introduce the learning method of agent in Q Learning.<br>$$<br> Q(s,a)\leftarrow Q(s,a)+α(r+γmax_{a^′}Q(s^′,a^′)−Q(s,a))\tag{4}<br>$$<br>Equation above is the update method of Q value. α is learning rate (basically learning rate defines the pace of updating), γ is discount factor. $Q(s,a)$ is the action value of current state; $Q(s^′,a^′)$ is the action value of next state; $max_{a^′}Q(s^′,a^′)$ means choosing the maximum $Q(s^′,a^′)$ among all the possible actions.</p>
<p>The Q learning is quite similar to value iteration. We are going to explain the differences between Q learning and value iteration in detail through a simple implementation.</p>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h3><p>The environment is a 4x4 grid. Our target is to train the agent and let it successfully moves from (0, 0) to (3,3).</p>
<p><img src="https://s2.loli.net/2024/08/03/qbufGrCwPdHk2oU.png" alt="image.png"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">grid_size = <span class="number">4</span></span><br><span class="line">goal_state = (<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">start_state = (<span class="number">0</span>,<span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Hyperparameters"><a href="#Hyperparameters" class="headerlink" title="Hyperparameters"></a>Hyperparameters</h3><p>$ϵ$ means the agent have probability equals to ϵ choosing a random action and $(1-ϵ)$ probability choosing the optimal action.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">alpha =<span class="number">0.1</span> <span class="comment">#learning rate</span></span><br><span class="line">gamma =<span class="number">0.9</span> <span class="comment">#discount factor</span></span><br><span class="line">epsilon =<span class="number">0.1</span> <span class="comment">#epsilon-greedy</span></span><br><span class="line">num_episodes = <span class="number">1000</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="State-s-and-action-s"><a href="#State-s-and-action-s" class="headerlink" title="State(s) and action(s)"></a>State(s) and action(s)</h3><p>The state is just all the points in the 4x4 girds. We create two functions for action selection and transition to next state.</p>
<p>In function <code>get_next_state</code>, the current coordinates based on the action:</p>
<ul>
<li>If the action is <code>'up'</code> and <code>x</code> is greater than 0, it moves up by decrementing <code>x</code> by 1.</li>
<li>If the action is <code>'down'</code> and <code>x</code> is less than <code>grid_size - 1</code>, it moves down by incrementing <code>x</code> by 1.</li>
<li>If the action is <code>'left'</code> and <code>y</code> is greater than 0, it moves left by decrementing <code>y</code> by 1.</li>
<li>If the action is <code>'right'</code> and <code>y</code> is less than <code>grid_size - 1</code>, it moves right by incrementing <code>y</code> by 1.</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_next_state</span>(<span class="params">state, action</span>):</span><br><span class="line">    x, y = state</span><br><span class="line">    <span class="keyword">if</span> action == <span class="string">'up'</span> <span class="keyword">and</span> x &gt; <span class="number">0</span>:</span><br><span class="line">        x -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="string">'down'</span> <span class="keyword">and</span> x &lt; grid_size - <span class="number">1</span>:</span><br><span class="line">        x += <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="string">'left'</span> <span class="keyword">and</span> y &gt; <span class="number">0</span>:</span><br><span class="line">        y -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="string">'right'</span> <span class="keyword">and</span> y &lt; grid_size - <span class="number">1</span>:</span><br><span class="line">        y += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> (x, y)</span><br><span class="line"></span><br><span class="line">actions = [<span class="string">'up'</span>,<span class="string">'down'</span>,<span class="string">'left'</span>,<span class="string">'right'</span>]</span><br><span class="line">num_actions =<span class="built_in">len</span>(actions)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_action</span>(<span class="params">state</span>):</span><br><span class="line">	<span class="keyword">if</span> np.random.uniform(<span class="number">0</span>,<span class="number">1</span>) &lt; epsilon:</span><br><span class="line">		<span class="keyword">return</span> np.random.choice(num_actions)</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="keyword">return</span> np.argmax(Q_table[state[<span class="number">0</span>], state[<span class="number">1</span>], :])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Reward-function"><a href="#Reward-function" class="headerlink" title="Reward function"></a>Reward function</h3><p>Agent will get reward = 100 if arriving at (3, 3), otherwise -1.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_reward</span>(<span class="params">state</span>):</span><br><span class="line">	<span class="keyword">if</span> state == goal_state:</span><br><span class="line">		<span class="keyword">return</span> <span class="number">100</span></span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="Q-value-function"><a href="#Q-value-function" class="headerlink" title="Q value function"></a>Q value function</h3><p>This part is the fundamental training loop of algorithm. <code>while state != goal_state</code> means continues iterations until the goal state is reached; <code>action_index = choose_action(state)</code> means select an action index based on policy which not shown; <code>action = actions[action_index]</code>is to convert the action index to an actual action; <code>next_state = get_next_state(state, action)</code> is used to determine the next state based on the current state and action; finally,<code>reward = get_reward(next_state)</code>can calculate the reward for transitioning to the next state.</p>
<p>Then, we store Q value under current (s, a) pair in Q table, computes the maximum Q value for next state and updates Q value for current $(s,a)$ pair using Q learning update rule.</p>
<p>Since we have the maximum Q value of each state stored in Q table, we can try to compute the optimal policy and turn it into the form of path.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> episodeinrange(num_episodes):</span><br><span class="line">    state = start_state</span><br><span class="line"><span class="keyword">while</span> state != goal_state:</span><br><span class="line">        action_index = choose_action(state)</span><br><span class="line">        action = actions[action_index]</span><br><span class="line">        next_state = get_next_state(state, action)</span><br><span class="line">        reward = get_reward(next_state)</span><br><span class="line"></span><br><span class="line">        Q_table[state[<span class="number">0</span>], state[<span class="number">1</span>], action_index] += alpha * (</span><br><span class="line">                reward + gamma * np.<span class="built_in">max</span>(Q_table[next_state[<span class="number">0</span>], next_state[<span class="number">1</span>], :]) - Q_table[</span><br><span class="line">            state[<span class="number">0</span>], state[<span class="number">1</span>], action_index]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        state = next_state</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Final Q-Table:"</span>)</span><br><span class="line"><span class="built_in">print</span>(Q_table)</span><br><span class="line"></span><br><span class="line">state = start_state</span><br><span class="line">path = [state]</span><br><span class="line"><span class="keyword">while</span> state != goal_state:</span><br><span class="line">    action_index = np.argmax(Q_table[state[<span class="number">0</span>], state[<span class="number">1</span>], :])</span><br><span class="line">    action = actions[action_index]</span><br><span class="line">    state = get_next_state(state, action)</span><br><span class="line">    path.append(state)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Path from start to goal:"</span>)</span><br><span class="line"><span class="built_in">print</span>(path)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>The path after training is shown in the figure.</p>
<p><img src="https://s2.loli.net/2024/08/03/kjtiXCEQDSoywsK.png" alt="image.png"></p>
<h1 id="Sarsa-An-on-policy-RL-algorithm"><a href="#Sarsa-An-on-policy-RL-algorithm" class="headerlink" title="Sarsa: An on-policy RL algorithm"></a>Sarsa: An on-policy RL algorithm</h1><h2 id="Difference-between-on-policy-off-policy"><a href="#Difference-between-on-policy-off-policy" class="headerlink" title="Difference between on-policy &amp; off-policy"></a>Difference between on-policy &amp; off-policy</h2><p>In reinforcement learning, <strong>two different policies</strong> are also used for active agents: <strong>a behavior policy</strong> and <strong>a target policy</strong>. A behavior policy is used to decide actions in a given state (what behavior the agent is currently using to interact with its environment), while a target policy is used to learn about desired actions and what rewards are received (the ideal policy the agent seeks to use to interact with its environment).</p>
<blockquote>
<p>If an algorithm’s behavior policy matches its target policy, this means it is an on-policy algorithm. If these policies in an algorithm don’t match, then it is an off-policy algorithm.</p>
</blockquote>
<p><img src="https://s2.loli.net/2024/08/03/D3tYshGI4uvSbdo.png" alt="image.png"></p>
<p>Sarsa operates by choosing an action following the current epsilon-greedy policy and updates its Q values accordingly. On-policy algorithms like Sarsa select random actions where non-greedy actions have some probability of being selected, providing a balance between exploitation and exploration techniques. Since Sarsa Q values are generally learned using the same epsilon-greedy policy for behavior and target, it classifies as on-policy.</p>
<p>Q learning, unlike Sarsa, tends to choose the greedy action in sequence. A greedy action is one that gives the maximum Q value for the state, that is, it follows an optimal policy. Off-policy algorithms like Q learning learn a target policy regardless of what actions are selected from exploration. Since Q learning uses greedy actions, and can evaluate one behavior policy while following a separate target policy, it classifies as off-policy.</p>
<h2 id="Algorithm-1"><a href="#Algorithm-1" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>SARSA, unlike Q learning, is an on-policy algorithm, which means it updates the policy based on the actions taken. Quite like policy iteration. But in Sarsa, the update of policy will not be as “hard” as policy iteration since we have the influence of learning rate $α$.</p>
<p>Still, the algorithm keeps updating Q value. One thing different is agent of Sarsa already come up with which action to choose and predict next state and next action. That is why the algorithm called SARSA - State, Action, Reward, State, Action.</p>
<h2 id="Implementation-1"><a href="#Implementation-1" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Maze</span></span><br><span class="line">maze = np.array([</span><br><span class="line">    [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, -<span class="number">1</span>,<span class="number">0</span>, -<span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>, -<span class="number">1</span>],</span><br><span class="line">    [-<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start state and terminal state</span></span><br><span class="line">start_state = (<span class="number">3</span>,<span class="number">0</span>)</span><br><span class="line">goal_state = (<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># action space</span></span><br><span class="line">actions = [(<span class="number">0</span>,<span class="number">1</span>), (<span class="number">0</span>, -<span class="number">1</span>), (-<span class="number">1</span>,<span class="number">0</span>), (<span class="number">1</span>,<span class="number">0</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize value function</span></span><br><span class="line">Q = np.zeros((<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyperparameters</span></span><br><span class="line">alpha = <span class="number">0.1</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">max_episodes = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SARSA</span></span><br><span class="line"><span class="keyword">for</span> episodeinrange(max_episodes):</span><br><span class="line">    state = start_state</span><br><span class="line">    action = np.random.choice(<span class="built_in">range</span>(<span class="number">4</span>))<span class="keyword">if</span> np.random.rand() &lt; epsilonelse np.argmax(Q[state])</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> state != goal_state:</span><br><span class="line"><span class="comment"># next_state = (state[0] + actions[action][0], state[1] + actions[action][1])</span></span><br><span class="line">        a = state[<span class="number">0</span>] + actions[action][<span class="number">0</span>]</span><br><span class="line">        b = state[<span class="number">1</span>] + actions[action][<span class="number">1</span>]</span><br><span class="line"><span class="keyword">if</span> a &gt;<span class="number">3</span>:</span><br><span class="line">            a-=<span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> b &gt;<span class="number">3</span>:</span><br><span class="line">            b-=<span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> a &lt; -<span class="number">4</span>:</span><br><span class="line">            a+=<span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> b &lt; -<span class="number">4</span>:</span><br><span class="line">            b+=<span class="number">1</span></span><br><span class="line">        next_state = (a,b)</span><br><span class="line">        reward = maze[next_state]</span><br><span class="line">        next_action = np.random.choice(<span class="built_in">range</span>(<span class="number">4</span>))<span class="keyword">if</span> np.random.rand() &lt; epsilonelse np.argmax(Q[next_state])</span><br><span class="line">        Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])</span><br><span class="line"></span><br><span class="line">        state = next_state</span><br><span class="line">        action = next_action</span><br><span class="line"></span><br><span class="line"><span class="comment"># print result</span></span><br><span class="line"><span class="keyword">for</span> iinrange(<span class="number">4</span>):</span><br><span class="line"><span class="keyword">for</span> jinrange(<span class="number">4</span>):</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"State:"</span>, (i, j))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Up:"</span>, Q[i][j][<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Down:"</span>, Q[i][j][<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Left:"</span>, Q[i][j][<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Right:"</span>, Q[i][j][<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>()</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">State: (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">Up: -<span class="number">0.008042294056935573</span></span><br><span class="line">Down: -<span class="number">0.007868742418369764</span></span><br><span class="line">Left: -<span class="number">0.016173595452674966</span></span><br><span class="line">Right: <span class="number">0.006662566560762523</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">Up: <span class="number">0.048576025675988774</span></span><br><span class="line">Down: -<span class="number">0.0035842473161881465</span></span><br><span class="line">Left: <span class="number">0.024420015715567546</span></span><br><span class="line">Right: -<span class="number">0.46168987981312615</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">Up: <span class="number">0.04523751845081987</span></span><br><span class="line">Down: <span class="number">0.04266319340558091</span></span><br><span class="line">Left: <span class="number">0.044949583791193154</span></span><br><span class="line">Right: <span class="number">0.026234839551098416</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">Up: <span class="number">0.01629652821649763</span></span><br><span class="line">Down: <span class="number">0.050272192325180515</span></span><br><span class="line">Left: -<span class="number">0.009916869922464355</span></span><br><span class="line">Right: -<span class="number">0.4681667868865369</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">Up: -<span class="number">0.09991342319696966</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: <span class="number">0.0</span></span><br><span class="line">Right: <span class="number">0.036699099068340166</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">Up: <span class="number">0.008563965102313987</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: <span class="number">0.0</span></span><br><span class="line">Right: <span class="number">0.3883250678150607</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">Up: -<span class="number">0.3435187267522706</span></span><br><span class="line">Down: -<span class="number">0.2554776873673874</span></span><br><span class="line">Left: <span class="number">0.05651543121932354</span></span><br><span class="line">Right: <span class="number">0.004593450910446022</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">Up: -<span class="number">0.1</span></span><br><span class="line">Down: -<span class="number">0.013616634831997914</span></span><br><span class="line">Left: <span class="number">0.01298827764814053</span></span><br><span class="line">Right: <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">2</span>, <span class="number">0</span>)</span><br><span class="line">Up: <span class="number">0.28092113053540924</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: <span class="number">0.0024286388798406364</span></span><br><span class="line">Right: <span class="number">0.06302299434701504</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">Up: <span class="number">0.0</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: -<span class="number">0.16509175606504775</span></span><br><span class="line">Right: <span class="number">1.9146361697676122</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">Up: -<span class="number">0.1</span></span><br><span class="line">Down: <span class="number">0.0</span></span><br><span class="line">Left: <span class="number">0.03399106390140035</span></span><br><span class="line">Right: <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">Up: -<span class="number">0.3438668479533914</span></span><br><span class="line">Down: <span class="number">0.004696957810272524</span></span><br><span class="line">Left: -<span class="number">0.19</span></span><br><span class="line">Right: <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">3</span>, <span class="number">0</span>)</span><br><span class="line">Up: <span class="number">3.3060693607932445</span></span><br><span class="line">Down: <span class="number">0.8893977121867367</span></span><br><span class="line">Left: <span class="number">0.0</span></span><br><span class="line">Right: <span class="number">0.13715553550041798</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">Up: <span class="number">4.825854511712306</span></span><br><span class="line">Down: -<span class="number">0.03438123168566812</span></span><br><span class="line">Left: <span class="number">0.10867882029322147</span></span><br><span class="line">Right: <span class="number">1.0015572397722665</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">Up: <span class="number">5.875704328143301</span></span><br><span class="line">Down: <span class="number">0.9315770230698863</span></span><br><span class="line">Left: <span class="number">0.0006851481810742227</span></span><br><span class="line">Right: <span class="number">0.47794799892127526</span></span><br><span class="line"></span><br><span class="line">State: (<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">Up: <span class="number">5.4028951599661275</span></span><br><span class="line">Down: <span class="number">2.6989177956329757</span></span><br><span class="line">Left: -<span class="number">0.6454474033238188</span></span><br><span class="line">Right: <span class="number">0.018474082554518417</span></span><br></pre></td></tr></tbody></table></figure>


  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Model-based-Model-free-RL"><span class="toc-number">1.</span> <span class="toc-text">Model-based &#x2F; Model-free RL</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Monte-Carlo-MC"><span class="toc-number">2.</span> <span class="toc-text">Monte-Carlo (MC)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Overview"><span class="toc-number">2.1.</span> <span class="toc-text">Overview</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Q-Learning-the-first-step-of-model-free-RL"><span class="toc-number">3.</span> <span class="toc-text">Q Learning: the first step of model-free RL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Algorithm"><span class="toc-number">3.1.</span> <span class="toc-text">Algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Implementation"><span class="toc-number">3.2.</span> <span class="toc-text">Implementation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Environment"><span class="toc-number">3.2.1.</span> <span class="toc-text">Environment</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hyperparameters"><span class="toc-number">3.2.2.</span> <span class="toc-text">Hyperparameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#State-s-and-action-s"><span class="toc-number">3.2.3.</span> <span class="toc-text">State(s) and action(s)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reward-function"><span class="toc-number">3.2.4.</span> <span class="toc-text">Reward function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q-value-function"><span class="toc-number">3.2.5.</span> <span class="toc-text">Q value function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Results"><span class="toc-number">3.2.6.</span> <span class="toc-text">Results</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Sarsa-An-on-policy-RL-algorithm"><span class="toc-number">4.</span> <span class="toc-text">Sarsa: An on-policy RL algorithm</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Difference-between-on-policy-off-policy"><span class="toc-number">4.1.</span> <span class="toc-text">Difference between on-policy &amp; off-policy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Algorithm-1"><span class="toc-number">4.2.</span> <span class="toc-text">Algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Implementation-1"><span class="toc-number">4.3.</span> <span class="toc-text">Implementation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#code"><span class="toc-number">4.3.1.</span> <span class="toc-text">code</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Result"><span class="toc-number">4.3.2.</span> <span class="toc-text">Result</span></a></li></ol></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&text=2. Begin with model-free algorithms"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&title=2. Begin with model-free algorithms"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&is_video=false&description=2. Begin with model-free algorithms"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=2. Begin with model-free algorithms&body=Check out this article: http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&title=2. Begin with model-free algorithms"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&title=2. Begin with model-free algorithms"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&title=2. Begin with model-free algorithms"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&title=2. Begin with model-free algorithms"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&name=2. Begin with model-free algorithms&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://ucm14.github.io/2024/10/02/2-Begin-with-model-free-algorithms/&t=2. Begin with model-free algorithms"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2025
    Minfeng &#34;Mason&#34; Yu
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
